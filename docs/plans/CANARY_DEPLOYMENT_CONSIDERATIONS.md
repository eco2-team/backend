# ğŸ¦ Canary ë°°í¬ ì ìš© ê°€ì´ë“œ

**í˜„ì¬ 7ë…¸ë“œ í´ëŸ¬ìŠ¤í„° êµ¬ì¡°ì—ì„œ Canary ë°°í¬ ì ìš© ì‹œ ê³ ë ¤ì‚¬í•­**

- **ì‘ì„±ì¼**: 2025-11-05
- **ëŒ€ìƒ í´ëŸ¬ìŠ¤í„°**: 7 ë…¸ë“œ (1 Master + 6 Workers)
- **ì „ì œ ì¡°ê±´**: Argo Rollouts ë„ì… í•„ìš”

---

## ğŸ“‹ ëª©ì°¨

1. [í˜„ì¬ êµ¬ì¡° ë¶„ì„](#í˜„ì¬-êµ¬ì¡°-ë¶„ì„)
2. [ì£¼ìš” ê³ ë ¤ì‚¬í•­](#ì£¼ìš”-ê³ ë ¤ì‚¬í•­)
3. [Argo Rollouts ì„¤ì¹˜](#argo-rollouts-ì„¤ì¹˜)
4. [êµ¬í˜„ ì „ëµ](#êµ¬í˜„-ì „ëµ)
5. [ë¦¬ì†ŒìŠ¤ ê³„íš](#ë¦¬ì†ŒìŠ¤-ê³„íš)
6. [ë§ˆì´ê·¸ë ˆì´ì…˜ ê³„íš](#ë§ˆì´ê·¸ë ˆì´ì…˜-ê³„íš)

---

## ğŸ—ï¸ í˜„ì¬ êµ¬ì¡° ë¶„ì„

### Application Worker ë…¸ë“œ

```yaml
k8s-worker-1:
  - ì¸ìŠ¤í„´ìŠ¤: t3.medium (2 vCPU, 4GB RAM)
  - AZ: ap-northeast-2b
  - ë¼ë²¨: workload=application
  - ì—­í• : FastAPI ë™ê¸° API

k8s-worker-2:
  - ì¸ìŠ¤í„´ìŠ¤: t3.medium (2 vCPU, 4GB RAM)
  - AZ: ap-northeast-2c
  - ë¼ë²¨: workload=async-workers
  - ì—­í• : Celery ë¹„ë™ê¸° ì‘ì—…

ì´ Application ë¦¬ì†ŒìŠ¤:
  - vCPU: 4 cores
  - RAM: 8GB
  - ë…¸ë“œ: 2ê°œ
```

### í˜„ì¬ ë°°í¬ ë°©ì‹

```yaml
ë°©ì‹: Rolling Update
ì„¤ì •:
  maxSurge: 1
  maxUnavailable: 0
  
íŠ¹ì§•:
  - Kubernetes ê¸°ë³¸ ì „ëµ
  - ìˆœì°¨ì  Pod êµì²´
  - íŠ¸ë˜í”½ ì œì–´ ë¶ˆê°€
  - ë¡¤ë°± ëŠë¦¼
```

---

## âš ï¸ ì£¼ìš” ê³ ë ¤ì‚¬í•­

### 1. íŠ¸ë˜í”½ ë¼ìš°íŒ… ì œì•½

#### ë¬¸ì œ

**Kubernetes ê¸°ë³¸ ServiceëŠ” ì •ë°€í•œ íŠ¸ë˜í”½ ì œì–´ ë¶ˆê°€**

```yaml
í˜„ì¬ êµ¬ì¡°:
  - CNI: Calico (Overlay Network)
  - Ingress: AWS ALB Ingress Controller
  - Service Type: ClusterIP
  - Routing: NodePort

ì œì•½:
  - ServiceëŠ” Round-robin ë¡œë“œë°¸ëŸ°ì‹±ë§Œ ê°€ëŠ¥
  - Pod ìˆ˜ë¡œë§Œ íŠ¸ë˜í”½ ë¹„ìœ¨ ì œì–´ (ê·¼ì‚¬ì¹˜)
  - ì •í™•í•œ % ì œì–´ ë¶ˆê°€
```

**ì˜ˆì‹œ: 10% íŠ¸ë˜í”½ ì œì–´**

```yaml
# ê¸°ë³¸ Kubernetes Service
Total Pods: 10
Canary: 1 Pod (ì‹¤ì œë¡  10% íŠ¸ë˜í”½ ë³´ì¥ ì•ˆ ë¨)
Stable: 9 Pods

ë¬¸ì œ:
  - ë¡œë“œë°¸ëŸ°ì‹± ì•Œê³ ë¦¬ì¦˜ì— ë”°ë¼ ì‹¤ì œ ë¹„ìœ¨ ë³€ë™
  - ì„¸ì…˜ ì–´í”¼ë‹ˆí‹° ìˆìœ¼ë©´ ë” ë¶€ì •í™•
  - ALB Health Checkë„ ì˜í–¥
```

#### í•´ê²° ë°©ì•ˆ

**Option 1: Argo Rollouts (ê¶Œì¥)**

```yaml
ë°©ë²•: Argo Rollouts Controller ì‚¬ìš©
íŠ¸ë˜í”½ ì œì–´: ReplicaSet ê¸°ë°˜ (Pod ìˆ˜ ì¡°ì ˆ)
ì •í™•ë„: ~90% (ê·¼ì‚¬ì¹˜)
ì¥ì : 
  - Istio ë¶ˆí•„ìš”
  - í˜„ì¬ êµ¬ì¡° ìœ ì§€
  - ArgoCDì™€ ë„¤ì´í‹°ë¸Œ í†µí•©
ë‹¨ì :
  - ì •ë°€í•œ % ì œì–´ëŠ” í•œê³„
```

**Option 2: Istio Service Mesh (ë¯¸ë˜)**

```yaml
ë°©ë²•: Istio VirtualService ì‚¬ìš©
íŠ¸ë˜í”½ ì œì–´: L7 ë¼ìš°íŒ… (ì •ë°€)
ì •í™•ë„: 100% (ì •í™•í•œ % ì œì–´)
ì¥ì :
  - ì •ë°€í•œ íŠ¸ë˜í”½ ì œì–´
  - Header/Cookie ê¸°ë°˜ ë¼ìš°íŒ…
  - ê³ ê¸‰ ê¸°ëŠ¥ ë‹¤ìˆ˜
ë‹¨ì :
  - ë³µì¡ë„ ë§¤ìš° ë†’ìŒ
  - ëª¨ë“  Podì— Sidecar í•„ìš”
  - ë¦¬ì†ŒìŠ¤ ì˜¤ë²„í—¤ë“œ í¼ (ê° Podë§ˆë‹¤ ~100MB)
  - 7ê°œ ë…¸ë“œì—ì„œ IstioëŠ” ê³¼ë„
```

**ê¶Œì¥: Argo Rollouts (Istio ì—†ì´)**

---

### 2. ë¦¬ì†ŒìŠ¤ ê´€ë¦¬

#### Application Pod ë¦¬ì†ŒìŠ¤

```yaml
í˜„ì¬ ì„¤ì • (ì˜ˆìƒ):
  requests:
    memory: "256Mi"
    cpu: "100m"
  limits:
    memory: "512Mi"
    cpu: "500m"

ìµœëŒ€ Replica: 8~10ê°œ
ìµœëŒ€ ë©”ëª¨ë¦¬: 8 x 512MB = 4GB
ê°€ìš© ë©”ëª¨ë¦¬: 8GB (worker-1: 4GB, worker-2: 4GB)
ì—¬ìœ : ì¶©ë¶„ âœ…
```

#### Canary ë‹¨ê³„ë³„ ë¦¬ì†ŒìŠ¤

```yaml
Phase 1 (10% Canary):
  Stable: 9 Pods x 512MB = 4.5GB
  Canary: 1 Pod x 512MB = 0.5GB
  Total: 5GB
  ìƒíƒœ: âš ï¸ ì—¬ìœ  ë¶€ì¡± (8GB ì¤‘ 5GB ì‚¬ìš©)

Phase 2 (30% Canary):
  Stable: 7 Pods x 512MB = 3.5GB
  Canary: 3 Pods x 512MB = 1.5GB
  Total: 5GB
  ìƒíƒœ: âš ï¸ ì—¬ìœ  ë¶€ì¡±

Phase 3 (50% Canary):
  Stable: 5 Pods x 512MB = 2.5GB
  Canary: 5 Pods x 512MB = 2.5GB
  Total: 5GB
  ìƒíƒœ: âš ï¸ ì—¬ìœ  ë¶€ì¡±
```

**ë¬¸ì œ: ê¸°ì¡´ 10 ReplicaëŠ” ê³¼ë„**

#### í•´ê²° ë°©ì•ˆ

**Replica ìˆ˜ ì¡°ì • (ê¶Œì¥)**

```yaml
# ê¸°ë³¸ Replica: 5ê°œë¡œ ì¡°ì •
replicas: 5

Phase 1 (10% Canary):
  Stable: 4-5 Pods = 2.5GB
  Canary: 0-1 Pod = 0.5GB
  Total: 3GB âœ…

Phase 2 (30% Canary):
  Stable: 3-4 Pods = 2GB
  Canary: 1-2 Pods = 1GB
  Total: 3GB âœ…

Phase 3 (50% Canary):
  Stable: 2-3 Pods = 1.5GB
  Canary: 2-3 Pods = 1.5GB
  Total: 3GB âœ…

ì—¬ìœ : 5GB (8GB ì¤‘ 3GB ì‚¬ìš©) âœ…
```

---

### 3. NodeSelector ì„¤ì •

#### í•„ìˆ˜ ì„¤ì •

```yaml
# Rollout ë¦¬ì†ŒìŠ¤ì— NodeSelector ì¶”ê°€ í•„ìˆ˜
spec:
  template:
    spec:
      nodeSelector:
        workload: application  # worker-1, worker-2ë§Œ íƒ€ê²ŒíŒ…
      
      # ë˜ëŠ” NodeAffinity ì‚¬ìš©
      affinity:
        nodeAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
            nodeSelectorTerms:
              - matchExpressions:
                  - key: workload
                    operator: In
                    values:
                      - application
```

**ì´ìœ **

```yaml
Infrastructure ë…¸ë“œ (ì „ìš©):
  - k8s-rabbitmq: RabbitMQ ì „ìš©
  - k8s-postgresql: PostgreSQL ì „ìš©
  - k8s-redis: Redis ì „ìš©
  - k8s-monitoring: Prometheus/Grafana ì „ìš©

Application Podì€:
  - k8s-worker-1, k8s-worker-2ì—ë§Œ ë°°í¬ ê°€ëŠ¥
  - NodeSelector ì—†ìœ¼ë©´ Infrastructure ë…¸ë“œì— ë°°í¬ë  ìˆ˜ ìˆìŒ
  - ë¦¬ì†ŒìŠ¤ ê²½í•© ë°œìƒ ê°€ëŠ¥
```

---

### 4. Analysis Template êµ¬ì„±

#### Prometheus ë©”íŠ¸ë¦­ í™•ì¸

**í•„ìˆ˜ ë©”íŠ¸ë¦­ ì‚¬ì „ ì„¤ì •**

```yaml
í•„ìš” ë©”íŠ¸ë¦­:
  1. http_requests_total (ì„±ê³µ/ì‹¤íŒ¨ ì¹´ìš´í„°)
  2. http_request_duration_seconds (ë ˆì´í„´ì‹œ íˆìŠ¤í† ê·¸ë¨)
  3. error_rate (ì—ëŸ¬ìœ¨)

í™•ì¸ ë°©ë²•:
  - Prometheus UI ì ‘ì†
  - ë©”íŠ¸ë¦­ ì¡´ì¬ ì—¬ë¶€ í™•ì¸
  - Label êµ¬ì¡° í™•ì¸ (service, pod, status ë“±)
```

**Application ê³„ì¸¡ í•„ìš”**

```python
# FastAPIì— Prometheus ê³„ì¸¡ ì¶”ê°€ (ì˜ˆì‹œ)
from prometheus_client import Counter, Histogram, make_asgi_app
from fastapi import FastAPI

app = FastAPI()

# ë©”íŠ¸ë¦­ ì •ì˜
http_requests_total = Counter(
    'http_requests_total',
    'Total HTTP requests',
    ['service', 'method', 'endpoint', 'status']
)

http_request_duration_seconds = Histogram(
    'http_request_duration_seconds',
    'HTTP request latency',
    ['service', 'method', 'endpoint']
)

# Prometheus ì—”ë“œí¬ì¸íŠ¸
metrics_app = make_asgi_app()
app.mount("/metrics", metrics_app)
```

**Analysis Template**

```yaml
apiVersion: argoproj.io/v1alpha1
kind: AnalysisTemplate
metadata:
  name: success-rate
spec:
  args:
    - name: service-name
  metrics:
    - name: success-rate
      interval: 1m
      successCondition: result >= 0.95
      failureLimit: 3
      provider:
        prometheus:
          address: http://prometheus.monitoring:9090
          query: |
            sum(rate(http_requests_total{
              service="{{args.service-name}}",
              status!~"5.."
            }[5m])) /
            sum(rate(http_requests_total{
              service="{{args.service-name}}"
            }[5m]))
```

---

### 5. ì „ìš© Monitoring ë…¸ë“œ í™œìš©

#### í˜„ì¬ êµ¬ì¡°ì˜ ì¥ì 

```yaml
k8s-monitoring:
  - ì¸ìŠ¤í„´ìŠ¤: t3.large (2 vCPU, 8GB RAM)
  - ì›Œí¬ë¡œë“œ: Prometheus + Grafana ì „ìš©
  - ìŠ¤í† ë¦¬ì§€: 60GB (TSDB)
  
ì¥ì :
  âœ… Prometheusê°€ ì•ˆì •ì ìœ¼ë¡œ ë™ì‘
  âœ… ë©”íŠ¸ë¦­ ìˆ˜ì§‘ ì˜í–¥ ì—†ìŒ
  âœ… Analysis Template ì•ˆì •ì„± ë³´ì¥
```

#### Prometheus ì„¤ì • í™•ì¸

```yaml
í•„ìš” ì„¤ì •:
  - Service Discovery: Kubernetes API ì—°ë™
  - ServiceMonitor: Application Pod ë©”íŠ¸ë¦­ ìˆ˜ì§‘
  - Retention: 7ì¼ ì´ìƒ ê¶Œì¥
  - Query Performance: ë†’ìŒ í•„ìš”
```

---

### 6. Deployment â†’ Rollout ë§ˆì´ê·¸ë ˆì´ì…˜

#### ê¸°ì¡´ Deployment í™•ì¸

```bash
# í˜„ì¬ Deployment í™•ì¸
kubectl get deployments -o yaml > current-deployment.yaml

# Replica, Resource, NodeSelector í™•ì¸
kubectl describe deployment backend
```

#### ë§ˆì´ê·¸ë ˆì´ì…˜ ì „ëµ

**Option 1: ì§ì ‘ ì „í™˜ (ê¶Œì¥)**

```yaml
ë‹¨ê³„:
  1. Deployment YAMLì„ Rollout YAMLë¡œ ë³€í™˜
  2. Deployment ì‚­ì œ
  3. Rollout ìƒì„±
  
ì¥ì :
  - ê¹”ë”í•œ ì „í™˜
  - Rolloutì´ ReplicaSet ê´€ë¦¬

ë‹¨ì :
  - ì¼ì‹œì  ë‹¤ìš´íƒ€ì„ (1~2ë¶„)
```

**Option 2: Blue-Green ì „í™˜**

```yaml
ë‹¨ê³„:
  1. Rollout ìƒì„± (ë‹¤ë¥¸ ì´ë¦„)
  2. Serviceë¥¼ Rolloutìœ¼ë¡œ ì „í™˜
  3. ê¸°ì¡´ Deployment ì‚­ì œ

ì¥ì :
  - ë‹¤ìš´íƒ€ì„ ì—†ìŒ
  - ì•ˆì „í•œ ì „í™˜

ë‹¨ì :
  - ì¼ì‹œì  ë¦¬ì†ŒìŠ¤ 2ë°°
```

---

### 7. ArgoCD í†µí•©

#### ArgoCD Application ìˆ˜ì •

```yaml
# ê¸°ì¡´: Deployment ì‚¬ìš©
apiVersion: argoproj.io/v1alpha1
kind: Application
metadata:
  name: backend
spec:
  source:
    path: k8s/deployment  # âŒ ë³€ê²½ í•„ìš”

# ë³€ê²½: Rollout ì‚¬ìš©
spec:
  source:
    path: k8s/rollouts   # âœ… ìƒˆ ê²½ë¡œ
```

#### Argo Rollouts Plugin

```bash
# ArgoCDì— Rollouts Plugin ì¶”ê°€
kubectl patch configmap argocd-cm -n argocd --type merge -p '
{
  "data": {
    "resource.customizations": |
      argoproj.io/Rollout:
        health.lua: |
          hs = {}
          if obj.status ~= nil then
            if obj.status.phase == "Healthy" then
              hs.status = "Healthy"
              hs.message = obj.status.message
              return hs
            elseif obj.status.phase == "Degraded" then
              hs.status = "Degraded"
              hs.message = obj.status.message
              return hs
            end
          end
          hs.status = "Progressing"
          hs.message = "Waiting for rollout"
          return hs
  }
}'

# ArgoCD ì¬ì‹œì‘
kubectl rollout restart deployment argocd-server -n argocd
```

---

## ğŸš€ Argo Rollouts ì„¤ì¹˜

### 1. Controller ì„¤ì¹˜

```bash
# Namespace ìƒì„±
kubectl create namespace argo-rollouts

# Argo Rollouts ì„¤ì¹˜
kubectl apply -n argo-rollouts -f \
  https://github.com/argoproj/argo-rollouts/releases/latest/download/install.yaml

# ì„¤ì¹˜ í™•ì¸
kubectl get pods -n argo-rollouts
```

### 2. Kubectl Plugin ì„¤ì¹˜

```bash
# Plugin ë‹¤ìš´ë¡œë“œ
curl -LO https://github.com/argoproj/argo-rollouts/releases/latest/download/kubectl-argo-rollouts-linux-amd64

# ì‹¤í–‰ ê¶Œí•œ ë¶€ì—¬
chmod +x kubectl-argo-rollouts-linux-amd64

# ì„¤ì¹˜
sudo mv kubectl-argo-rollouts-linux-amd64 /usr/local/bin/kubectl-argo-rollouts

# ì„¤ì¹˜ í™•ì¸
kubectl argo rollouts version
```

### 3. Dashboard (ì„ íƒ)

```bash
# Dashboard ì‹¤í–‰
kubectl argo rollouts dashboard

# í¬íŠ¸ í¬ì›Œë”© (ë¡œì»¬ ì ‘ì†)
kubectl port-forward -n argo-rollouts \
  svc/argo-rollouts-dashboard 3100:3100

# ë¸Œë¼ìš°ì €: http://localhost:3100
```

---

## ğŸ“ êµ¬í˜„ ì „ëµ

### Phase 1: ì¤€ë¹„ (1ì£¼)

```yaml
Week 1:
  Day 1-2:
    - Argo Rollouts ì„¤ì¹˜
    - Dashboard ì„¤ì •
    - Kubectl Plugin ì„¤ì¹˜
    - íŒ€ êµìœ¡
    
  Day 3-4:
    - Application ë©”íŠ¸ë¦­ ì¶”ê°€
    - Prometheus ì„¤ì • í™•ì¸
    - ServiceMonitor ìƒì„±
    - ë©”íŠ¸ë¦­ ìˆ˜ì§‘ ê²€ì¦
    
  Day 5:
    - Analysis Template ì‘ì„±
    - í…ŒìŠ¤íŠ¸ í™˜ê²½ êµ¬ì¶• (Kind ë˜ëŠ” ìŠ¤í…Œì´ì§•)
    - PoC ì‹¤í–‰
```

### Phase 2: ê°œë°œ í™˜ê²½ ì ìš© (1ì£¼)

```yaml
Week 2:
  Day 1-2:
    - Deployment â†’ Rollout ë³€í™˜
    - NodeSelector ì„¤ì •
    - Replica ìˆ˜ ì¡°ì • (10 â†’ 5)
    
  Day 3-4:
    - ê°œë°œ í™˜ê²½ ë°°í¬
    - Canary ë°°í¬ í…ŒìŠ¤íŠ¸
    - Analysis Template íŠœë‹
    
  Day 5:
    - ëª¨ë‹ˆí„°ë§ ê²€ì¦
    - ìë™ ë¡¤ë°± í…ŒìŠ¤íŠ¸
    - ë¬¸ì„œ ì‘ì„±
```

### Phase 3: í”„ë¡œë•ì…˜ ì ìš© (1ì£¼)

```yaml
Week 3:
  Day 1-2:
    - í”„ë¡œë•ì…˜ Rollout YAML ì‘ì„±
    - ë¦¬ì†ŒìŠ¤ ì¬í™•ì¸
    - ArgoCD Application ì—…ë°ì´íŠ¸
    
  Day 3:
    - ë‚®ì€ íŠ¸ë˜í”½ ì‹œê°„ëŒ€ ë°°í¬
    - Canary 10% í…ŒìŠ¤íŠ¸
    - ëª¨ë‹ˆí„°ë§
    
  Day 4-5:
    - Canary 30%, 50% í…ŒìŠ¤íŠ¸
    - ìë™ ìŠ¹ê²© ê²€ì¦
    - íšŒê³  ë° ê°œì„ 
```

---

## ğŸ“Š ë¦¬ì†ŒìŠ¤ ê³„íš

### ê¶Œì¥ Rollout ì„¤ì •

```yaml
apiVersion: argoproj.io/v1alpha1
kind: Rollout
metadata:
  name: backend
spec:
  replicas: 5  # 10 â†’ 5ë¡œ ì¡°ì •
  
  selector:
    matchLabels:
      app: backend
  
  template:
    metadata:
      labels:
        app: backend
    spec:
      # NodeSelector í•„ìˆ˜
      nodeSelector:
        workload: application
      
      containers:
        - name: backend
          image: ghcr.io/org/backend:latest
          
          # ë¦¬ì†ŒìŠ¤ ì œí•œ
          resources:
            requests:
              memory: "256Mi"
              cpu: "100m"
            limits:
              memory: "512Mi"
              cpu: "500m"
          
          # Health Check
          livenessProbe:
            httpGet:
              path: /health
              port: 8000
            initialDelaySeconds: 30
            periodSeconds: 10
          
          readinessProbe:
            httpGet:
              path: /ready
              port: 8000
            initialDelaySeconds: 5
            periodSeconds: 5
  
  strategy:
    canary:
      # Canary ë‹¨ê³„
      steps:
        # 10% Canary
        - setWeight: 20  # 5ê°œ ì¤‘ 1ê°œ = 20%
        - pause: {duration: 5m}
        - analysis:
            templates:
              - templateName: success-rate
              - templateName: latency
        
        # 50% Canary
        - setWeight: 50
        - pause: {duration: 5m}
        - analysis:
            templates:
              - templateName: success-rate
              - templateName: latency
      
      # ìë™ ìŠ¹ê²© ë¹„í™œì„±í™” (ìˆ˜ë™ ì œì–´)
      autoPromotionEnabled: false
```

---

## ğŸ—ºï¸ ë§ˆì´ê·¸ë ˆì´ì…˜ ê³„íš

### 1. ì‚¬ì „ ì¤€ë¹„

```bash
# 1. í˜„ì¬ Deployment ë°±ì—…
kubectl get deployment backend -o yaml > backup-deployment.yaml

# 2. í˜„ì¬ Pod ìˆ˜ í™•ì¸
kubectl get pods -l app=backend

# 3. ë¦¬ì†ŒìŠ¤ ì‚¬ìš©ëŸ‰ í™•ì¸
kubectl top pods -l app=backend
kubectl top nodes k8s-worker-1 k8s-worker-2
```

### 2. Rollout ë°°í¬ (Blue-Green ë°©ì‹)

```bash
# 1. Rollout ìƒì„± (ë‹¤ë¥¸ ì´ë¦„)
kubectl apply -f backend-rollout.yaml

# 2. Rollout Pod í™•ì¸
kubectl get pods -l app=backend-rollout

# 3. Serviceë¥¼ Rolloutìœ¼ë¡œ ì „í™˜
kubectl patch service backend -p '
{
  "spec": {
    "selector": {
      "app": "backend-rollout"
    }
  }
}'

# 4. ê¸°ì¡´ Deployment ì‚­ì œ
kubectl delete deployment backend

# 5. Rollout ì´ë¦„ ë³€ê²½ (backend-rollout â†’ backend)
# ë˜ëŠ” ì²˜ìŒë¶€í„° backendë¡œ ìƒì„±
```

### 3. ì²« Canary ë°°í¬

```bash
# 1. ì´ë¯¸ì§€ ì—…ë°ì´íŠ¸
kubectl argo rollouts set image backend backend=ghcr.io/org/backend:v2.0.0

# 2. Rollout ìƒíƒœ í™•ì¸
kubectl argo rollouts get rollout backend --watch

# 3. ìˆ˜ë™ ìŠ¹ê²©
kubectl argo rollouts promote backend

# 4. ë¡¤ë°± (ë¬¸ì œ ë°œìƒ ì‹œ)
kubectl argo rollouts undo backend
```

---

## âš¡ ë¹ ë¥¸ ì‹œì‘

### ìµœì†Œ êµ¬ì„±ìœ¼ë¡œ ì‹œì‘

```yaml
# 1. Argo Rollouts ì„¤ì¹˜ë§Œ
kubectl create namespace argo-rollouts
kubectl apply -n argo-rollouts -f \
  https://github.com/argoproj/argo-rollouts/releases/latest/download/install.yaml

# 2. ê°„ë‹¨í•œ Rollout (Analysis ì—†ì´)
apiVersion: argoproj.io/v1alpha1
kind: Rollout
metadata:
  name: backend
spec:
  replicas: 5
  selector:
    matchLabels:
      app: backend
  template:
    # ... (ê¸°ì¡´ Pod spec)
  strategy:
    canary:
      steps:
        - setWeight: 20
        - pause: {duration: 5m}
        - setWeight: 50
        - pause: {duration: 5m}

# 3. ë°°í¬ ë° ìˆ˜ë™ ìŠ¹ê²©
kubectl argo rollouts set image backend backend=new-image:tag
kubectl argo rollouts promote backend
```

---

## ğŸ“š ì°¸ê³  ìë£Œ

- [Argo Rollouts ê³µì‹ ë¬¸ì„œ](https://argoproj.github.io/argo-rollouts/)
- [ë°°í¬ ì „ëµ ë¹„êµ](DEPLOYMENT_STRATEGIES_COMPARISON.md)
- [í´ëŸ¬ìŠ¤í„° ë¦¬ì†ŒìŠ¤ í˜„í™©](../infrastructure/CLUSTER_RESOURCES.md)
- [CI/CD íŒŒì´í”„ë¼ì¸](../architecture/CI_CD_PIPELINE.md)

---

**ë¬¸ì„œ ë²„ì „**: 1.0  
**ìµœì¢… ì—…ë°ì´íŠ¸**: 2025-11-05  
**ëŒ€ìƒ**: 7ë…¸ë“œ í´ëŸ¬ìŠ¤í„° (14 vCPU, 30GB RAM)  
**ìƒíƒœ**: êµ¬í˜„ ê°€ëŠ¥, Phase 2 ì¶”ì§„ ê¶Œì¥

