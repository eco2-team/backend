# Troubleshooting Guide - ì´ì½”ì—ì½”(EcoÂ²)

> 14-Node Microservices Architecture + Worker Local SQLite WAL êµ¬ì¶• ê³¼ì •ì—ì„œ ë°œìƒí•œ ë¬¸ì œ ë° í•´ê²° ë°©ì•ˆ
>
> **ì—…ë°ì´íŠ¸Â (2025-11-15)**  
> ë³¸ ë¬¸ì„œì—ëŠ” `k8s/atlantis/atlantis-deployment.yaml` ë“± ë ˆê±°ì‹œ ê²½ë¡œê°€ ë‹¤ìˆ˜ ì–¸ê¸‰ë©ë‹ˆë‹¤.  
> í˜„ì¬ AtlantisëŠ” `platform/charts/platform/atlantis` Helm Chartì™€ `argocd/apps/70-gitops-tools.yaml`ì„ í†µí•´ ë°°í¬ë˜ë¯€ë¡œ, ë™ì¼ ë¬¸ì œ ë°œìƒ ì‹œ ìµœì‹  ì ˆì°¨(`docs/architecture/gitops/ATLANTIS_TERRAFORM_FLOW.md`)ë„ í•¨ê»˜ ì°¸ê³ í•˜ì„¸ìš”.

## ğŸ“‹ ëª©ì°¨

- [1. Terraform ê´€ë ¨ ë¬¸ì œ](#1-terraform-ê´€ë ¨-ë¬¸ì œ)
- [2. IAM Policy ì¤‘ë³µ ë¬¸ì œ](#2-iam-policy-ì¤‘ë³µ-ë¬¸ì œ)
- [3. AWS í•œë„ ê´€ë ¨ ë¬¸ì œ](#3-aws-í•œë„-ê´€ë ¨-ë¬¸ì œ)
- [4. ìŠ¤í¬ë¦½íŠ¸ ì‹¤í–‰ ë¬¸ì œ](#4-ìŠ¤í¬ë¦½íŠ¸-ì‹¤í–‰-ë¬¸ì œ)
- [5. GitHub CLI ì¸ì¦ ë¬¸ì œ](#5-github-cli-ì¸ì¦-ë¬¸ì œ)
- [6. CloudFront ê´€ë ¨ ë¬¸ì œ](#6-cloudfront-ê´€ë ¨-ë¬¸ì œ)
- [7. VPC ì‚­ì œ ì§€ì—° ë¬¸ì œ](#7-vpc-ì‚­ì œ-ì§€ì—°-ë¬¸ì œ)
- [8. ArgoCD ë¦¬ë””ë ‰ì…˜ ë£¨í”„ ë¬¸ì œ](#8-argocd-ë¦¬ë””ë ‰ì…˜-ë£¨í”„-ë¬¸ì œ)
- [9. Prometheus ë©”ëª¨ë¦¬ ë¶€ì¡± ë¬¸ì œ](#9-prometheus-ë©”ëª¨ë¦¬-ë¶€ì¡±-ë¬¸ì œ)
- [10. Atlantis Pod CrashLoopBackOff ë¬¸ì œ](#10-atlantis-pod-crashloopbackoff-ë¬¸ì œ)
- [11. Atlantis Podì—ì„œ kubectlì„ ì°¾ì„ ìˆ˜ ì—†ëŠ” ë¬¸ì œ](#11-atlantis-podì—ì„œ-kubectlì„-ì°¾ì„-ìˆ˜-ì—†ëŠ”-ë¬¸ì œ)
- [12. Atlantis Deployment íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ëŠ” ë¬¸ì œ](#12-atlantis-deployment-íŒŒì¼ì„-ì°¾ì„-ìˆ˜-ì—†ëŠ”-ë¬¸ì œ)
- [13. Atlantis ì‹¤í–‰ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ëŠ” ë¬¸ì œ](#13-atlantis-ì‹¤í–‰-íŒŒì¼ì„-ì°¾ì„-ìˆ˜-ì—†ëŠ”-ë¬¸ì œ)
- [14. Atlantis ConfigMap YAML íŒŒì‹± ì—ëŸ¬](#14-atlantis-configmap-yaml-íŒŒì‹±-ì—ëŸ¬)
- [15. ë² ìŠ¤íŠ¸ í”„ë™í‹°ìŠ¤](#15-ë² ìŠ¤íŠ¸-í”„ë™í‹°ìŠ¤)
- [16. ì°¸ê³  ë¬¸ì„œ](#16-ì°¸ê³ -ë¬¸ì„œ)
- [17. ì§€ì›](#17-ì§€ì›)
- [18. GitOps ë°°í¬ ë¬¸ì œ (2025-11-16 ì¶”ê°€)](#18-gitops-ë°°í¬-ë¬¸ì œ-2025-11-16-ì¶”ê°€)
- [19. ë² ìŠ¤íŠ¸ í”„ë™í‹°ìŠ¤ (2025-11-16 ì—…ë°ì´íŠ¸)](#19-ë² ìŠ¤íŠ¸-í”„ë™í‹°ìŠ¤-2025-11-16-ì—…ë°ì´íŠ¸)
- [20. Legacy Troubleshooting Archive (docs/troubleshooting/*)](#20-legacy-troubleshooting-archive-docstroubleshooting)

---

## 1. Terraform ê´€ë ¨ ë¬¸ì œ

### 1.1. Duplicate Resource Configuration

#### ë¬¸ì œ
```
Error: Duplicate resource "aws_iam_policy" configuration
Error: Duplicate resource "aws_iam_role_policy_attachment" configuration
```

**ì›ì¸**: ë™ì¼í•œ ë¦¬ì†ŒìŠ¤ê°€ ì—¬ëŸ¬ íŒŒì¼ì— ì„ ì–¸ë¨
- `terraform/iam.tf`ì™€ `terraform/alb-controller-iam.tf`ì— `aws_iam_policy.alb_controller` ì¤‘ë³µ

#### í•´ê²°
```bash
# iam.tfì—ì„œ ì¤‘ë³µ ì„ ì–¸ ì œê±°
# alb-controller-iam.tfì—ë§Œ ìœ ì§€
```

**ì ìš© íŒŒì¼**:
- `terraform/iam.tf` (ì¤‘ë³µ ì œê±°)
- `terraform/alb-controller-iam.tf` (ìœ ì§€)

**ì»¤ë°‹**: `feat: Remove duplicate IAM policy declarations`

---

### 1.2. Provider Configuration Not Present

#### ë¬¸ì œ
```
Error: Provider configuration not present
To work with aws_acm_certificate.cdn its original provider configuration at 
provider["registry.terraform.io/hashicorp/aws"].us_east_1 is required
```

**ì›ì¸**: CloudFront ACM ì¸ì¦ì„œëŠ” `us-east-1` ë¦¬ì „ì— ìˆì–´ì•¼ í•˜ì§€ë§Œ provider ì„¤ì • ëˆ„ë½

#### í•´ê²°
`terraform/main.tf`ì— `us-east-1` provider ì¶”ê°€:

```hcl
# CloudFront requires ACM certificate in us-east-1
provider "aws" {
  alias  = "us_east_1"
  region = "us-east-1"
  
  default_tags {
    tags = {
      Project     = "SeSACTHON"
      ManagedBy   = "Terraform"
      Environment = var.environment
      Team        = "Backend"
    }
  }
}
```

**ì»¤ë°‹**: `fix: Add us-east-1 provider for CloudFront ACM certificates`

---

### 1.3. Reference to Undeclared Resource

#### ë¬¸ì œ
```
Error: Reference to undeclared resource
A managed resource "aws_iam_role" "ec2_ssm_role" has not been declared
```

**ì›ì¸**: ì˜ëª»ëœ IAM role ì°¸ì¡°
- `alb-controller-iam.tf`ì™€ `s3.tf`ì—ì„œ `aws_iam_role.ec2_ssm_role` ì°¸ì¡°
- ì‹¤ì œ ë¦¬ì†ŒìŠ¤ ì´ë¦„ì€ `aws_iam_role.k8s_node`

#### í•´ê²°
```hcl
# Before
role = aws_iam_role.ec2_ssm_role.name

# After
role = aws_iam_role.k8s_node.name
```

**ì ìš© íŒŒì¼**:
- `terraform/alb-controller-iam.tf`
- `terraform/s3.tf`

**ì»¤ë°‹**: `fix: Correct IAM role reference from ec2_ssm_role to k8s_node`

---

### 1.4. Missing Resource Instance Key

#### ë¬¸ì œ
```
Error: Missing resource instance key
Because data.aws_route53_zone.main has "count" set, its attributes must be 
accessed on specific instances.
```

**ì›ì¸**: `count` ê¸°ë°˜ ë¦¬ì†ŒìŠ¤ë¥¼ ì¸ë±ìŠ¤ ì—†ì´ ì°¸ì¡°

#### í•´ê²°
```hcl
# Before
zone_id = data.aws_route53_zone.main.zone_id

# After
zone_id = data.aws_route53_zone.main[0].zone_id
```

**ì ìš© íŒŒì¼**: `terraform/cloudfront.tf`

**ì»¤ë°‹**: `fix: Add index to count-based Route53 zone reference`

---

### 1.5. Invalid Attribute Combination (S3 Lifecycle)

#### ë¬¸ì œ
```
Warning: Invalid Attribute Combination
No attribute specified when one (and only one) of [rule[0].filter,rule[0].prefix] is required
```

**ì›ì¸**: S3 lifecycle ruleì— `filter` ë˜ëŠ” `prefix` í•„ìˆ˜

#### í•´ê²°
```hcl
resource "aws_s3_bucket_lifecycle_configuration" "images" {
  bucket = aws_s3_bucket.images.id

  rule {
    id     = "cleanup-old-images"
    status = "Enabled"

    filter {
      prefix = ""  # ëª¨ë“  ê°ì²´ì— ì ìš©
    }

    transition {
      days          = 30
      storage_class = "STANDARD_IA"
    }

    expiration {
      days = 90
    }
  }
}
```

**ì ìš© íŒŒì¼**: `terraform/s3.tf`

**ì»¤ë°‹**: `fix: Add filter block to S3 lifecycle configuration`

---

### 1.6. No Configuration Files

#### ë¬¸ì œ
```
Error: No configuration files
Apply requires configuration to be present.
```

**ì›ì¸**: `auto-rebuild.sh`ì˜ Step 2ì—ì„œ terraform ë””ë ‰í† ë¦¬ë¡œ ì´ë™í•˜ì§€ ì•ŠìŒ

#### í•´ê²°
```bash
# Step 2 ì‹œì‘ ì‹œ ëª…ì‹œì ìœ¼ë¡œ ë””ë ‰í† ë¦¬ ì´ë™
cd "$PROJECT_ROOT/terraform"

echo "ğŸ”§ Terraform ì´ˆê¸°í™” (ì¬í™•ì¸)..."
terraform init -migrate-state -upgrade
```

**ì ìš© íŒŒì¼**: `scripts/cluster/auto-rebuild.sh`

**ì»¤ë°‹**: `fix: Add missing cd to terraform directory in auto-rebuild.sh Step 2`

---

## 2. IAM Policy ì¤‘ë³µ ë¬¸ì œ

### 2.1. EntityAlreadyExists - IAM Policy

#### ë¬¸ì œ
```
Error: creating IAM Policy (prod-alb-controller-policy): EntityAlreadyExists
Error: creating IAM Policy (prod-s3-presigned-url-policy): EntityAlreadyExists
```

**ì›ì¸**: ì´ì „ ë°°í¬ì˜ IAM Policyê°€ Terraform stateì— ì—†ì§€ë§Œ AWSì— ë‚¨ì•„ìˆìŒ

#### í•´ê²°
`scripts/maintenance/destroy-with-cleanup.sh`ì— IAM Policy ê°•ì œ ì •ë¦¬ ì¶”ê°€:

```bash
# IAM Policy ê°•ì œ ì •ë¦¬
echo "ğŸ” IAM Policy ê°•ì œ ì •ë¦¬..."
POLICY_ARNS=$(aws iam list-policies --scope Local \
    --query "Policies[?contains(PolicyName, 'alb-controller') || contains(PolicyName, 'ecoeco') || contains(PolicyName, 's3-presigned-url')].Arn" \
    --output text)

if [ -n "$POLICY_ARNS" ]; then
    for policy_arn in $POLICY_ARNS; do
        # Roleì—ì„œ detach
        ATTACHED_ROLES=$(aws iam list-entities-for-policy \
            --policy-arn "$policy_arn" \
            --entity-filter Role \
            --query 'PolicyRoles[*].RoleName' \
            --output text)
        
        for role in $ATTACHED_ROLES; do
            aws iam detach-role-policy --role-name "$role" --policy-arn "$policy_arn"
        done
        
        # Policy ì‚­ì œ
        aws iam delete-policy --policy-arn "$policy_arn"
    done
fi
```

**ì»¤ë°‹**: `feat: Enhance destroy-with-cleanup.sh with comprehensive AWS resource cleanup`

---

### 2.2. auto-rebuild.sh í†µí•©

#### ë¬¸ì œ
`auto-rebuild.sh`ê°€ ë³µì¡í•œ ì •ë¦¬ ë¡œì§ì„ ì§ì ‘ êµ¬í˜„í•˜ì—¬ ìœ ì§€ë³´ìˆ˜ ì–´ë ¤ì›€

#### í•´ê²°
`destroy-with-cleanup.sh`ë¥¼ í˜¸ì¶œí•˜ë„ë¡ ë¦¬íŒ©í† ë§:

```bash
# Step 1: Terraform Destroy (destroy-with-cleanup.sh í˜¸ì¶œ)
if [ -f "$PROJECT_ROOT/scripts/maintenance/destroy-with-cleanup.sh" ]; then
    echo "ğŸ”§ destroy-with-cleanup.sh ì‹¤í–‰ ì¤‘..."
    export AUTO_MODE=true
    bash "$PROJECT_ROOT/scripts/maintenance/destroy-with-cleanup.sh"
else
    # Fallback: ê°„ë‹¨í•œ destroy
    terraform destroy -auto-approve
fi
```

**íš¨ê³¼**:
- 200+ ì¤„ ì½”ë“œ ì œê±°
- ë‹¨ì¼ ì •ë¦¬ ë¡œì§ìœ¼ë¡œ í†µí•©
- IAM, S3, CloudFront, Route53, ACM ëª¨ë‘ ìë™ ì •ë¦¬

**ì»¤ë°‹**: `feat: Integrate destroy-with-cleanup.sh into auto-rebuild.sh + S3 Policy cleanup`

---

## 3. AWS í•œë„ ê´€ë ¨ ë¬¸ì œ

### 3.1. VcpuLimitExceeded

#### ë¬¸ì œ
```
Error: creating EC2 Instance: VcpuLimitExceeded
You have requested more vCPU capacity than your current vCPU limit of 16 allows
```

**ì›ì¸**: 14-Node ì•„í‚¤í…ì²˜ í•„ìš” vCPU (26) > ê³„ì • í•œë„ (16)

**vCPU ê³„ì‚°**:
| ë…¸ë“œ íƒ€ì… | ê°œìˆ˜ | ì¸ìŠ¤í„´ìŠ¤ | vCPU/ë…¸ë“œ | ì´ vCPU |
|----------|------|---------|-----------|---------|
| Master | 1 | t3a.large | 2 | 2 |
| API | 6 | t3a.medium | 2 | 12 |
| Worker | 2 | t3a.large | 2 | 4 |
| Infrastructure | 4 | t3a.medium | 2 | 8 |
| **í•©ê³„** | **13** | | | **26** |

#### í•´ê²° ë°©ë²• 1: vCPU í•œë„ ì¦ê°€ ìš”ì²­ (ê¶Œì¥)

**ìë™ ìŠ¤í¬ë¦½íŠ¸**:
```bash
./scripts/utilities/request-vcpu-increase.sh
```

**ìˆ˜ë™ ìš”ì²­**:
```bash
aws service-quotas request-service-quota-increase \
    --service-code ec2 \
    --quota-code L-1216C47A \
    --desired-value 32 \
    --region ap-northeast-2
```

**AWS Console**: Service Quotas â†’ EC2 â†’ "Running On-Demand Standard instances"

**ìŠ¹ì¸ ì‹œê°„**: 15ë¶„-2ì‹œê°„ (ì¼ë°˜ì )

**í•œë„ í™•ì¸**:
```bash
aws service-quotas get-service-quota \
    --service-code ec2 \
    --quota-code L-1216C47A \
    --region ap-northeast-2 \
    --query 'Quota.Value'
```

#### í•´ê²° ë°©ë²• 2: ë©€í‹° ë¦¬ì „ ë°°í¬ (ê¶Œì¥)

âœ… **ì„±ëŠ¥ ìœ ì§€** - í•œë„ ì¦ê°€ ì „ê¹Œì§€ ì„ì‹œ ìš´ì˜

**ì „ëµ**: Stateless ì„œë¹„ìŠ¤ë¥¼ ë„ì¿„ ë¦¬ì „(ap-northeast-1)ìœ¼ë¡œ ë¶„ì‚°

**í•„ìˆ˜ API í˜„í™© (7ê°œ)**:
1. auth - ì¸ì¦/ì¸ê°€
2. my - ë§ˆì´í˜ì´ì§€ (userinfo)
3. info - ì¬í™œìš© ì •ë³´ (recycle_info)
4. location - ìœ„ì¹˜/ì§€ë„
5. character - ìºë¦­í„°/ë¯¸ì…˜ (ì‹ ê·œ ì¶”ê°€ í•„ìš”)
6. scan - íê¸°ë¬¼ ìŠ¤ìº”/ë¶„ì„ (waste)
7. chat - ì±—ë´‡ (chat_llm)

**ì„œìš¸ ë¦¬ì „ (ap-northeast-2) - 14 vCPU**:
```
Master        (t3.large)  = 2 vCPU  â† Kubernetes ì»¨íŠ¸ë¡¤ í”Œë ˆì¸
API-Auth      (t3.micro)  = 2 vCPU  â† ì¸ì¦ (ì§€ì—° ë¯¼ê°)
API-My        (t3.micro)  = 2 vCPU  â† ë§ˆì´í˜ì´ì§€ (DB ì§ì ‘ ì ‘ê·¼)
API-Info      (t3.micro)  = 2 vCPU  â† ì¬í™œìš© ì •ë³´ (DB ì§ì ‘ ì ‘ê·¼)
PostgreSQL    (t3.medium) = 2 vCPU  â† Stateful DB
Redis         (t3.small)  = 2 vCPU  â† ìºì‹œ/ì„¸ì…˜
RabbitMQ      (t3.small)  = 2 vCPU  â† ë©”ì‹œì§€ í
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
í•©ê³„:                     14 vCPU âœ… (ì—¬ìœ  2 vCPU)
```

**ë„ì¿„ ë¦¬ì „ (ap-northeast-1) - 14 vCPU**:
```
API-Location  (t3.micro)  = 2 vCPU  â† ìœ„ì¹˜ (ì™¸ë¶€ API í˜¸ì¶œ)
API-Character (t3.micro)  = 2 vCPU  â† ìºë¦­í„° (ì‚¬ìš© ë¹ˆë„ ë‚®ìŒ, ì‹ ê·œ)
API-Scan      (t3.small)  = 2 vCPU  â† ìŠ¤ìº” (AI Worker ì—°ê³„)
API-Chat      (t3.small)  = 2 vCPU  â† ì±—ë´‡ (ì™¸ë¶€ LLM í˜¸ì¶œ)
Worker-Storage(t3.small)  = 2 vCPU  â† S3 ì—…ë¡œë“œ (ë¹„ë™ê¸°)
Worker-AI     (t3.small)  = 2 vCPU  â† AI ì²˜ë¦¬ (ë¹„ë™ê¸°)
Monitoring    (t3.medium) = 2 vCPU  â† Prometheus/Grafana
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
í•©ê³„:                     14 vCPU âœ…
```

**ì„¤ì • ë°©ë²•**:

1. **Terraform ëª¨ë“ˆ ì—…ë°ì´íŠ¸** (`terraform/main.tf`):
```hcl
# ì´ë¦„ ë³€ê²½ ë° ì‹ ê·œ ì¶”ê°€
module "api_my" { ... }          # userinfo â†’ my
module "api_info" { ... }        # recycle_info â†’ info  
module "api_scan" { ... }        # waste â†’ scan
module "api_chat" { ... }        # chat_llm â†’ chat
module "api_character" { ... }   # ì‹ ê·œ ì¶”ê°€ (t3.micro)

# ë„ì¿„ ë¦¬ì „ í”„ë¡œë°”ì´ë” ì¶”ê°€
provider "aws" {
  alias  = "tokyo"
  region = "ap-northeast-1"
}
```

2. **VPC Peering ìë™ ì„¤ì •**:
   - ì„œìš¸ â†” ë„ì¿„ ë¦¬ì „ ê°„ Private í†µì‹ 
   - RabbitMQ, PostgreSQL ì ‘ê·¼ ê°€ëŠ¥
   - ë°ì´í„° ì „ì†¡ ë¹„ìš©: ~$0.09/GB (ì˜ˆìƒ $0.45/ì›”)

3. **Kubernetes í¬ë¡œìŠ¤ ë¦¬ì „ ì„¤ì •**:
   - ë„ì¿„ ë…¸ë“œë¥¼ ì„œìš¸ Masterì— ì¡°ì¸
   - NodeSelectorë¡œ ë¦¬ì „ë³„ Pod ë°°ì¹˜
   - ì§€ì—°ì‹œê°„: 5-10ms (í—ˆìš© ê°€ëŠ¥)

**ì¬ë°°ì¹˜ ê³„íš** (í•œë„ ì¦ê°€ í›„):
```bash
# 1. ë„ì¿„ ë¦¬ì†ŒìŠ¤ë¥¼ ì„œìš¸ë¡œ ë§ˆì´ê·¸ë ˆì´ì…˜
./scripts/utilities/migrate-tokyo-to-seoul.sh

# 2. Terraformìœ¼ë¡œ ìë™ ì¬ë°°ì¹˜
terraform apply -var="enable_multi_region=false"
```

**ì¥ì **:
- âœ… ì„±ëŠ¥ ì €í•˜ ì—†ìŒ
- âœ… í•„ìˆ˜ API 7ê°œ ëª¨ë‘ ì‚¬ìš© ê°€ëŠ¥
- âœ… í•œë„ ì¦ê°€ í›„ ì‰¬ìš´ ì¬ë°°ì¹˜
- âœ… API Character ì¶”ê°€ ê°€ëŠ¥

**ë‹¨ì **:
- âš ï¸ ì•½ê°„ì˜ ì¶”ê°€ ë¹„ìš© (~$0.45/ì›”)
- âš ï¸ ë„¤íŠ¸ì›Œí¬ ì„¤ì • ë³µì¡ë„ ì¦ê°€
- âš ï¸ ë¦¬ì „ ê°„ ì§€ì—° 5-10ms

**ì»¤ë°‹**: `feat: Add multi-region deployment strategy for 14-node architecture`

---

#### í•´ê²° ë°©ë²• 3: ì¸ìŠ¤í„´ìŠ¤ íƒ€ì… ë‹¤ìš´ê·¸ë ˆì´ë“œ (ë¹„ê¶Œì¥)

âš ï¸ **ì„±ëŠ¥ ì €í•˜ ì‹¬ê°** - ê°œë°œ/í…ŒìŠ¤íŠ¸ìš©ìœ¼ë¡œë§Œ

`terraform.tfvars` ìƒì„±:
```hcl
master_instance_type = "t3.small"   # 2 vCPU (ê¸°ì¡´: medium)
api_instance_type = "t3.nano"       # 0.5 vCPU (ê¸°ì¡´: micro)
worker_instance_type = "t3.micro"   # 2 vCPU (ê¸°ì¡´: small)
infra_instance_type = "t3.nano"     # 0.5 vCPU (ê¸°ì¡´: micro/small)
```

**ì´ vCPU**: ~12 (í•œë„ ë‚´)

âŒ **ë¬¸ì œì **:
- PostgreSQL, RabbitMQ ì„±ëŠ¥ ì €í•˜
- API ì‘ë‹µ ì‹œê°„ ì¦ê°€
- Worker ì²˜ë¦¬ ì†ë„ ê°ì†Œ

---

### 3.2. ResourceAlreadyExistsException

#### ë¬¸ì œ
```
Error: Only one open service quota increase request is allowed per quota.
```

**ì›ì¸**: ì´ë¯¸ ì§„í–‰ ì¤‘ì¸ í•œë„ ì¦ê°€ ìš”ì²­ì´ ìˆìŒ

#### í™•ì¸
```bash
aws service-quotas list-requested-service-quota-change-history-by-quota \
    --service-code ec2 \
    --quota-code L-1216C47A \
    --region ap-northeast-2 \
    --query 'RequestedQuotas[?Status==`PENDING` || Status==`CASE_OPENED`]'
```

**Status**:
- `CASE_OPENED`: AWSê°€ ê²€í†  ì¤‘ (ì¢‹ì€ ì‹ í˜¸!)
- `PENDING`: ëŒ€ê¸° ì¤‘
- `APPROVED`: ìŠ¹ì¸ ì™„ë£Œ
- `DENIED`: ê±°ì ˆ (ë“œë¬¼ìŒ)

#### í•´ê²°
ê¸°ì¡´ ìš”ì²­ì´ ì²˜ë¦¬ë  ë•Œê¹Œì§€ ëŒ€ê¸° (5-10ë¶„ë§ˆë‹¤ í•œë„ í™•ì¸)

---

## 4. ìŠ¤í¬ë¦½íŠ¸ ì‹¤í–‰ ë¬¸ì œ

### 4.1. InvalidKeyPair.Duplicate

#### ë¬¸ì œ
```
Error: importing EC2 Key Pair (k8s-cluster-key): InvalidKeyPair.Duplicate
```

**ì›ì¸**: ì´ì „ Key Pairê°€ ì‚­ì œë˜ì§€ ì•ŠìŒ

#### í•´ê²°
`destroy-with-cleanup.sh`ì— Key Pair ì •ë¦¬ ì¶”ê°€:

```bash
KEY_NAME="k8s-cluster-key"
if aws ec2 describe-key-pairs --key-names "$KEY_NAME" --region "$AWS_REGION" >/dev/null 2>&1; then
    aws ec2 delete-key-pair --key-name "$KEY_NAME" --region "$AWS_REGION"
fi
```

---

### 4.2. BucketAlreadyOwnedByYou

#### ë¬¸ì œ
```
Error: creating S3 Bucket (prod-sesacthon-images): BucketAlreadyOwnedByYou
```

**ì›ì¸**: S3 ë²„í‚·ì´ ì‚­ì œë˜ì§€ ì•Šê³  ë‚¨ì•„ìˆìŒ

#### í•´ê²°
`destroy-with-cleanup.sh`ì— S3 Bucket ì •ë¦¬ ì¶”ê°€:

```bash
BUCKETS=$(aws s3api list-buckets \
    --query "Buckets[?starts_with(Name, 'prod-sesacthon')].Name" \
    --output text)

for bucket in $BUCKETS; do
    # ë²„í‚· ë‚´ìš©ë¬¼ ì‚­ì œ
    aws s3 rm "s3://$bucket" --recursive
    # ë²„í‚· ì‚­ì œ
    aws s3api delete-bucket --bucket "$bucket" --region "$AWS_REGION"
done
```

---

## 5. GitHub CLI ì¸ì¦ ë¬¸ì œ

### 5.1. Missing Required Scope

#### ë¬¸ì œ
```
error validating token: missing required scope 'read:org'
```

**ì›ì¸**: GitHub Personal Access Tokenì— í•„ìš”í•œ scope ëˆ„ë½

#### í•´ê²° ë°©ë²• 1: ëŒ€í™”í˜• ë¡œê·¸ì¸ (ê¶Œì¥)

```bash
gh auth login
# GitHub.com ì„ íƒ
# HTTPS ì„ íƒ
# Login with a web browser ì„ íƒ
# ë¸Œë¼ìš°ì €ì—ì„œ ì½”ë“œ ì…ë ¥
```

#### í•´ê²° ë°©ë²• 2: ìƒˆ PAT ìƒì„±

1. https://github.com/settings/tokens ì ‘ì†
2. "Generate new token (classic)" í´ë¦­
3. í•„ìš”í•œ scopes ì„ íƒ:
   - âœ… `repo` (ì „ì²´)
   - âœ… `read:org`
   - âœ… `write:packages`
4. í† í° ìƒì„± í›„:
```bash
echo "<your-token>" | gh auth login --with-token
```

#### í•´ê²° ë°©ë²• 3: GitHub Web UI (ìˆ˜ë™)

1. Repository â†’ Settings â†’ Secrets and variables â†’ Actions
2. Secret ë“±ë¡:
   - `GITHUB_USERNAME`: `mangowhoiscloud`
3. Variable ë“±ë¡:
   - `VERSION`: `v0.6.0`

âš ï¸ **ì£¼ì˜**: `GITHUB_TOKEN`ì€ GitHub Actionsì—ì„œ ìë™ìœ¼ë¡œ ì œê³µë˜ë¯€ë¡œ ë“±ë¡ ë¶ˆí•„ìš”

---

## 6. CloudFront ê´€ë ¨ ë¬¸ì œ

### 6.1. CloudFront ìƒì„± ì‹œê°„

#### í˜„ìƒ
```
aws_cloudfront_distribution.images: Still creating... [6m0s elapsed]
```

**ì›ì¸**: CloudFront distribution ìƒì„±ì€ ì „ì„¸ê³„ edge locationì— ë°°í¬ë˜ë¯€ë¡œ ì‹œê°„ ì†Œìš”

**ì •ìƒ ë²”ìœ„**: 5-15ë¶„

**í™•ì¸**:
```bash
aws cloudfront get-distribution --id <distribution-id> \
    --query 'Distribution.Status'
```

**Status**:
- `InProgress`: ë°°í¬ ì¤‘ (ì •ìƒ)
- `Deployed`: ë°°í¬ ì™„ë£Œ

---

### 6.2. CloudFront ì‚­ì œ í•„ìš”ì„±

#### ë¬¸ì œ
CloudFrontê°€ ë‚¨ì•„ìˆìœ¼ë©´:
- S3 ë²„í‚· ì‚­ì œ ë¶ˆê°€
- ë¹„ìš© ì§€ì† ë°œìƒ ($0.085/GB + ìš”ì²­ ë¹„ìš©)
- Route53 ë ˆì½”ë“œ ì¶©ëŒ

#### í•´ê²°
`destroy-with-cleanup.sh`ì— CloudFront ì •ë¦¬ ì¶”ê°€:

```bash
# 1. Distribution disable
CONFIG=$(aws cloudfront get-distribution-config --id "$dist_id" --output json)
ETAG=$(echo "$CONFIG" | jq -r '.ETag')
NEW_CONFIG=$(echo "$CONFIG" | jq '.DistributionConfig | .Enabled = false')

aws cloudfront update-distribution \
    --id "$dist_id" \
    --if-match "$ETAG" \
    --distribution-config "$NEW_CONFIG"

# 2. Disabled ìƒíƒœ ëŒ€ê¸° (2ë¶„)
sleep 120

# 3. Distribution ì‚­ì œ
FINAL_CONFIG=$(aws cloudfront get-distribution-config --id "$dist_id" --output json)
FINAL_ETAG=$(echo "$FINAL_CONFIG" | jq -r '.ETag')

aws cloudfront delete-distribution --id "$dist_id" --if-match "$FINAL_ETAG"
```

**ì†Œìš” ì‹œê°„**: 5-10ë¶„

---

### 6.3. CloudFront ê²€ìƒ‰ ë¡œì§ ë¶€ì¡±ìœ¼ë¡œ ì¸í•œ ACM Certificate ì‚­ì œ ì‹¤íŒ¨

#### ë¬¸ì œ
```
ğŸ” ACM Certificate ì •ë¦¬ (us-east-1)...
âš ï¸  ACM Certificate ë°œê²¬:
  - ë„ë©”ì¸: images.growbin.app
    ARN: arn:aws:acm:us-east-1:...:certificate/...
    âš ï¸  Certificateê°€ ì•„ì§ ì‚¬ìš© ì¤‘ì…ë‹ˆë‹¤:
       - arn:aws:cloudfront::...:distribution/E1GGDPUBLRQG59
    â³ CloudFront ì™„ì „ ì‚­ì œ ëŒ€ê¸° ì¤‘ (ìµœëŒ€ 10ë¶„)...
       â³ ëŒ€ê¸° ì¤‘... (120ì´ˆ ê²½ê³¼)
       ... (ê³„ì† ëŒ€ê¸°)
```

**ì›ì¸**: CloudFront Distributionì´ ê²€ìƒ‰ë˜ì§€ ì•Šì•„ ì‚­ì œë˜ì§€ ì•ŠìŒ

**ê·¼ë³¸ ì›ì¸**:
```bash
# ê¸°ì¡´ ê²€ìƒ‰ ì¿¼ë¦¬ (ë¬¸ì œ)
CF_DISTRIBUTIONS=$(aws cloudfront list-distributions \
    --query "DistributionList.Items[?contains(Origins.Items[].DomainName, 'sesacthon-images')].Id" \
    --output text)
```

- S3 ë²„í‚· ì´ë¦„(`sesacthon-images`)ë§Œ ê²€ìƒ‰
- ACM Certificateë¥¼ ì‚¬ìš©í•˜ëŠ” ë‹¤ë¥¸ Originì˜ Distributionì€ ê²€ìƒ‰ ì•ˆ ë¨
- ê²°ê³¼: CloudFrontê°€ Enable ìƒíƒœë¡œ ë‚¨ì•„ìˆìŒ â†’ ACM Certificate ì‚­ì œ ë¶ˆê°€

---

#### ì§„ë‹¨

**CloudFront Distribution ìƒíƒœ í™•ì¸**:
```bash
# Distribution ID í™•ì¸
aws cloudfront list-distributions \
    --query "DistributionList.Items[*].{Id:Id,Status:Status,Enabled:DistributionConfig.Enabled,Aliases:Aliases.Items}" \
    --output table

# íŠ¹ì • Distribution ìƒì„¸ í™•ì¸
aws cloudfront get-distribution --id E1GGDPUBLRQG59 \
    --query 'Distribution.{Status:Status,Enabled:DistributionConfig.Enabled,DomainName:DomainName}' \
    --output json
```

**ì˜ˆìƒ ê²°ê³¼ (ë¬¸ì œ ìƒí™©)**:
```json
{
    "Status": "Deployed",
    "Enabled": true,  // âš ï¸ ì—¬ì „íˆ í™œì„±í™” ìƒíƒœ
    "DomainName": "d3f4l2e8xigfr9.cloudfront.net"
}
```

---

#### í•´ê²° ë°©ë²• 1: ìŠ¤í¬ë¦½íŠ¸ ê°œì„  (ê¶Œì¥)

`destroy-with-cleanup.sh`ì˜ CloudFront ê²€ìƒ‰ ë¡œì§ ê°œì„ :

```bash
# 5-1. CloudFront Distribution í™•ì¸ ë° ì‚­ì œ
echo "ğŸŒ CloudFront Distribution í™•ì¸..."

# 1. S3 ë²„í‚· ê¸°ë°˜ ê²€ìƒ‰
CF_DISTRIBUTIONS_S3=$(aws cloudfront list-distributions \
    --query "DistributionList.Items[?contains(Origins.Items[].DomainName, 'sesacthon-images')].Id" \
    --output text 2>/dev/null || echo "")

# 2. ACM Certificate ê¸°ë°˜ ê²€ìƒ‰ (images. ë„ë©”ì¸)
CF_DISTRIBUTIONS_ACM=$(aws cloudfront list-distributions \
    --query "DistributionList.Items[?contains(to_string(ViewerCertificate.ACMCertificateArn), 'images') || contains(to_string(Aliases.Items), 'images')].Id" \
    --output text 2>/dev/null || echo "")

# 3. ì¤‘ë³µ ì œê±°í•˜ê³  ë³‘í•©
CF_DISTRIBUTIONS=$(echo "$CF_DISTRIBUTIONS_S3 $CF_DISTRIBUTIONS_ACM" | tr ' ' '\n' | sort -u | tr '\n' ' ')

if [ -n "$CF_DISTRIBUTIONS" ]; then
    echo "âš ï¸  CloudFront Distribution ë°œê²¬ (ì‚­ì œ ì‹œ 5-10ë¶„ ì†Œìš”):"
    for dist_id in $CF_DISTRIBUTIONS; do
        echo "  ğŸ“‹ Distribution ID: $dist_id"
        
        # Distribution Config ê°€ì ¸ì˜¤ê¸°
        CONFIG=$(aws cloudfront get-distribution-config --id "$dist_id" --output json 2>/dev/null || echo "")
        
        if [ -n "$CONFIG" ] && [ "$CONFIG" != "" ]; then
            ETAG=$(echo "$CONFIG" | jq -r '.ETag' 2>/dev/null || echo "")
            IS_ENABLED=$(echo "$CONFIG" | jq -r '.DistributionConfig.Enabled' 2>/dev/null || echo "true")
            
            if [ "$IS_ENABLED" = "true" ]; then
                echo "  - Disabling Distribution: $dist_id"
                
                # Enabledë¥¼ falseë¡œ ë³€ê²½
                NEW_CONFIG=$(echo "$CONFIG" | jq '.DistributionConfig | .Enabled = false' 2>/dev/null)
                
                if [ -n "$NEW_CONFIG" ] && [ -n "$ETAG" ]; then
                    aws cloudfront update-distribution \
                        --id "$dist_id" \
                        --if-match "$ETAG" \
                        --distribution-config "$NEW_CONFIG" \
                        >/dev/null 2>&1 || true
                    
                    echo "  â³ Distribution Disabled ìƒíƒœ ëŒ€ê¸° (2ë¶„)..."
                    sleep 120
                fi
            fi
            
            # ì‚­ì œ
            echo "  - Deleting Distribution: $dist_id"
            FINAL_CONFIG=$(aws cloudfront get-distribution-config --id "$dist_id" --output json 2>/dev/null || echo "")
            FINAL_ETAG=$(echo "$FINAL_CONFIG" | jq -r '.ETag' 2>/dev/null || echo "")
            
            if [ -n "$FINAL_ETAG" ]; then
                aws cloudfront delete-distribution --id "$dist_id" --if-match "$FINAL_ETAG" 2>/dev/null || \
                    echo "    âš ï¸  ì‚­ì œ ì‹¤íŒ¨ (ì•„ì§ ë°°í¬ ì¤‘ì´ê±°ë‚˜ ì‚¬ìš© ì¤‘)"
            fi
        fi
    done
else
    echo "  âœ… CloudFront Distribution ì—†ìŒ"
fi
```

**ê°œì„  í¬ì¸íŠ¸**:
- âœ… S3 ë²„í‚· ì´ë¦„ ê¸°ë°˜ ê²€ìƒ‰
- âœ… ACM Certificate ARN ê¸°ë°˜ ê²€ìƒ‰ (ìƒˆë¡œ ì¶”ê°€)
- âœ… Aliases(CNAME) ê¸°ë°˜ ê²€ìƒ‰ (ìƒˆë¡œ ì¶”ê°€)
- âœ… ì¤‘ë³µ ì œê±° ë° ì¼ê´„ ì²˜ë¦¬

---

#### í•´ê²° ë°©ë²• 2: ìˆ˜ë™ í•´ê²° (ì¦‰ì‹œ í•„ìš” ì‹œ)

í˜„ì¬ ìŠ¤í¬ë¦½íŠ¸ë¥¼ ì¤‘ë‹¨(Ctrl+C)í•˜ê³  ìˆ˜ë™ìœ¼ë¡œ ì²˜ë¦¬:

```bash
# 1. CloudFront Distribution ë¹„í™œì„±í™”
DIST_ID="E1GGDPUBLRQG59"  # ì‹¤ì œ Distribution ID ì…ë ¥

CONFIG=$(aws cloudfront get-distribution-config --id "$DIST_ID" --output json)
ETAG=$(echo "$CONFIG" | jq -r '.ETag')
NEW_CONFIG=$(echo "$CONFIG" | jq '.DistributionConfig | .Enabled = false')

aws cloudfront update-distribution \
    --id "$DIST_ID" \
    --if-match "$ETAG" \
    --distribution-config "$NEW_CONFIG"

# 2. Disabled ìƒíƒœ ëŒ€ê¸° (2-5ë¶„)
echo "â³ CloudFront Disabled ëŒ€ê¸° ì¤‘..."
sleep 180

# 3. CloudFront Distribution ì‚­ì œ
FINAL_CONFIG=$(aws cloudfront get-distribution-config --id "$DIST_ID" --output json)
FINAL_ETAG=$(echo "$FINAL_CONFIG" | jq -r '.ETag')

aws cloudfront delete-distribution \
    --id "$DIST_ID" \
    --if-match "$FINAL_ETAG"

# 4. ACM Certificate ì‚­ì œ (CloudFront ì‚­ì œ ì™„ë£Œ í›„ 5ë¶„ ëŒ€ê¸°)
echo "â³ CloudFront ì™„ì „ ì‚­ì œ ëŒ€ê¸° ì¤‘..."
sleep 300

aws acm delete-certificate \
    --certificate-arn "arn:aws:acm:us-east-1:721622471953:certificate/b34e6013-babe-4495-88f6-77f4d9bdd39f" \
    --region us-east-1
```

---

#### ì˜ˆë°© ë°©ë²•

**ë°°í¬ ì‹œ íƒœê·¸ ì¶”ê°€**:
```hcl
# terraform/cloudfront.tf
resource "aws_cloudfront_distribution" "images" {
  # ... ê¸°ì¡´ ì„¤ì • ...
  
  tags = {
    Name        = "sesacthon-images-cdn"
    Project     = "SeSACTHON"
    ManagedBy   = "Terraform"
    Environment = var.environment
    # ì‚­ì œ ìŠ¤í¬ë¦½íŠ¸ ê²€ìƒ‰ìš©
    SearchKey   = "sesacthon-cleanup"  # â† ì¶”ê°€
  }
}
```

**ìŠ¤í¬ë¦½íŠ¸ì—ì„œ íƒœê·¸ ê¸°ë°˜ ê²€ìƒ‰**:
```bash
# íƒœê·¸ ê¸°ë°˜ ê²€ìƒ‰ (ê°€ì¥ í™•ì‹¤í•¨)
CF_DISTRIBUTIONS_TAG=$(aws cloudfront list-distributions \
    --query "DistributionList.Items[?Tags.Items[?Key=='SearchKey' && Value=='sesacthon-cleanup']].Id" \
    --output text 2>/dev/null || echo "")
```

---

#### ë””ë²„ê¹… ëª…ë ¹ì–´

```bash
# 1. ëª¨ë“  CloudFront Distribution ëª©ë¡
aws cloudfront list-distributions \
    --query "DistributionList.Items[*].{Id:Id,DomainName:DomainName,Status:Status,Enabled:DistributionConfig.Enabled}" \
    --output table

# 2. ACM Certificate ì‚¬ìš© ì—¬ë¶€ í™•ì¸
aws acm describe-certificate \
    --certificate-arn "arn:aws:acm:us-east-1:...:certificate/..." \
    --region us-east-1 \
    --query 'Certificate.{InUseBy:InUseBy,Status:Status}' \
    --output json

# 3. Distributionì˜ Origin í™•ì¸
aws cloudfront get-distribution --id E1GGDPUBLRQG59 \
    --query 'Distribution.DistributionConfig.Origins.Items[*].DomainName' \
    --output json

# 4. Distributionì˜ Certificate í™•ì¸
aws cloudfront get-distribution --id E1GGDPUBLRQG59 \
    --query 'Distribution.DistributionConfig.ViewerCertificate' \
    --output json
```

---

#### ê´€ë ¨ ì»¤ë°‹

- `fix: Improve CloudFront detection logic to include ACM Certificate-based search`
- `fix: Add multiple search strategies for CloudFront Distribution cleanup`

---

## 7. VPC ì‚­ì œ ì§€ì—° ë¬¸ì œ

### 7.1. VPC ì‚­ì œê°€ 15ë¶„ ì´ìƒ ì†Œìš”ë˜ëŠ” ë¬¸ì œ

#### í˜„ìƒ
```
module.vpc.aws_vpc.main: Still destroying... [id=vpc-02562955fe60907d8, 15m30s elapsed]
aws_acm_certificate.cdn: Still destroying... [id=arn:aws:acm:..., 15m30s elapsed]
```

**ì›ì¸**: AWS ë¦¬ì†ŒìŠ¤ê°€ ì™„ì „íˆ ì‚­ì œë˜ì§€ ì•Šì€ ìƒíƒœì—ì„œ VPC ì‚­ì œ ì‹œë„

#### ì£¼ìš” ì›ì¸ ë¶„ì„

##### 1. NAT Gateway ì‚­ì œ ì§€ì—° (ê°€ì¥ í° ì›ì¸)

**ë¬¸ì œ**:
- NAT GatewayëŠ” ì‚­ì œ ì‹œ **3-5ë¶„** ì†Œìš”
- ê¸°ì¡´ ìŠ¤í¬ë¦½íŠ¸ëŠ” 30ì´ˆë§Œ ëŒ€ê¸° í›„ ë‹¤ìŒ ë‹¨ê³„ë¡œ ì§„í–‰
- NAT Gatewayê°€ ì™„ì „íˆ ì‚­ì œë˜ì§€ ì•Šìœ¼ë©´ VPC ì‚­ì œ ë¶ˆê°€

**í•´ê²°**:
```bash
# NAT Gateway ì™„ì „ ì‚­ì œ ëŒ€ê¸° (ìµœëŒ€ 5ë¶„)
MAX_NAT_WAIT=300  # 5ë¶„
NAT_WAIT_COUNT=0

while [ $NAT_WAIT_COUNT -lt $MAX_NAT_WAIT ]; do
    REMAINING_NATS=$(aws ec2 describe-nat-gateways \
        --filter "Name=vpc-id,Values=$VPC_ID" "Name=state,Values=available,pending,deleting" \
        --region "$AWS_REGION" \
        --query 'NatGateways[*].NatGatewayId' \
        --output text 2>/dev/null || echo "")
    
    if [ -z "$REMAINING_NATS" ]; then
        echo "âœ… ëª¨ë“  NAT Gateway ì‚­ì œ ì™„ë£Œ (${NAT_WAIT_COUNT}ì´ˆ ì†Œìš”)"
        break
    fi
    
    if [ $((NAT_WAIT_COUNT % 30)) -eq 0 ]; then
        echo "â³ ëŒ€ê¸° ì¤‘... (${NAT_WAIT_COUNT}ì´ˆ ê²½ê³¼)"
    fi
    
    sleep 5
    NAT_WAIT_COUNT=$((NAT_WAIT_COUNT + 5))
done
```

##### 2. ENI (Elastic Network Interface) ì‚­ì œ ì‹¤íŒ¨

**ë¬¸ì œ**:
- NAT Gatewayì™€ ì—°ê²°ëœ ENIê°€ ìë™ ì‚­ì œë˜ì§€ ì•ŠìŒ
- 5ì´ˆ ëŒ€ê¸° í›„ ì¬ì‹œë„í•˜ì§€ë§Œ ì—¬ì „íˆ ì‹¤íŒ¨ ê°€ëŠ¥
- ENIê°€ VPCì— ì—°ê²°ë˜ì–´ ìˆì–´ VPC ì‚­ì œ ë¶ˆê°€

**í•´ê²°**:
```bash
# ENIëŠ” NAT Gateway ì‚­ì œ í›„ì— ìë™ìœ¼ë¡œ í•´ì œë˜ë¯€ë¡œ ì—¬ëŸ¬ ë²ˆ ì¬ì‹œë„
MAX_ENI_RETRY=3
ENI_RETRY_COUNT=0

while [ $ENI_RETRY_COUNT -lt $MAX_ENI_RETRY ]; do
    ENI_IDS=$(aws ec2 describe-network-interfaces \
        --filters "Name=vpc-id,Values=$VPC_ID" \
        --region "$AWS_REGION" \
        --query 'NetworkInterfaces[?Status==`available`].NetworkInterfaceId' \
        --output text 2>/dev/null || echo "")
    
    if [ -z "$ENI_IDS" ]; then
        echo "âœ… ë‚¨ì€ ENI ì—†ìŒ"
        break
    fi
    
    echo "âš ï¸  ë‚¨ì€ ENI ë°œê²¬ (ì‹œë„ $((ENI_RETRY_COUNT + 1))/$MAX_ENI_RETRY):"
    
    for eni in $ENI_IDS; do
        aws ec2 delete-network-interface --network-interface-id "$eni" --region "$AWS_REGION" 2>/dev/null || true
    done
    
    ENI_RETRY_COUNT=$((ENI_RETRY_COUNT + 1))
    
    if [ $ENI_RETRY_COUNT -lt $MAX_ENI_RETRY ]; then
        echo "â³ 10ì´ˆ í›„ ì¬ì‹œë„..."
        sleep 10
    fi
done
```

##### 3. ACM Certificate ì‚­ì œ ì§€ì—°

**ë¬¸ì œ**:
- CloudFrontê°€ ACM Certificateë¥¼ ì‚¬ìš© ì¤‘ì´ë©´ ì‚­ì œ ë¶ˆê°€
- CloudFront disable í›„ 2ë¶„ë§Œ ëŒ€ê¸°í•˜ëŠ”ë°, ì‹¤ì œë¡œëŠ” 5-10ë¶„ ì†Œìš”
- Certificateê°€ ì‚­ì œë˜ì§€ ì•Šìœ¼ë©´ Terraform destroyê°€ ê³„ì† ëŒ€ê¸°

**í•´ê²°**:
```bash
# CloudFront ì‚¬ìš© ì—¬ë¶€ í™•ì¸
CERT_IN_USE=$(aws acm describe-certificate \
    --certificate-arn "$cert_arn" \
    --region us-east-1 \
    --query 'Certificate.InUseBy' \
    --output json 2>/dev/null || echo "[]")

IN_USE_COUNT=$(echo "$CERT_IN_USE" | jq '. | length' 2>/dev/null || echo "0")

if [ "$IN_USE_COUNT" -gt 0 ]; then
    echo "âš ï¸  Certificateê°€ ì•„ì§ ì‚¬ìš© ì¤‘ì…ë‹ˆë‹¤:"
    
    # CloudFront ì™„ì „ ì‚­ì œ ëŒ€ê¸° (ìµœëŒ€ 10ë¶„)
    MAX_CF_WAIT=600  # 10ë¶„
    CF_WAIT_COUNT=0
    
    while [ $CF_WAIT_COUNT -lt $MAX_CF_WAIT ]; do
        CURRENT_IN_USE=$(aws acm describe-certificate \
            --certificate-arn "$cert_arn" \
            --region us-east-1 \
            --query 'Certificate.InUseBy' \
            --output json 2>/dev/null || echo "[]")
        
        CURRENT_COUNT=$(echo "$CURRENT_IN_USE" | jq '. | length' 2>/dev/null || echo "0")
        
        if [ "$CURRENT_COUNT" -eq 0 ]; then
            echo "âœ… Certificate í•´ì œ ì™„ë£Œ (${CF_WAIT_COUNT}ì´ˆ ì†Œìš”)"
            break
        fi
        
        if [ $((CF_WAIT_COUNT % 30)) -eq 0 ]; then
            echo "â³ ëŒ€ê¸° ì¤‘... (${CF_WAIT_COUNT}ì´ˆ ê²½ê³¼)"
        fi
        
        sleep 10
        CF_WAIT_COUNT=$((CF_WAIT_COUNT + 10))
    done
fi
```

---

### 7.2. ë¦¬ì†ŒìŠ¤ ì‚­ì œ ìˆœì„œ ìµœì í™”

#### ê¶Œì¥ ìˆœì„œ

```
1. Kubernetes ë¦¬ì†ŒìŠ¤ ì‚­ì œ (Ingress, Service, PVC)
   â””â”€> ALB, Target Groups ìë™ ì‚­ì œ
   
2. AWS ë¦¬ì†ŒìŠ¤ í™•ì¸ ë° ì‚­ì œ
   â”œâ”€> Load Balancer ì‚­ì œ (ìµœëŒ€ 60ì´ˆ ëŒ€ê¸°)
   â”œâ”€> Security Groups ì‚­ì œ
   â”œâ”€> ENI ì‚­ì œ (ì¬ì‹œë„ 3íšŒ)
   â”œâ”€> Target Groups ì‚­ì œ
   â”œâ”€> CloudFront Distribution ì‚­ì œ (ìµœëŒ€ 10ë¶„ ëŒ€ê¸°)
   â”œâ”€> Route53 ë ˆì½”ë“œ ì‚­ì œ
   â”œâ”€> S3 Bucket ì •ë¦¬
   â”œâ”€> ACM Certificate ì‚­ì œ (CloudFront ëŒ€ê¸° ìµœëŒ€ 10ë¶„)
   â””â”€> IAM Policy ê°•ì œ ì •ë¦¬

3. NAT Gateway ì‚­ì œ (ìµœëŒ€ 5ë¶„ ëŒ€ê¸°) â­ ì¤‘ìš”!
   â””â”€> ì™„ì „ ì‚­ì œ í™•ì¸ í›„ ë‹¤ìŒ ë‹¨ê³„ ì§„í–‰

4. VPC Endpoints ì‚­ì œ
   VPC Peering Connections ì‚­ì œ
   Elastic IP í•´ì œ

5. Terraform destroy ì‹¤í–‰
   â””â”€> VPC ë° ë‚˜ë¨¸ì§€ ë¦¬ì†ŒìŠ¤ ì‚­ì œ
```

---

### 7.3. ìˆ˜ë™ ë””ë²„ê¹… ëª…ë ¹ì–´

#### VPC ì‚­ì œë¥¼ ë§‰ëŠ” ë¦¬ì†ŒìŠ¤ í™•ì¸

```bash
# 1. NAT Gateway ìƒíƒœ í™•ì¸
aws ec2 describe-nat-gateways \
    --filter "Name=vpc-id,Values=$VPC_ID" \
    --region ap-northeast-2 \
    --query 'NatGateways[*].[NatGatewayId,State]' \
    --output table

# 2. ENI í™•ì¸
aws ec2 describe-network-interfaces \
    --filters "Name=vpc-id,Values=$VPC_ID" \
    --region ap-northeast-2 \
    --query 'NetworkInterfaces[*].[NetworkInterfaceId,Status,Description]' \
    --output table

# 3. Security Groups í™•ì¸
aws ec2 describe-security-groups \
    --filters "Name=vpc-id,Values=$VPC_ID" \
    --region ap-northeast-2 \
    --query 'SecurityGroups[*].[GroupId,GroupName]' \
    --output table

# 4. Subnets í™•ì¸
aws ec2 describe-subnets \
    --filters "Name=vpc-id,Values=$VPC_ID" \
    --region ap-northeast-2 \
    --query 'Subnets[*].[SubnetId,AvailabilityZone,CidrBlock]' \
    --output table

# 5. Internet Gateway í™•ì¸
aws ec2 describe-internet-gateways \
    --filters "Name=attachment.vpc-id,Values=$VPC_ID" \
    --region ap-northeast-2 \
    --query 'InternetGateways[*].[InternetGatewayId]' \
    --output table

# 6. VPC Endpoints í™•ì¸
aws ec2 describe-vpc-endpoints \
    --filters "Name=vpc-id,Values=$VPC_ID" \
    --region ap-northeast-2 \
    --query 'VpcEndpoints[*].[VpcEndpointId,ServiceName,State]' \
    --output table

# 7. ACM Certificate ì‚¬ìš© ì—¬ë¶€ í™•ì¸
aws acm describe-certificate \
    --certificate-arn "$CERT_ARN" \
    --region us-east-1 \
    --query 'Certificate.InUseBy' \
    --output json
```

#### ìˆ˜ë™ ë¦¬ì†ŒìŠ¤ ì‚­ì œ

```bash
# NAT Gateway ê°•ì œ ì‚­ì œ
NAT_GW_ID="nat-xxxxx"
aws ec2 delete-nat-gateway --nat-gateway-id "$NAT_GW_ID" --region ap-northeast-2

# ENI ê°•ì œ ì‚­ì œ
ENI_ID="eni-xxxxx"
aws ec2 delete-network-interface --network-interface-id "$ENI_ID" --region ap-northeast-2

# VPC ì§ì ‘ ì‚­ì œ ì‹œë„
VPC_ID="vpc-xxxxx"
aws ec2 delete-vpc --vpc-id "$VPC_ID" --region ap-northeast-2
```

---

### 7.4. ì˜ˆìƒ ì†Œìš” ì‹œê°„

| ë‹¨ê³„ | ì†Œìš” ì‹œê°„ | ì„¤ëª… |
|-----|----------|------|
| Kubernetes ë¦¬ì†ŒìŠ¤ ì •ë¦¬ | 30ì´ˆ-1ë¶„ | Ingress, Service, PVC ì‚­ì œ |
| Load Balancer ì‚­ì œ | 1-2ë¶„ | ALB ì™„ì „ ì‚­ì œ ëŒ€ê¸° |
| CloudFront ì‚­ì œ | 5-10ë¶„ | Distribution disable + ì‚­ì œ |
| ACM Certificate ëŒ€ê¸° | 5-10ë¶„ | CloudFront í•´ì œ ëŒ€ê¸° |
| **NAT Gateway ì‚­ì œ** | **3-5ë¶„** | â­ ê°€ì¥ ì‹œê°„ ì†Œìš” |
| Terraform destroy | 2-3ë¶„ | ë‚˜ë¨¸ì§€ ë¦¬ì†ŒìŠ¤ ì‚­ì œ |
| **ì´ ì˜ˆìƒ ì‹œê°„** | **15-30ë¶„** | ì •ìƒ ë²”ìœ„ |

---

### 7.5. ê°œì„  ê²°ê³¼

#### ê¸°ì¡´ ë¬¸ì œ
- NAT Gateway: 30ì´ˆë§Œ ëŒ€ê¸° â†’ ì™„ì „ ì‚­ì œë˜ì§€ ì•ŠìŒ
- ENI: 1íšŒ ì¬ì‹œë„ â†’ ì‚­ì œ ì‹¤íŒ¨
- ACM Certificate: ëŒ€ê¸° ì—†ìŒ â†’ Terraformì´ ê³„ì† ëŒ€ê¸°

#### ê°œì„  í›„
- âœ… NAT Gateway: ìµœëŒ€ 5ë¶„ ëŒ€ê¸° â†’ ì™„ì „ ì‚­ì œ í™•ì¸
- âœ… ENI: 3íšŒ ì¬ì‹œë„ (10ì´ˆ ê°„ê²©) â†’ ì‚­ì œ ì„±ê³µë¥  í–¥ìƒ
- âœ… ACM Certificate: ìµœëŒ€ 10ë¶„ ëŒ€ê¸° â†’ CloudFront í•´ì œ í™•ì¸ í›„ ì‚­ì œ

**ê²°ê³¼**: VPC ì‚­ì œ ì§€ì—° ë¬¸ì œ í•´ê²° ë° ì•ˆì •ì ì¸ cleanup ê°€ëŠ¥

---

### 7.6. ê´€ë ¨ ì»¤ë°‹

- `fix: Add comprehensive NAT Gateway deletion wait in destroy-with-cleanup.sh`
- `fix: Improve ENI deletion with retry mechanism`
- `fix: Add CloudFront deletion wait for ACM Certificate cleanup`

---

## 8. ArgoCD ë¦¬ë””ë ‰ì…˜ ë£¨í”„ ë¬¸ì œ

### ë¬¸ì œ
ë¸Œë¼ìš°ì €ì—ì„œ `https://argocd.growbin.app` ì ‘ì† ì‹œ "ë¦¬ë””ë ‰ì…˜í•œ íšŸìˆ˜ê°€ ë„ˆë¬´ ë§ìŠµë‹ˆë‹¤" ì—ëŸ¬ ë°œìƒ.

### ì›ì¸
- Ingressê°€ í¬íŠ¸ 443ì„ ì‚¬ìš©í•˜ì§€ë§Œ `backend-protocol: HTTP`ê°€ ì„¤ì •ë˜ì§€ ì•ŠìŒ
- ALBê°€ HTTPSë¡œ ì—°ê²° ì‹œë„í•˜ì§€ë§Œ ArgoCDëŠ” HTTPë§Œ ì§€ì›
- Health Check ì‹¤íŒ¨ë¡œ Target Groupì´ unhealthy ìƒíƒœ

### í•´ê²° (2025-11-16 ì—…ë°ì´íŠ¸)
1. **HTTPS â†’ HTTP NAT ëª…ì‹œ**  
   - ëª¨ë“  ALB Ingressì— `alb.ingress.kubernetes.io/backend-protocol: HTTP` ì¶”ê°€  
   - ì°¸ê³ : [AWS Load Balancer Controller ê³µì‹ ê°€ì´ë“œ](https://kubernetes-sigs.github.io/aws-load-balancer-controller/v2.7/guide/ingress/annotations/#backend-protocol)
2. **ALB Listener ë‹¨ì¼í™”**  
   - `alb.ingress.kubernetes.io/listen-ports: '[{"HTTPS": 443}]'` ë¡œ ê³ ì •  
   - `alb.ingress.kubernetes.io/ssl-redirect` ì œê±° (ì¤‘ë³µ ë¦¬ë‹¤ì´ë ‰ì…˜ ë°©ì§€)
3. **ArgoCD Service ì •ë¦¬**  
   - NodePort ì„œë¹„ìŠ¤ì—ì„œ 443 í¬íŠ¸ë¥¼ ì œê±°í•˜ê³  HTTP(80)ë§Œ ë…¸ì¶œ  
   - ALBê°€ TLS ì¢…ë£Œ â†’ ë°±ì—”ë“œ HTTP ì „ë‹¬ (AWS ë² ìŠ¤íŠ¸ í”„ë™í‹°ìŠ¤)
4. **Health Check ì¼ì›í™”**  
   - `/healthz`(ArgoCD)ì™€ `/api/health`(Grafana) ë“± ì‹¤ì œ ì—”ë“œí¬ì¸íŠ¸ë¡œ í†µì¼

**ì ìš© íŒŒì¼**
- `k8s/ingress/domain-based-api-ingress.yaml`
- `k8s/ingress/infrastructure-ingress.yaml`
- `ansible/roles/argocd/tasks/main.yml`
- `ansible/playbooks/07-ingress-resources.yml`

> ğŸ’¡ HTTPSâ†’HTTP NAT êµ¬ì„±ì€ ALBì—ì„œ TLSë¥¼ ì¢…ë£Œí•˜ê³ , Target Groupì—ëŠ” HTTPë§Œ ì „ë‹¬í•˜ëŠ” ê³µì‹ ê¶Œì¥ êµ¬ì„±ì´ë¯€ë¡œ ë³„ë„ í”„ë¡ì‹œ/ì¬ì•”í˜¸í™”ê°€ í•„ìš” ì—†ë‹¤.

---

## 9. Prometheus ë©”ëª¨ë¦¬ ë¶€ì¡± ë¬¸ì œ

### ë¬¸ì œ
Prometheus Podê°€ `Pending` ìƒíƒœë¡œ ìŠ¤ì¼€ì¤„ë§ë˜ì§€ ì•ŠìŒ.

### ì›ì¸
- Prometheusê°€ 2Gi ë©”ëª¨ë¦¬ë¥¼ ìš”ì²­
- Monitoring ë…¸ë“œ(t3.small, 2GB RAM)ì˜ Allocatable ë©”ëª¨ë¦¬ê°€ ë¶€ì¡±
- ì—ëŸ¬: `9 Insufficient memory`

### í•´ê²°
1. **ì˜µì…˜ 1 (ê¶Œì¥):** Prometheus ë¦¬ì†ŒìŠ¤ ìš”ì²­ì„ 2Gi â†’ 1.5Gië¡œ ê°ì†Œ
2. **ì˜µì…˜ 2:** Monitoring ë…¸ë“œ ì¸ìŠ¤í„´ìŠ¤ íƒ€ì…ì„ t3.medium(4GB RAM)ìœ¼ë¡œ ì—…ê·¸ë ˆì´ë“œ
3. **ì˜µì…˜ 3:** Prometheusë¥¼ ë‹¤ë¥¸ ë…¸ë“œë¡œ ìŠ¤ì¼€ì¤„ë§

**ìì„¸í•œ ë‚´ìš©:** [PROMETHEUS_MEMORY_INSUFFICIENT.md](./troubleshooting/PROMETHEUS_MEMORY_INSUFFICIENT.md)

---

## 10. Atlantis Pod CrashLoopBackOff ë¬¸ì œ

### ë¬¸ì œ
Atlantis Podê°€ `CrashLoopBackOff` ìƒíƒœë¡œ ê³„ì† ì¬ì‹œì‘ë¨.

### ì›ì¸
1. **í¬íŠ¸ íŒŒì‹± ì—ëŸ¬**: Atlantisê°€ í™˜ê²½ ë³€ìˆ˜ì—ì„œ í¬íŠ¸ë¥¼ íŒŒì‹±í•  ë•Œ Serviceì˜ ClusterIP í˜•ì‹ì„ í¬íŠ¸ë¡œ ì¸ì‹
2. **ê¶Œí•œ ë¬¸ì œ**: PersistentVolumeì— ëŒ€í•œ ì“°ê¸° ê¶Œí•œ ì—†ìŒ (`fsGroup`, `runAsUser` ì„¤ì • ëˆ„ë½)

### í•´ê²°
1. í¬íŠ¸ ëª…ì‹œì  ì„¤ì • (`--port=4141`)
2. SecurityContext ì¶”ê°€ (`fsGroup: 1000`, `runAsUser: 1000`, `runAsGroup: 1000`)

**ìì„¸í•œ ë‚´ìš©:** [ATLANTIS_POD_CRASHLOOPBACKOFF.md](./troubleshooting/ATLANTIS_POD_CRASHLOOPBACKOFF.md)

---

## 11. Atlantis Podì—ì„œ kubectlì„ ì°¾ì„ ìˆ˜ ì—†ëŠ” ë¬¸ì œ

### ë¬¸ì œ
Atlantis Podì—ì„œ kubectlì„ ì‹¤í–‰í•  ë•Œ `executable file not found in $PATH` ì—ëŸ¬ ë°œìƒ.

### ì›ì¸
Init Containerì—ì„œ kubectlì„ ì„¤ì¹˜í–ˆì§€ë§Œ, Main Containerì—ì„œ ì˜¬ë°”ë¥¸ ê²½ë¡œë¡œ ë§ˆìš´íŠ¸ë˜ì§€ ì•ŠìŒ.

### í•´ê²°
1. Init Containerì—ì„œ `/shared/usr/local/bin/kubectl`ì— ë³µì‚¬
2. Main Containerì—ì„œ `/shared/usr/local/bin`ì„ `/usr/local/bin`ì— subPathë¡œ ë§ˆìš´íŠ¸
3. PATH í™˜ê²½ ë³€ìˆ˜ì— `/usr/local/bin` ì¶”ê°€

**ìì„¸í•œ ë‚´ìš©:** [ATLANTIS_KUBECTL_NOT_FOUND.md](./troubleshooting/ATLANTIS_KUBECTL_NOT_FOUND.md)

---

## 12. Atlantis Deployment íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ëŠ” ë¬¸ì œ

### ë¬¸ì œ
Master ë…¸ë“œì—ì„œ `kubectl apply -f k8s/atlantis/atlantis-deployment.yaml` ì‹¤í–‰ ì‹œ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŒ.

### ì›ì¸
Master ë…¸ë“œì—ëŠ” Git ì €ì¥ì†Œê°€ ì—†ìŒ. AtlantisëŠ” Ansibleì„ í†µí•´ ë°°í¬ë˜ë©°, íŒŒì¼ì€ ë¡œì»¬ ê°œë°œ í™˜ê²½ì—ë§Œ ì¡´ì¬.

### í•´ê²°
ë¡œì»¬ì—ì„œ Ansibleì„ ì‹¤í–‰í•˜ì—¬ ì¬ë°°í¬:
```bash
ansible-playbook -i ansible/inventory/hosts.ini ansible/playbooks/09-atlantis.yml
```

**ìì„¸í•œ ë‚´ìš©:** [ATLANTIS_DEPLOYMENT_FILE_NOT_FOUND.md](./troubleshooting/ATLANTIS_DEPLOYMENT_FILE_NOT_FOUND.md)

---

## 13. Atlantis ì‹¤í–‰ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ëŠ” ë¬¸ì œ

### ë¬¸ì œ
Atlantis Podê°€ ì‹œì‘ë˜ì§€ ì•Šê³  ë‹¤ìŒ ì—ëŸ¬ ë°œìƒ:
```
exec: "atlantis": executable file not found in $PATH: unknown
```

### ì›ì¸
`command: ["atlantis"]`ë¡œ ì§€ì •í–ˆì§€ë§Œ, ì»¨í…Œì´ë„ˆ ë‚´ë¶€ì—ì„œ `atlantis` ì‹¤í–‰ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŒ. Atlantis ì´ë¯¸ì§€ëŠ” ì´ë¯¸ ENTRYPOINTê°€ ì„¤ì •ë˜ì–´ ìˆì–´ì„œ `command`ë¥¼ ì§€ì •í•  í•„ìš”ê°€ ì—†ìŒ.

### í•´ê²°
`command`ë¥¼ ì œê±°í•˜ê³  ì´ë¯¸ì§€ì˜ ê¸°ë³¸ ENTRYPOINTë¥¼ ì‚¬ìš©:
```yaml
# ìˆ˜ì • ì „
command: ["atlantis"]
args:
  - server
  # ...

# ìˆ˜ì • í›„
# commandëŠ” ì œê±° (ì´ë¯¸ì§€ì˜ ê¸°ë³¸ ENTRYPOINT ì‚¬ìš©)
args:
  - server
  # ...
```

**ìì„¸í•œ ë‚´ìš©:** [ATLANTIS_EXECUTABLE_NOT_FOUND.md](./troubleshooting/ATLANTIS_EXECUTABLE_NOT_FOUND.md)

---

## 14. Atlantis ConfigMap YAML íŒŒì‹± ì—ëŸ¬

### 14.1. ì¦ìƒ
```
Error: initializing server: parsing /etc/atlantis/atlantis.yaml file: yaml: unmarshal errors:
  line 1: field version not found in type raw.GlobalCfg
  line 2: field automerge not found in type raw.GlobalCfg
  line 3: field delete_source_branch_on_merge not found in type raw.GlobalCfg
  line 4: field parallel_plan not found in type raw.GlobalCfg
  line 5: field parallel_apply not found in type raw.GlobalCfg
  line 7: field projects not found in type raw.GlobalCfg
```

Atlantis Podê°€ `CrashLoopBackOff` ìƒíƒœ

### 14.2. ì›ì¸
**Atlantis Config íŒŒì¼ì˜ ë‘ ê°€ì§€ íƒ€ì… í˜¼ë™**

1. **Repo-level Config** (`.atlantis.yaml` in repository)
   - `version`, `automerge`, `projects`, `workflows` ë“±ì„ ì§ì ‘ ì •ì˜
   
2. **Server-side Repo Config** (`ATLANTIS_REPO_CONFIG` í™˜ê²½ë³€ìˆ˜)
   - `repos`ì™€ `workflows` ë‘ ì„¹ì…˜ìœ¼ë¡œ êµ¬ì„±

í˜„ì¬ëŠ” **Repo-level Config í˜•ì‹**ì„ **Server-side Config**ë¡œ ì‚¬ìš©í•˜ë ¤ê³  í•´ì„œ íŒŒì‹± ì—ëŸ¬ ë°œìƒ

### 14.3. í•´ê²° ë°©ë²•

#### Master ë…¸ë“œì—ì„œ ì‹¤í–‰:
```bash
# ê¸°ì¡´ ConfigMap ì‚­ì œ
kubectl delete configmap atlantis-repo-config -n atlantis

# ì˜¬ë°”ë¥¸ í˜•ì‹ìœ¼ë¡œ ì¬ìƒì„±
cat <<EOF | kubectl apply -f -
apiVersion: v1
kind: ConfigMap
metadata:
  name: atlantis-repo-config
  namespace: atlantis
data:
  atlantis.yaml: |
    # Repositories Configuration
    repos:
    - id: github.com/SeSACTHON/*
      workflow: infrastructure-workflow
      allowed_overrides:
        - workflow
        - apply_requirements
      allow_custom_workflows: true
      delete_source_branch_on_merge: true
    
    # Workflows Configuration
    workflows:
      infrastructure-workflow:
        plan:
          steps:
            - run: echo "ğŸ” Terraform Plan ì‹œì‘..."
            - init
            - plan
        apply:
          steps:
            - run: echo "ğŸš€ Terraform Apply ì‹œì‘..."
            - apply
            - run: echo "âœ… Terraform Apply ì™„ë£Œ"
EOF

# Pod ì¬ì‹œì‘
kubectl delete pod atlantis-0 -n atlantis
```

#### ë˜ëŠ” ìë™ ìŠ¤í¬ë¦½íŠ¸:
```bash
./scripts/utilities/fix-atlantis-config.sh
```

### 14.4. ì ìš©ëœ ìˆ˜ì •ì‚¬í•­
- `ansible/playbooks/09-atlantis.yml`: Server-side Repo Config ìƒì„± Task ì¶”ê°€
- `scripts/utilities/fix-atlantis-config.sh`: ConfigMap ìˆ˜ì • ìŠ¤í¬ë¦½íŠ¸ ìƒì„±

**ìì„¸í•œ ë‚´ìš©:** [ATLANTIS_CONFIG_YAML_PARSE_ERROR.md](./troubleshooting/ATLANTIS_CONFIG_YAML_PARSE_ERROR.md)

---

## 15. ë² ìŠ¤íŠ¸ í”„ë™í‹°ìŠ¤

### 15.1. ì¬êµ¬ì¶• ì „ ì²´í¬ë¦¬ìŠ¤íŠ¸

```bash
# 1. vCPU í•œë„ í™•ì¸
aws service-quotas get-service-quota \
    --service-code ec2 \
    --quota-code L-1216C47A \
    --region ap-northeast-2 \
    --query 'Quota.Value'
# ê²°ê³¼: 32.0 ì´ìƒì´ì–´ì•¼ í•¨

# 2. ì´ì „ ë¦¬ì†ŒìŠ¤ ì™„ì „ ì •ë¦¬
./scripts/maintenance/destroy-with-cleanup.sh

# 3. í™˜ê²½ ë³€ìˆ˜ ì„¤ì •
export GITHUB_TOKEN="<your-token>"
export GITHUB_USERNAME="<your-username>"
export VERSION="v0.6.0"

# 4. ì¬êµ¬ì¶• ì‹¤í–‰
./scripts/cluster/auto-rebuild.sh
```

---

### 15.2. ë””ë²„ê¹… ëª…ë ¹ì–´ ëª¨ìŒ

```bash
# Terraform ìƒíƒœ í™•ì¸
terraform state list
terraform output -json

# AWS ë¦¬ì†ŒìŠ¤ í™•ì¸
aws ec2 describe-instances --region ap-northeast-2
aws iam list-policies --scope Local
aws s3api list-buckets

# Kubernetes ìƒíƒœ í™•ì¸
kubectl get nodes -o wide
kubectl get pods -A
kubectl get svc -A

# GitHub ì¸ì¦ ìƒíƒœ
gh auth status

# ë¡œê·¸ í™•ì¸
tail -f /var/log/cloud-init-output.log  # Master ë…¸ë“œì—ì„œ
journalctl -u kubelet -f                 # Worker ë…¸ë“œì—ì„œ
```

---

### 15.3. ë¬¸ì œ ë°œìƒ ì‹œ ëŒ€ì‘ ìˆœì„œ

1. **ì—ëŸ¬ ë©”ì‹œì§€ í™•ì¸**: ì •í™•í•œ ì—ëŸ¬ ë‚´ìš© íŒŒì•…
2. **ì´ ë¬¸ì„œ ê²€ìƒ‰**: ìœ ì‚¬í•œ ë¬¸ì œ í•´ê²° ë°©ë²• í™•ì¸
3. **AWS ìƒíƒœ í™•ì¸**: ë¦¬ì†ŒìŠ¤ê°€ ì‹¤ì œë¡œ ë‚¨ì•„ìˆëŠ”ì§€ í™•ì¸
4. **ì •ë¦¬ ìŠ¤í¬ë¦½íŠ¸ ì‹¤í–‰**: `destroy-with-cleanup.sh`
5. **í•œë„ í™•ì¸**: vCPU, IAM Policy ë“±
6. **ì¬ì‹œë„**: ì •ë¦¬ í›„ ì¬êµ¬ì¶•

---

## 16. ì°¸ê³  ë¬¸ì„œ

- [AWS Service Quotas Documentation](https://docs.aws.amazon.com/servicequotas/)
- [Terraform AWS Provider](https://registry.terraform.io/providers/hashicorp/aws/latest/docs)
- [GitHub CLI Authentication](https://cli.github.com/manual/gh_auth_login)
- [CloudFront Developer Guide](https://docs.aws.amazon.com/cloudfront/)

---

## 17. ì§€ì›

ë¬¸ì œê°€ í•´ê²°ë˜ì§€ ì•Šìœ¼ë©´:
- GitHub Issues: https://github.com/mangowhoiscloud/backend/issues
- AWS Support: https://console.aws.amazon.com/support/
- Terraform Registry: https://discuss.hashicorp.com/

---

## 18. GitOps ë°°í¬ ë¬¸ì œ (2025-11-16 ì¶”ê°€)

### 18.1. Kustomize ìƒìœ„ ë””ë ‰í† ë¦¬ ì°¸ì¡° ì˜¤ë¥˜

#### ë¬¸ì œ
```
Error: file '../namespaces/domain-based.yaml' is not in or below 'k8s/namespaces'
```

**ì›ì¸**: KustomizeëŠ” ë³´ì•ˆìƒ ìƒìœ„ ë””ë ‰í† ë¦¬ ì°¸ì¡° ë¶ˆê°€

#### í•´ê²°
```bash
# ëª¨ë“  Namespace ë¦¬ì†ŒìŠ¤ëŠ” k8s/namespaces ë””ë ‰í„°ë¦¬ ì•ˆì— ì¡´ì¬í•´ì•¼ í•¨
# (Wave 00: namespaces)
```

**ì»¤ë°‹**: `c17defd`

---

### 18.2. ApplicationSet kustomize.images ë¬¸ë²• ì˜¤ë¥˜

#### ë¬¸ì œ
```
ApplicationSet.argoproj.io "api-services" is invalid: 
spec.template.spec.source.kustomize.images[0]: Invalid value: "object"
```

**ì›ì¸**: ApplicationSetì—ì„œ kustomize.imagesëŠ” ê°ì²´ í˜•íƒœ ì‚¬ìš© ë¶ˆê°€

#### í•´ê²°
```yaml
# argocd/apps/80-apis-app-of-apps.yaml
# BEFORE (ì˜¤ë¥˜)
source:
  path: k8s/overlays/{{domain}}
  kustomize:
    images:
      - name: ghcr.io/sesacthon/{{domain}}-api
        newTag: latest

# AFTER (ìˆ˜ì •)
source:
  path: k8s/overlays/{{domain}}
  # kustomize.images ì œê±° - overlayì˜ patch-deployment.yamlì—ì„œ ì´ë¯¸ latest ì§€ì •
```

**ì»¤ë°‹**: `7f79d30`

---

### 18.3. CI Workflow YAML íŒŒì‹± ì˜¤ë¥˜

#### ë¬¸ì œ
```
YAML parsing failed: could not find expected ':'
in ".github/workflows/ci-quality-gate.yml", line 186
```

**ì›ì¸**: Python heredocì˜ ë“¤ì—¬ì“°ê¸° ë¬¸ì œ

#### í•´ê²°
```yaml
# .github/workflows/ci-quality-gate.yml
# BEFORE (ì˜¤ë¥˜)
python <<'PY'
import json  # ë“¤ì—¬ì“°ê¸° ì—†ìŒ
...
PY

# AFTER (ìˆ˜ì •)
python3 <<'PYEOF'
  import json  # YAML ë¬¸ë²•ì— ë§ê²Œ ë“¤ì—¬ì“°ê¸°
  ...
PYEOF
```

**ì»¤ë°‹**: `84b1c1d`

---

### 18.4. GHCR ImagePullBackOff (ê¶Œí•œ ë¬¸ì œ)

#### ë¬¸ì œ
```
Failed to pull image "ghcr.io/sesacthon/auth-api:latest": 403 Forbidden
```

**ì›ì¸**: Secretì˜ GitHub tokenì— `read:packages` ê¶Œí•œ ì—†ìŒ

#### í•´ê²°
```bash
# 1. read:packages ê¶Œí•œì´ ìˆëŠ” í† í° ìƒì„±
# GitHub Settings â†’ Developer settings â†’ Personal access tokens

# 2. ëª¨ë“  namespaceì— Secret ì¬ìƒì„±
for ns in auth character chat info location my scan workers; do
  kubectl delete secret ghcr-secret -n $ns
  kubectl create secret docker-registry ghcr-secret \
    --docker-server=ghcr.io \
    --docker-username=<USERNAME> \
    --docker-password=<TOKEN_WITH_READ_PACKAGES> \
    --namespace=$ns
done

# 3. Pods ì¬ìƒì„±
kubectl delete pod --all -n auth
```

**í•„ìˆ˜ ê¶Œí•œ**: `read:packages`, `write:packages` (ë¹Œë“œ ì‹œ)

**ì»¤ë°‹**: Secret ìƒì„± (ìˆ˜ë™), `0f6663e` (imagePullSecrets ì¶”ê°€)

---

### 18.5. RabbitMQ Bitnami Debian ì´ë¯¸ì§€ ì¤‘ë‹¨

#### ë¬¸ì œ
```
bitnami/rabbitmq:4.1.3-debian-12-r1: not found
bitnami/rabbitmq:3.13.7-debian-12-r0: not found
```

**ì›ì¸**: Bitnamiì˜ Debian ê¸°ë°˜ RabbitMQ ì´ë¯¸ì§€ê°€ 2025-08-28ë¶€í„° ì¤‘ë‹¨ë¨

#### í•´ê²° ë°©ë²•

**Option A: Docker Official Image (ì„ì‹œ)**
```yaml
# platform/charts/data/databases/values.yaml
rabbitmq:
  image:
    registry: docker.io
    repository: rabbitmq
    tag: "3.13-management"
```

**ì£¼ì˜**: Bitnami Chartì˜ init scriptsê°€ Docker Official Imageì™€ í˜¸í™˜ë˜ì§€ ì•Šì„ ìˆ˜ ìˆìŒ

**Option B: RabbitMQ Cluster Operator (ê¶Œì¥)**
```yaml
# argocd/apps/50-data-operators.yamlì— ì¶”ê°€
# RabbitMQ Operator ì„¤ì¹˜ í›„
# RabbitMQCluster CRDë¡œ ë°°í¬
```

**ì»¤ë°‹**: `dd51c46`

**ì°¸ê³ **: https://www.rabbitmq.com/kubernetes/operator/operator-overview.html

---

### 18.6. Ansible Playbook import_tasks ë¬¸ë²• ì¶©ëŒ

#### ë¬¸ì œ
```
ERROR: conflicting action statements: hosts, tasks
Origin: ansible/playbooks/07-alb-controller.yml:4:3
```

**ì›ì¸**: `import_tasks`ë¡œ í˜¸ì¶œë˜ëŠ” playbookì— `hosts` ì •ì˜ ë¶ˆê°€

#### í•´ê²°
```yaml
# ansible/playbooks/07-alb-controller.yml
# BEFORE (ì˜¤ë¥˜)
---
- name: Task name
  hosts: masters  # â† import_tasksë¡œ í˜¸ì¶œ ì‹œ ë¶ˆê°€
  tasks:
    - ...

# AFTER (ìˆ˜ì •)
---
- name: Task name
  # hosts ì œê±°, tasksë§Œ ì •ì˜
  set_fact:
    ...
```

**ì»¤ë°‹**: `7f79d30`

---

### 18.7. VPC ì‚­ì œ ì‹¤íŒ¨ (ALB/Target Groups ë‚¨ìŒ)

#### ë¬¸ì œ
```
terraform destroy ì‹¤íŒ¨
Error: VPC has dependencies and cannot be deleted
```

**ì›ì¸**: Kubernetes ALB Controllerê°€ ìƒì„±í•œ ALB, Target Groupsê°€ ë‚¨ì•„ìˆìŒ

#### í•´ê²°
```bash
# VPC cleanup ìŠ¤í¬ë¦½íŠ¸ ì‚¬ìš©
bash scripts/cleanup-vpc-resources.sh

# ìˆ˜ë™ ì •ë¦¬
VPC_ID=$(aws ec2 describe-vpcs --filters "Name=tag:Project,Values=SeSACTHON" --query 'Vpcs[0].VpcId' --output text)

# Target Groups ì‚­ì œ
aws elbv2 describe-target-groups --query "TargetGroups[?VpcId=='$VPC_ID'].TargetGroupArn" --output text | \
  xargs -I {} aws elbv2 delete-target-group --target-group-arn {}

# Load Balancers ì‚­ì œ
aws elbv2 describe-load-balancers --query "LoadBalancers[?VpcId=='$VPC_ID'].LoadBalancerArn" --output text | \
  xargs -I {} aws elbv2 delete-load-balancer --load-balancer-arn {}

# 30ì´ˆ ëŒ€ê¸° í›„ terraform destroy
sleep 30
terraform destroy -auto-approve
```

**ìŠ¤í¬ë¦½íŠ¸**: `scripts/cleanup-vpc-resources.sh` ìƒì„±ë¨

---

### 18.8. scan-api CrashLoopBackOff (ëª¨ë“ˆ ê²½ë¡œ)

#### ë¬¸ì œ
```
ERROR: Error loading ASGI app. Could not import module "main".
```

**ì›ì¸**: Dockerfileì˜ uvicorn ê²½ë¡œê°€ ì˜ëª»ë¨

#### í•´ê²°
```dockerfile
# services/scan/Dockerfile
# BEFORE
CMD ["uvicorn", "main:app", ...]

# AFTER  
CMD ["uvicorn", "app.main:app", ...]
```

**ì»¤ë°‹**: `eb154a7`

---

### 18.9. ArgoCD Application ìë™ Sync ì•ˆë¨

#### ë¬¸ì œ
Applicationsê°€ OutOfSync ìƒíƒœë¡œ ë‚¨ì•„ìˆìŒ

#### ì›ì¸
```yaml
syncPolicy:
  automated:
    prune: true
    selfHeal: true  # ì„¤ì •ë˜ì–´ ìˆì§€ë§Œ delay ìˆìŒ
```

#### í•´ê²°
```bash
# ìˆ˜ë™ sync íŠ¸ë¦¬ê±°
kubectl patch application <app-name> -n argocd --type merge \
  -p '{"operation":{"initiatedBy":{"username":"admin"},"sync":{"revision":"develop"}}}'

# ë˜ëŠ” Application ì¬ìƒì„± (root-appì´ ìë™ ì¬ìƒì„±)
kubectl delete application <app-name> -n argocd
```

**ì‹œê°„ì´ ì§€ë‚˜ë©´ ìë™ìœ¼ë¡œ syncë¨** (retryPolicy ì„¤ì •)

---

### 18.10. ALB Controller VPC ID í•˜ë“œì½”ë”©

#### ë¬¸ì œ
```
ALB Controller CrashLoopBackOff
Error: unable to create controller
```

**ì›ì¸**: ArgoCD Applicationì— ì´ì „ VPC ID í•˜ë“œì½”ë”©ë¨

#### í•´ê²°
```yaml
# argocd/apps/20-alb-controller.yaml
# í˜„ì¬ VPC IDë¡œ ìˆ˜ì • í•„ìš”
parameters:
  - name: vpcId
    value: vpc-0cb5bbb41f25671f5  # ìƒˆ VPC ID
```

**ê°œì„ ì•ˆ**: ConfigMapì´ë‚˜ External Secretsë¡œ ë™ì  ì£¼ì… ê³ ë ¤

**ì»¤ë°‹**: `0645847`

---

### 18.11. ALB Controller egress ì°¨ë‹¨ (NetworkPolicy)

#### ë¬¸ì œ
```
aws-load-balancer-controller-7cbcb46f48-xxxxx  CrashLoopBackOff
unable to create controller: Post "https://10.96.0.1:443/...": dial tcp 10.96.0.1:443: i/o timeout
```

- ì™¸ë¶€ í†µì‹  (Kubernetes API, AWS API, IMDS ë“±)ì— ì ‘ê·¼í•˜ì§€ ëª»í•´ Operatorê°€ ê¸°ë™ ì§í›„ ì¢…ë£Œ
- ALB, TargetGroup, Listener ë™ê¸°í™”ê°€ ëª¨ë‘ ë©ˆì¶”ë©´ì„œ Wave 15~70 ì „ì²´ê°€ OutOfSync

#### ì›ì¸
- GitOps v0.7.3 (`5341203`)ì—ì„œ ë°°í¬ëœ `k8s/infrastructure/networkpolicies/domain-isolation.yaml`ì´ **ëª¨ë“  ë„¤ì„ìŠ¤í˜ì´ìŠ¤ egressë¥¼ TCP 80/443 + data namespace**ë¡œë§Œ ì œí•œ
- `kube-system` â†’ API Server(10.96.0.1), DNS(UDP 53), AWS Public API(0.0.0.0/0:443) CIDR ì´ í—ˆìš©ë˜ì§€ ì•Šì•„ Control Planeê³¼ IRSA STS í˜¸ì¶œì´ ëª¨ë‘ ì°¨ë‹¨
- ë™ì¼í•œ í…œí”Œë¦¿ì´ business-logic namespace ì „ì²´ì— ë°˜ë³µ ì ìš©ë˜ë©´ì„œ ALB Controller ë¿ ì•„ë‹ˆë¼ ExternalDNS, Metrics Serverê¹Œì§€ ì—°ì‡„ CrashLoop

#### í•´ê²°
1. ë¬¸ì œ Policy ì œê±° (`git revert 5341203 -- k8s/infrastructure/networkpolicies/domain-isolation.yaml` í˜¹ì€ `kubectl delete`ë¡œ ì¦‰ì‹œ ì™„í™”)
2. ì „ìš© egress í—ˆìš© ì •ì±…ì„ ë³„ë„ íŒŒì¼ë¡œ ì¬ì‘ì„±  
   - `workloads/network-policies/base/allow-dns.yaml` (UDP/TCP 53)  
   - `workloads/network-policies/base/default-deny-all.yaml` (ê¸°ë³¸ ì°¨ë‹¨)  
   - `alb-controller-egress` ì»¤ìŠ¤í…€ ì •ì±…:  
     ```yaml
     egress:
       - to:
           - ipBlock: { cidr: 10.96.0.1/32 }        # Kubernetes API
         ports:
           - protocol: TCP
             port: 443
       - to:
           - namespaceSelector:
               matchLabels:
                 kubernetes.io/metadata.name: kube-system
         ports:
           - protocol: UDP
             port: 53
           - protocol: TCP
             port: 53
       - to:
           - ipBlock: { cidr: 169.254.169.254/32 }  # Instance Metadata
           - ipBlock: { cidr: 0.0.0.0/0 }           # AWS API (STS ë“±)
         ports:
           - protocol: TCP
             port: 443
     ```
3. Wave 5/6 (Calico â†’ NetworkPolicy)ì—ì„œ ìœ„ ì •ì±… ì„¸íŠ¸ë¥¼ ë°°í¬í•˜ë„ë¡ `clusters/{env}/apps/05-calico.yaml`, `06-network-policies.yaml` ìˆœì„œë¥¼ ê³ ì •
4. ALB Controller ì¬ê¸°ë™ (`kubectl rollout restart deployment/aws-load-balancer-controller -n kube-system`)

#### ì‚¬í›„ ì¡°ì¹˜
- `5c4f5cc`ì—ì„œ legacy `k8s/infrastructure/networkpolicies` ê²½ë¡œë¥¼ ì •ë¦¬í•˜ê³ , ëª¨ë“  ì •ì±…ì„ `workloads/network-policies`ë¡œ í†µí•©
- `77d694c`ì—ì„œ overlays ì—†ì´ base/í™˜ê²½ë³„ í‰ë©´ êµ¬ì¡°ë¡œ ì¬ì •ë¹„í•˜ì—¬ ArgoCD Diff, GitHub PR ë¦¬ë·°ê°€ ê°€ëŠ¥í•˜ë„ë¡ í•¨
- `docs/architecture/networking/NAMESPACE_NETWORKPOLICY_INGRESS.md`ì— ë™ì¼ ì‚¬ë¡€ë¥¼ í‘œì¤€ ê°€ì´ë“œë¡œ ì—°ê²°í•˜ê³ , NetworkPolicy ë³€ê²½ ì‹œ `kubectl logs` / `kubectl describe networkpolicy` ì ê²€ì„ ë°°í¬ ì²´í¬ë¦¬ìŠ¤íŠ¸ì— ì¶”ê°€

---

## 19. ë² ìŠ¤íŠ¸ í”„ë™í‹°ìŠ¤ (2025-11-16 ì—…ë°ì´íŠ¸)

### 19.1. GitOps ë°°í¬

**ê¶Œì¥:**
- âœ… NamespaceëŠ” ArgoCD namespacesì—ì„œë§Œ ê´€ë¦¬ (Ansible ì¤‘ë³µ ì œê±°)
- âœ… Cert-manager ì œê±°, ACM ì‚¬ìš©
- âœ… Kustomize ë¦¬ì†ŒìŠ¤ëŠ” ê°™ì€ ë””ë ‰í† ë¦¬ë‚˜ í•˜ìœ„ì—ë§Œ
- âœ… ApplicationSetì—ì„œ kustomize.images ì‚¬ìš© ê¸ˆì§€
- âœ… CI YAML heredocëŠ” ì˜¬ë°”ë¥¸ ë“¤ì—¬ì“°ê¸°

### 19.2. GHCR ì´ë¯¸ì§€ ê´€ë¦¬

**ê¶Œì¥:**
- âœ… Tokenì— `read:packages`, `write:packages` ê¶Œí•œ í•„ìˆ˜
- âœ… imagePullSecretsë¥¼ base deploymentì— ì •ì˜
- âœ… Private packages ì‚¬ìš© ì‹œ ëª¨ë“  namespaceì— Secret ìƒì„±
- âœ… CIì—ì„œ `secrets.GH_TOKEN` ì‚¬ìš© (GITHUB_TOKENì€ ì œí•œì )

### 19.3. Bitnami Charts

**ì£¼ì˜:**
- âš ï¸ Bitnami Debian ì´ë¯¸ì§€ ì¤‘ë‹¨ (2025-08-28)
- âœ… Docker Official Image ë˜ëŠ” Operator ì‚¬ìš© ê¶Œì¥
- âœ… Bitnami Chartì™€ Docker Official Image í˜¸í™˜ì„± í™•ì¸ í•„ìš”

---

**ìµœì¢… ì—…ë°ì´íŠ¸**: 2025-11-16  
**ë²„ì „**: v0.7.3  
**ì•„í‚¤í…ì²˜**: 14-Node GitOps Production

## 20. Legacy Troubleshooting Archive (docs/troubleshooting/\*)

2025-11-11 `84dcb7fa` ë“± ë¬¸ì„œ ì •ë¦¬ ì»¤ë°‹ì—ì„œ ì‚­ì œëœ `docs/troubleshooting/*.md` 19ê±´ì„ ë‹¤ì‹œ ì´ ë¬¸ì„œì— í†µí•©í–ˆìŠµë‹ˆë‹¤. ê¸°ì¡´ ì„¹ì…˜ê³¼ ë‚´ìš©ì´ ê²¹ì¹˜ëŠ” í•­ëª©ì€ **ë²„ì „/ì•„í‚¤í…ì²˜ ì •ë³´**ë§Œ í‘œì— ê¸°ë¡í•˜ê³ , ëˆ„ë½ë¼ ìˆë˜ ì‚¬ë¡€ëŠ” `20.1~20.10`ì— ì „ë¬¸ì„ ìš”ì•½í•´ ë³µì›í–ˆìŠµë‹ˆë‹¤.

| Legacy ID | ì´ìŠˆ | ë²„ì „Â·ì•„í‚¤í…ì²˜ | í˜„ì¬ ë°˜ì˜ |
|-----------|------|---------------|-----------|
| 01 | ALB Provider ID ëˆ„ë½ | GitOps v0.6.0 Â· 14-Node | 20.1 (ë³µì›) |
| 02 | auto-rebuild Ansible SSH íƒ€ì„ì•„ì›ƒ | GitOps v0.7.0 Â· 14-Node | 20.2 (ë³µì›) |
| 03 | ArgoCD 502 Bad Gateway | GitOps v0.6.0 Â· 14-Node | 20.3 (ë³µì›) |
| 04 | ArgoCD ë¦¬ë””ë ‰ì…˜ ë£¨í”„ | GitOps v0.6.0 Â· 14-Node | ì„¹ì…˜ 8 |
| 05 | Atlantis Config YAML íŒŒì‹± ì˜¤ë¥˜ | GitOps v0.6.0 Â· 13-Node | ì„¹ì…˜ 14 |
| 06 | Atlantis Deployment íŒŒì¼ ì—†ìŒ | GitOps v0.6.0 Â· 13-Node | ì„¹ì…˜ 12 |
| 07 | Atlantis executable not found | GitOps v0.6.0 Â· 13-Node | ì„¹ì…˜ 13 |
| 08 | Atlantis Podì—ì„œ kubectl ë¯¸íƒ‘ì¬ | GitOps v0.6.0 Â· 13-Node | ì„¹ì…˜ 11 |
| 09 | Atlantis CrashLoopBackOff | GitOps v0.6.0 Â· 13-Node | ì„¹ì…˜ 10 |
| 10 | CloudFront ACM Certificate stuck | GitOps v0.6.0 Â· 13-Node | ì„¹ì…˜ 6.3 |
| 11 | deploy.sh SSH í‚¤/Ingress Deprecated ì˜µì…˜ | GitOps v0.6.0 Â· 14-Node | 20.4 (ë³µì›) |
| 12 | macOS TLS ì¸ì¦ì„œ ì˜¤ë¥˜ | GitOps v0.6.0 Â· Local Dev | 20.5 (ë³µì›) |
| 13 | Monitoring ë…¸ë“œ ë¦¬ì†ŒìŠ¤ ë¶„ì„ | GitOps v0.6.0 Â· 14-Node | 20.6 (ë³µì›) |
| 14 | PostgreSQL FailedScheduling | GitOps v0.6.0 Â· 14-Node | 20.7 (ë³µì›) |
| 15 | Prometheus ë©”ëª¨ë¦¬ ë¶€ì¡± | GitOps v0.6.0 Â· 14-Node | ì„¹ì…˜ 9 |
| 16 | Prometheus Pending (CPU ë¶€ì¡±) | GitOps v0.6.0 Â· 14-Node | 20.8 (ë³µì›) |
| 17 | Route53 â†’ ALB ë¼ìš°íŒ… ìˆ˜ì • | GitOps v0.6.0 Â· 14-Node | 20.9 (ë³µì›) |
| 18 | CloudFront/ACM/VPC ì‚­ì œ ì¥ì•  | GitOps v0.6.0 Â· 14-Node | 20.10 (ë³µì›) |
| 19 | VPC ì‚­ì œ ì§€ì—° (Security Group) | GitOps v0.6.0 Â· 13-Node | ì„¹ì…˜ 7 |

### 20.1. ALB Provider ID ëˆ„ë½ (GitOps v0.6.0 Â· 14-Node)

- **ì¦ìƒ**: ALBê°€ ë§Œë“¤ì–´ì¡Œì§€ë§Œ TargetGroupì´ ë¹„ì–´ ìˆê³  `503 Service Unavailable`.
- **ì›ì¸**: Worker/Infra ë…¸ë“œ `spec.providerID`ê°€ `aws:///ap-northeast-2a/`ì²˜ëŸ¼ Instance ID ì—†ì´ ë¹„ì–´ ìˆì–´ AWS Load Balancer Controllerê°€ ë…¸ë“œë¥¼ ì‹ë³„í•˜ì§€ ëª»í•¨.
- **í•´ê²°**:
  1. ê¸´ê¸‰: ê° ë…¸ë“œì—ì„œ `kubeadm-flags.env` ëì— `--provider-id=aws:///AZ/i-xxxxxxxx`ë¥¼ ì¶”ê°€ í›„ `systemctl restart kubelet`.
  2. ì˜êµ¬: `ansible/playbooks/03-worker-join.yml`ì— `ec2-metadata` ê¸°ë°˜ providerID ì£¼ì… íƒœìŠ¤í¬ ì¶”ê°€.
  3. ê²€ì¦: `kubectl get nodes -o custom-columns='NAME:.metadata.name,PROVIDER:.spec.providerID'`.
- **ì°¸ê³ **: `terraform/alb-controller-iam.tf`, `ansible/playbooks/03-worker-join.yml`.

### 20.2. auto-rebuild Ansible SSH íƒ€ì„ì•„ì›ƒ (GitOps v0.7.0 Â· 14-Node)

- **ì¦ìƒ**: `ansible-playbook site.yml` ë‹¨ê³„ì—ì„œ `Timeout when waiting for <old-ip>:22`.
- **ì›ì¸**: Terraformì´ ìƒˆ Public IPë¥¼ ë°œê¸‰í–ˆìœ¼ë‚˜ `auto-rebuild.sh`ê°€ `ansible/inventory/hosts.ini`ë¥¼ ì¬ìƒì„±í•˜ì§€ ì•Šì•„ êµ¬ë²„ì „ IP(ì´ì „ í´ëŸ¬ìŠ¤í„°)ë¥¼ ê³„ì† ì‚¬ìš©.
- **í•´ê²°**:
  - Terraform outputì„ ì§„ì‹¤ì›ìœ¼ë¡œ ì‚¬ìš©:  
    `terraform output -raw ansible_inventory > ansible/inventory/hosts.ini`.
  - ìŠ¤í¬ë¦½íŠ¸ ê°•í™”: apply ì§í›„ inventory ì¬ìƒì„±, ë…¸ë“œ ê°œìˆ˜ ë™ì  ê³„ì‚°, Phase3/4 ë¼ë²¨ë§Â·í—¬ìŠ¤ì²´í¬ ì¶”ê°€.
  - ì‹¤í–‰ ìˆœì„œ: inventory ì¬ìƒì„± â†’ `ansible all -m ping` â†’ `ansible-playbook site.yml`.

### 20.3. ArgoCD 502 Bad Gateway (GitOps v0.6.0 Â· 14-Node)

- **ì¦ìƒ**: `https://growbin.app/argocd` ì ‘ê·¼ ì‹œ 502, TargetHealth `unhealthy`.
- **ì›ì¸**: Ingress annotationì´ `alb.ingress.kubernetes.io/backend-protocol: HTTPS`ì´ê³  Service Portê°€ 443ì¸ ë°˜ë©´ ArgoCD ì„œë²„ëŠ” `server.insecure: true`ë¡œ HTTP 8080ë§Œ ë¦¬ìŠ¨.
- **í•´ê²°**:
  1. `backend-protocol`ì„ HTTPë¡œ, backend Service Portë¥¼ 80ìœ¼ë¡œ ìˆ˜ì •.  
     `kubectl annotate ingress argocd-ingress alb.ingress.kubernetes.io/backend-protocol=HTTP --overwrite`
  2. `ansible/playbooks/07-ingress-resources.yml` í…œí”Œë¦¿ ì—…ë°ì´íŠ¸.
  3. `aws elbv2 describe-target-health`ë¡œ Health í™•ì¸.

### 20.4. deploy.sh SSH í‚¤Â·Ingress Deprecated ì˜µì…˜ (GitOps v0.6.0 Â· 14-Node)

- **ì¦ìƒ**: `deploy.sh`ê°€ `~/.ssh/sesacthon.pem` ë¯¸ì¡´ì¬ë¡œ ì¤‘ë‹¨, `kubernetes.io/ingress.class` warning ë‹¤ìˆ˜ ë°œìƒ.
- **í•´ê²°**:
  - SSH: Terraformì´ ì—…ë¡œë“œí•œ `~/.ssh/id_rsa`ë¥¼ ê¸°ë³¸ í‚¤ë¡œ ì‚¬ìš©í•˜ë„ë¡ ìŠ¤í¬ë¦½íŠ¸ ìˆ˜ì •.
  - Ingress: annotation ëŒ€ì‹  `spec.ingressClassName: alb` ì‚¬ìš©.
  - `ansible/ansible.cfg`ì— `deprecation_warnings = False`.
- **ê²€ì¦**: `kubectl get ingress -A -o jsonpath='{.items[*].spec.ingressClassName}'`.

### 20.5. macOS TLS ì¸ì¦ì„œ ì˜¤ë¥˜ (GitOps v0.6.0 Â· Local Dev)

- **ì¦ìƒ**: macOSì—ì„œ Terraform/kubectl ì‹¤í–‰ ì‹œ `x509: certificate signed by unknown authority`.
- **ì›ì¸**: Go ëŸ°íƒ€ì„ì´ macOS Keychain ì¸ì¦ì„œë¥¼ ì¸ì‹í•˜ì§€ ëª»í•˜ê±°ë‚˜ í”„ë¡ì‹œí˜• SSL ê²€ì‚¬ ë„êµ¬ê°€ ì¤‘ê°„ì ì¸ì¦ì„œë¥¼ ì‚½ì….
- **í•´ê²° ì˜µì…˜**:
  - Terraformì„ Docker ì´ë¯¸ì§€ì—ì„œ ì‹¤í–‰í•˜ì—¬ ë¦¬ëˆ…ìŠ¤ cacerts ì‚¬ìš©.
  - ê°œë°œ í™˜ê²½ì— í•œí•´ `SSL_CERT_FILE` ì§€ì • ë˜ëŠ” `GODEBUG=x509ignoreCN=0`.
  - Keychainì—ì„œ ë£¨íŠ¸ ì¸ì¦ì„œë¥¼ ì¬ì‹ ë¢°.

### 20.6. Monitoring ë…¸ë“œ ë¦¬ì†ŒìŠ¤ ë¶„ì„ (GitOps v0.6.0 Â· 14-Node)

- **ì¦ìƒ**: Prometheus/Grafana/Alertmanagerê°€ ë‹¤ë¥¸ ë…¸ë“œì— ìŠ¤ì¼€ì¤„ë§ë˜ì–´ Monitoring ì „ìš© ë…¸ë“œê°€ ë¹„ê²Œ ë¨.
- **ì›ì¸**: Monitoring ë…¸ë“œì— `node-role.kubernetes.io/infrastructure=true:NoSchedule` taintê°€ ìˆëŠ”ë° Helm valuesì— tolerationê³¼ nodeSelectorê°€ ì—†ì—ˆìŒ.
- **í•´ê²°**:
  - `kube-prometheus-stack` valuesì— ê³µí†µ toleration/nodeSelector ì¶”ê°€(í˜„ì¬ dev/prod valuesì— ë°˜ì˜).
  - ë¦¬ì†ŒìŠ¤ ë¶„ì„: t3.medium ê¸°ì¤€ CPU 1780m ì‚¬ìš©(89%), RAM 2.4Gi(65%)ë¡œ ì—…ìŠ¤ì¼€ì¼ ë¶ˆí•„ìš”.

### 20.7. PostgreSQL FailedScheduling (GitOps v0.6.0 Â· 14-Node)

- **ì¦ìƒ**: `postgres-0`ê°€ `FailedScheduling: ... node(s) didn't match Pod's node selector`.
- **ì›ì¸**: Storage ë…¸ë“œì— `workload=storage` ë ˆì´ë¸”ì´ ëˆ„ë½ë˜ê±°ë‚˜ ì˜ëª»ëœ ë…¸ë“œëª…ì„ ì‚¬ìš©í•´ Ansible ë¼ë²¨ë§ì´ ì‹¤íŒ¨.
- **í•´ê²°**:
  1. `kubectl get nodes -L workload`ë¡œ í™•ì¸ í›„ `kubectl label nodes <node> workload=storage --overwrite`.
  2. í•„ìš” ì‹œ `scripts/fix-node-labels.sh` ì‹¤í–‰ ë˜ëŠ” Ansible ë ˆì´ë¸” ë‹¨ê³„ ì¬ì‹¤í–‰.
  3. `kubectl rollout restart statefulset/postgres -n postgres`.

### 20.8. Prometheus Pending (CPU ë¶€ì¡±) (GitOps v0.6.0 Â· 14-Node)

- **ì¦ìƒ**: Prometheus StatefulSetì´ Pending, ì´ë²¤íŠ¸ì— `1 Insufficient cpu`.
- **ì›ì¸**: Monitoring ë…¸ë“œ(t3.large, 2000m)ê°€ ì´ë¯¸ 1130më¥¼ ì‚¬ìš© ì¤‘ì¸ ìƒíƒœì—ì„œ Prometheusê°€ 1000m ìš”ì²­ â†’ ê°€ìš©ëŸ‰ ì´ˆê³¼.
- **í•´ê²°**:
  - CPU requestë¥¼ 500më¡œ ë‚®ì¶¤(`kubectl patch prometheus ...` ë˜ëŠ” Helm values ìˆ˜ì •).
  - ëŒ€ì•ˆ: Grafanaë¥¼ ë‹¤ë¥¸ ë…¸ë“œë¡œ ì´ë™ ë˜ëŠ” Monitoring ì¸ìŠ¤í„´ìŠ¤ë¥¼ t3.xlargeë¡œ ì—…ê·¸ë ˆì´ë“œ.

### 20.9. Route53 â†’ ALB ë¼ìš°íŒ… ìˆ˜ì • (GitOps v0.6.0 Â· 14-Node)

- **ì¦ìƒ**: `growbin.app`, `argocd.growbin.app` ë“±ì´ Master Public IPë¥¼ ì§ì ‘ ê°€ë¦¬ì¼œ TLS ì¢…ë£ŒÂ·ë¶€í•˜ë¶„ì‚°ì´ ë¬´ë ¥í™”.
- **í•´ê²°**:
  - Terraform `aws_route53_record`ë¥¼ ALB Aliasë¡œ êµì²´í•˜ê±°ë‚˜,
  - ALB Controllerê°€ ìƒì„±ëœ ë’¤ Ansible Playbookìœ¼ë¡œ Route53 ë ˆì½”ë“œë¥¼ ìë™ ì—…ë°ì´íŠ¸.
- **ì˜ˆì‹œ**:
  ```hcl
  alias {
    name    = data.aws_lb.alb[0].dns_name
    zone_id = data.aws_lb.alb[0].zone_id
    evaluate_target_health = true
  }
  ```

### 20.10. CloudFront/ACM/VPC ì‚­ì œ ì¥ì•  (GitOps v0.6.0 Â· 14-Node)

- **ì¦ìƒ**: `force-destroy-all.sh` ì‹¤í–‰ ì‹œ CloudFront Distributionì„ ê°ì§€í•˜ì§€ ëª»í•´ ACM Certificateì™€ VPC ì‚­ì œê°€ 20ë¶„ ì´ìƒ ì§€ì—°.
- **ì›ì¸**:
  - `aws cloudfront list-distributions` JMESPathê°€ ë³µì¡í•´ `E1GGDPUBLRQG59`ë¥¼ ì°¾ì§€ ëª»í•¨.
  - CloudFrontê°€ ë‚¨ì•„ ìˆì–´ `arn:aws:acm:us-east-1:...` Certificate ì‚­ì œê°€ ê³„ì† pending.
- **í•´ê²°**:
  1. `jq` ê¸°ë°˜ ë‹¨ìˆœ ë¦¬ìŠ¤íŠ¸ë¡œ Distribution IDë¥¼ ê²€ìƒ‰í•˜ê³  Disabled ìƒíƒœ í™•ì¸ í›„ ëª…ì‹œì ìœ¼ë¡œ ì‚­ì œ.
  2. Certificate `InUseBy`ë¥¼ ê²€ì‚¬í•´ CloudFront ì‚­ì œ ì™„ë£Œê¹Œì§€ ëŒ€ê¸°.
  3. í•„ìš” ì‹œ `scripts/utilities/manual-cleanup-cloudfront-acm.sh`ë¡œ ìˆ˜ë™ ì •ë¦¬ í›„ Terraform destroy ì¬ì‹œë„.

---

## 21. Ansible ë…¸ë“œ ë¼ë²¨ê³¼ Kubernetes Manifest ë™ê¸°í™” (2025-11-16 ì¶”ê°€)

### 21.1. ë…¸ë“œ ë¼ë²¨ê³¼ nodeSelector ë¶ˆì¼ì¹˜ë¡œ ì¸í•œ Pod ìŠ¤ì¼€ì¤„ë§ ì‹¤íŒ¨

#### ë¬¸ì œ
**ì¦ìƒ**:
```bash
# API Deploymentsê°€ ë°°í¬ë˜ì§€ ì•ŠìŒ
kubectl get pods -n auth
No resources found in auth namespace.

# ë˜ëŠ” Pending ìƒíƒœ
NAME                       READY   STATUS    RESTARTS   AGE
auth-api-bff55b88f-xxxxx   0/1     Pending   0          5m
```

**Pod describe ê²°ê³¼**:
```
Events:
  Type     Reason            Age   From               Message
  ----     ------            ----  ----               -------
  Warning  FailedScheduling  5m    default-scheduler  0/14 nodes are available: 14 node(s) didn't match Pod's node selector.
```

#### ì›ì¸
Ansible playbook (`ansible/playbooks/fix-node-labels.yml`)ì´ ì„¤ì •í•˜ëŠ” ë…¸ë“œ ë¼ë²¨ê³¼ Kubernetes Deploymentì˜ `nodeSelector`ê°€ ë¶ˆì¼ì¹˜:

**Ansibleì´ ì„¤ì •í•œ ë…¸ë“œ ë¼ë²¨** (ì‹¤ì œ í´ëŸ¬ìŠ¤í„°):
```bash
kubectl get nodes k8s-api-auth --show-labels
# ì¶œë ¥:
sesacthon.io/node-role=api
sesacthon.io/service=auth
workload=api
domain=auth
tier=business-logic
phase=1
```

**Deploymentê°€ ìš”êµ¬í•˜ëŠ” nodeSelector** (êµ¬ë²„ì „ manifest):
```yaml
# workloads/apis/auth/base/deployment.yaml (ìˆ˜ì • ì „)
spec:
  template:
    spec:
      nodeSelector:
        node-role.kubernetes.io/api: auth  # âŒ ë…¸ë“œì— ì—†ëŠ” ë¼ë²¨
```

**ë¶ˆì¼ì¹˜ ë§¤í•‘**:
| ë¦¬ì†ŒìŠ¤ | Ansible ë¼ë²¨ | êµ¬ë²„ì „ Manifest | ê²°ê³¼ |
|--------|-------------|----------------|------|
| API | `sesacthon.io/service=auth` | `node-role.kubernetes.io/api: auth` | âŒ ë¶ˆì¼ì¹˜ |
| PostgreSQL | `sesacthon.io/infra-type=postgresql` | `node-role.kubernetes.io/infrastructure: postgresql` | âŒ ë¶ˆì¼ì¹˜ |
| Redis | `sesacthon.io/infra-type=redis` | `node-role.kubernetes.io/infrastructure: redis` | âŒ ë¶ˆì¼ì¹˜ |

**ì˜í–¥ë°›ëŠ” ì„œë¹„ìŠ¤**: ì „ì²´ 9ê°œ (auth, my, scan, character, location, info, chat + PostgreSQL + Redis)

#### í•´ê²°

**1. Kubernetes Manifests ìˆ˜ì •** (ê¶Œì¥):

ëª¨ë“  deploymentì˜ nodeSelectorë¥¼ Ansible ë¼ë²¨ê³¼ ì¼ì¹˜ì‹œí‚´:

```yaml
# workloads/apis/auth/base/deployment.yaml (ìˆ˜ì • í›„)
spec:
  template:
    spec:
      nodeSelector:
        sesacthon.io/service: auth  # âœ… Ansible ë¼ë²¨ê³¼ ì¼ì¹˜
      tolerations:
        - key: domain
          operator: Equal
          value: auth
          effect: NoSchedule
```

**Infrastructure ë¦¬ì†ŒìŠ¤**:
```yaml
# workloads/data/postgres/base/postgres-cluster.yaml
spec:
  nodeAffinity:
    requiredDuringSchedulingIgnoredDuringExecution:
      nodeSelectorTerms:
        - matchExpressions:
            - key: sesacthon.io/infra-type  # âœ… ë³€ê²½
              operator: In
              values:
                - postgresql
  tolerations:
    - key: sesacthon.io/infrastructure  # âœ… ë³€ê²½
      operator: Equal
      value: "true"
      effect: NoSchedule
```

**2. ìˆ˜ì •ëœ íŒŒì¼ ëª©ë¡**:
- API Deployments (7ê°œ): auth, my, scan, character, location, info, chat
- PostgreSQL: `workloads/data/postgres/base/postgres-cluster.yaml`
- Redis: `workloads/data/redis/base/redis-failover.yaml`
- ë¬¸ì„œ: `docs/infrastructure/k8s-label-annotation-system.md`

**3. ê²€ì¦**:
```bash
# ë…¸ë“œ ë¼ë²¨ í™•ì¸
kubectl get nodes k8s-api-auth --show-labels | grep sesacthon

# Deployment nodeSelector í™•ì¸
kubectl get deploy auth-api -n auth -o yaml | grep -A 3 'nodeSelector:'

# Pod ìŠ¤ì¼€ì¤„ë§ í™•ì¸
kubectl get pods -n auth -o wide
# ì˜ˆìƒ ê²°ê³¼:
NAME                       READY   STATUS    NODE           
auth-api-bff55b88f-xxxxx   1/1     Running   k8s-api-auth  # âœ… ì˜¬ë°”ë¥¸ ë…¸ë“œì— ë°°ì¹˜
```

**ì»¤ë°‹**:
- `f191d18` - fix: Ansible ë…¸ë“œ ë¼ë²¨ê³¼ Kubernetes manifest ë™ê¸°í™”

---

### 21.2. Ansible Playbook root-app.yaml ê²½ë¡œ ì˜¤ë¥˜

#### ë¬¸ì œ
**Ansible ì‹¤í–‰ ë¡œê·¸**:
```
TASK [argocd : root-app.yaml ë³µì‚¬ (Master ë…¸ë“œë¡œ)] *****************************
[ERROR]: Task failed: Unexpected AnsibleActionFail error: Could not find or access 
'/Users/mango/workspace/SeSACTHON/backend/ansible/../../argocd/root-app.yaml' on the Ansible Controller.
fatal: [k8s-master]: FAILED!
```

**ê²°ê³¼**:
- ArgoCDëŠ” ì„¤ì¹˜ë˜ì—ˆì§€ë§Œ root-appì´ ë°°í¬ë˜ì§€ ì•ŠìŒ
- Child applications (Calico, Namespaces, APIs ë“±) ì „í˜€ ìƒì„± ì•ˆ ë¨

#### ì›ì¸
GitOps ë¦¬íŒ©í† ë§ìœ¼ë¡œ `argocd/` ë””ë ‰í† ë¦¬ê°€ `clusters/dev/`, `clusters/prod/`ë¡œ ì´ë™í–ˆëŠ”ë°, Ansible playbookì´ ì˜›ë‚  ê²½ë¡œë¥¼ ì°¸ì¡°:

```yaml
# ansible/roles/argocd/tasks/main.yml (ìˆ˜ì • ì „)
- name: root-app.yaml ë³µì‚¬ (Master ë…¸ë“œë¡œ)
  copy:
    src: "{{ playbook_dir }}/../../../argocd/root-app.yaml"  # âŒ ê²½ë¡œ ì—†ìŒ
    dest: /tmp/root-app.yaml
```

#### í•´ê²°
```yaml
# ansible/roles/argocd/tasks/main.yml (ìˆ˜ì • í›„)
- name: root-app.yaml ë³µì‚¬ (Master ë…¸ë“œë¡œ)
  copy:
    src: "{{ playbook_dir }}/../../clusters/dev/root-app.yaml"  # âœ… ìƒˆ ê²½ë¡œ
    dest: /tmp/root-app.yaml
    mode: '0644'
```

**í™˜ê²½ ë¶„ë¦¬ ê³ ë ¤** (prod ë°°í¬ ì‹œ):
```yaml
- name: root-app.yaml ë³µì‚¬ (í™˜ê²½ë³„)
  copy:
    src: "{{ playbook_dir }}/../../clusters/{{ environment | default('dev') }}/root-app.yaml"
    dest: /tmp/root-app.yaml
    mode: '0644'
  vars:
    environment: "{{ lookup('env', 'DEPLOY_ENV') | default('dev', true) }}"
```

**ê²€ì¦**:
```bash
# root-app ë°°í¬ í™•ì¸
kubectl get application dev-root -n argocd

# Child applications ìƒì„± í™•ì¸
kubectl get applications -n argocd
# ì˜ˆìƒ: dev-namespaces, dev-crds, dev-calico, dev-apis ë“± 12+ applications
```

**ì»¤ë°‹**: `ansible/roles/argocd/tasks/main.yml` ìˆ˜ì • (í˜„ì¬ ì„¸ì…˜)

---

### 21.3. CNI ë¯¸ì„¤ì¹˜ë¡œ ì¸í•œ ìˆœí™˜ ì˜ì¡´ì„± (Bootstrap Chicken-and-Egg)

#### ë¬¸ì œ
**ì¦ìƒ**:
```bash
kubectl get nodes
NAME         STATUS     ROLES           AGE   VERSION
k8s-master   NotReady   control-plane   5m    v1.28.4
# ëª¨ë“  ë…¸ë“œê°€ NotReady

kubectl describe node k8s-master
Conditions:
  Ready   False   KubeletNotReady   container runtime network not ready: 
                                    NetworkReady=false reason:NetworkPluginNotReady 
                                    message:Network plugin returns error: cni plugin not initialized
```

**ArgoCD Pod ìƒíƒœ**:
```bash
kubectl get pods -n argocd
No resources found in argocd namespace.
# Podê°€ ì „í˜€ ì‹¤í–‰ë˜ì§€ ì•ŠìŒ
```

#### ì›ì¸
**ìˆœí™˜ ì˜ì¡´ì„± (Chicken-and-Egg Problem)**:
1. ArgoCDê°€ Calico CNIë¥¼ ë°°í¬í•´ì•¼ í•¨ (GitOps íŒ¨í„´)
2. í•˜ì§€ë§Œ ArgoCD Podê°€ ì‹¤í–‰ë˜ë ¤ë©´ CNIê°€ í•„ìš”í•¨ (Kubernetes ìš”êµ¬ì‚¬í•­)
3. root-app ë°°í¬ ì‹¤íŒ¨ë¡œ Calico Applicationì´ ìƒì„±ë˜ì§€ ì•ŠìŒ
4. CNI ì—†ì–´ì„œ ëª¨ë“  Podê°€ Pending ìƒíƒœë¡œ ë‚¨ìŒ

#### í•´ê²°

**ê¸´ê¸‰ ë³µêµ¬** (í´ëŸ¬ìŠ¤í„° ì´ë¯¸ ë°°í¬ëœ ê²½ìš°):
```bash
# ë§ˆìŠ¤í„° ë…¸ë“œì—ì„œ Calico ìˆ˜ë™ ì„¤ì¹˜
kubectl apply -f https://raw.githubusercontent.com/projectcalico/calico/v3.26.1/manifests/calico.yaml

# ë…¸ë“œ Ready ìƒíƒœ í™•ì¸ (30ì´ˆ ëŒ€ê¸°)
sleep 30 && kubectl get nodes
# ëª¨ë“  ë…¸ë“œ Ready í™•ì¸

# ArgoCD ìˆ˜ë™ ì„¤ì¹˜
kubectl create namespace argocd
kubectl apply -n argocd -f https://raw.githubusercontent.com/argoproj/argo-cd/stable/manifests/install.yaml

# ArgoCD Pod Ready ëŒ€ê¸°
kubectl wait --for=condition=ready pod -l app.kubernetes.io/name=argocd-server -n argocd --timeout=300s

# AppProject ìƒì„±
kubectl apply -f - <<EOF
apiVersion: argoproj.io/v1alpha1
kind: AppProject
metadata:
  name: dev
  namespace: argocd
spec:
  description: Development Environment
  sourceRepos: ['*']
  destinations:
    - namespace: '*'
      server: '*'
  clusterResourceWhitelist:
    - group: '*'
      kind: '*'
EOF

# root-app ë°°í¬
kubectl apply -f /tmp/root-app.yaml
```

**Ansible ê°œì„ ** (ë‹¤ìŒ ë¶€íŠ¸ìŠ¤íŠ¸ë©):

`ansible/roles/argocd/tasks/main.yml`ì— CNI pre-check ì¶”ê°€:

```yaml
# ArgoCD ì„¤ì¹˜ ì „ì— CNI í™•ì¸
- name: CNI í”ŒëŸ¬ê·¸ì¸ ì„¤ì¹˜ ì—¬ë¶€ í™•ì¸
  shell: kubectl get pods -n kube-system -l k8s-app=calico-node --no-headers 2>/dev/null | wc -l
  register: calico_count
  changed_when: false
  failed_when: false

- name: Calico CNI ìˆ˜ë™ ì„¤ì¹˜ (ë¯¸ì„¤ì¹˜ ì‹œ)
  command: kubectl apply -f https://raw.githubusercontent.com/projectcalico/calico/v3.26.1/manifests/calico.yaml
  when: calico_count.stdout | int == 0
  register: calico_installed

- name: Calico Pod Ready ëŒ€ê¸°
  command: kubectl wait --for=condition=ready pod -l k8s-app=calico-node -n kube-system --timeout=120s --all
  when: calico_installed.changed

- name: ë…¸ë“œ Ready ìƒíƒœ í™•ì¸
  shell: kubectl get nodes --no-headers | grep -v " Ready " | wc -l
  register: notready_nodes
  changed_when: false
  failed_when: notready_nodes.stdout | int > 0
  retries: 6
  delay: 10
```

**ì»¤ë°‹**: Ansible CNI pre-check ì¶”ê°€ í•„ìš”

---

### 21.4. ArgoCD AppProject ë¯¸ìƒì„±ìœ¼ë¡œ ì¸í•œ Application InvalidSpecError

#### ë¬¸ì œ
**ì¦ìƒ**:
```bash
kubectl get application dev-root -n argocd
NAME       SYNC STATUS   HEALTH STATUS
dev-root   Unknown       Unknown

kubectl describe application dev-root -n argocd
Conditions:
  Message: Application referencing project dev which does not exist
  Type:    InvalidSpecError
```

**ArgoCD controller ë¡œê·¸**:
```json
{"level":"warning","msg":"error getting app project \"dev\": appproject.argoproj.io \"dev\" not found"}
```

#### ì›ì¸
Ansible playbookì´ ArgoCD ì„¤ì¹˜ë§Œ í•˜ê³  AppProjectë¥¼ ìƒì„±í•˜ì§€ ì•ŠìŒ. root-appì€ `spec.project: dev`ë¥¼ ì°¸ì¡°í•˜ëŠ”ë° projectê°€ ì—†ì–´ì„œ ê²€ì¦ ì‹¤íŒ¨.

#### í•´ê²°

**Ansible ê°œì„ ** - `ansible/roles/argocd/tasks/main.yml`ì— ì¶”ê°€:

```yaml
- name: ArgoCD AppProject ìƒì„± (dev)
  shell: |
    kubectl apply -f - <<EOF
    apiVersion: argoproj.io/v1alpha1
    kind: AppProject
    metadata:
      name: dev
      namespace: {{ argocd_namespace }}
    spec:
      description: Development Environment
      sourceRepos:
        - '*'
      destinations:
        - namespace: '*'
          server: '*'
      clusterResourceWhitelist:
        - group: '*'
          kind: '*'
      namespaceResourceWhitelist:
        - group: '*'
          kind: '*'
    EOF
  register: appproject_created
  changed_when: "'created' in appproject_created.stdout or 'configured' in appproject_created.stdout"

- name: ArgoCD AppProject ìƒì„± (prod)
  shell: |
    kubectl apply -f - <<EOF
    apiVersion: argoproj.io/v1alpha1
    kind: AppProject
    metadata:
      name: prod
      namespace: {{ argocd_namespace }}
    spec:
      description: Production Environment
      sourceRepos:
        - '*'
      destinations:
        - namespace: '*'
          server: '*'
      clusterResourceWhitelist:
        - group: '*'
          kind: '*'
      namespaceResourceWhitelist:
        - group: '*'
          kind: '*'
    EOF
  register: appproject_prod_created
  changed_when: "'created' in appproject_prod_created.stdout or 'configured' in appproject_prod_created.stdout"
  when: environment == "prod"
```

**ê²€ì¦**:
```bash
kubectl get appproject -n argocd
# ì˜ˆìƒ ì¶œë ¥:
NAME   AGE
dev    30s
prod   30s  # (if environment=prod)
```

---

### 21.5. ArgoCD NetworkPolicyë¡œ ì¸í•œ DNS Timeout

#### ë¬¸ì œ
**ì¦ìƒ**:
```bash
kubectl get applications -n argocd
NAME       SYNC STATUS   HEALTH STATUS
dev-root   Unknown       Unknown

kubectl logs -n argocd sts/argocd-application-controller
{"level":"warning","msg":"Reconnect to redis because error: \"dial tcp: lookup argocd-redis: i/o timeout\""}
{"level":"warning","msg":"failed to set app resource tree: dial tcp: lookup argocd-repo-server on 10.96.0.10:53: dial udp 10.96.0.10:53: i/o timeout"}
```

**Application describe**:
```yaml
status:
  conditions:
  - message: 'Failed to load target state: rpc error: code = Unavailable 
      desc = dns: A record lookup error: lookup argocd-repo-server on 10.96.0.10:53: 
      dial udp 10.96.0.10:53: i/o timeout'
    type: ComparisonError
```

#### ì›ì¸
ArgoCD ê¸°ë³¸ ì„¤ì¹˜ ë§¤ë‹ˆí˜ìŠ¤íŠ¸ì— í¬í•¨ëœ NetworkPolicyê°€ ë„ˆë¬´ ì œí•œì :
- ArgoCD Application Controller â†’ repo-server DNS ì¡°íšŒ ì°¨ë‹¨
- ArgoCD Components ê°„ í†µì‹  ì°¨ë‹¨

#### í•´ê²°

**ì¦‰ì‹œ ì™„í™”**:
```bash
# ArgoCD NetworkPolicy ì „ì²´ ì‚­ì œ
kubectl delete networkpolicy --all -n argocd

# ArgoCD Pods ì¬ì‹œì‘ (ì„ íƒ)
kubectl rollout restart deployment -n argocd
kubectl rollout restart statefulset -n argocd
```

**ê·¼ë³¸ í•´ê²°** - ArgoCD ì„¤ì¹˜ í›„ NetworkPolicy ì‚­ì œ ìë™í™”:

`ansible/roles/argocd/tasks/main.yml`ì— ì¶”ê°€:
```yaml
- name: ArgoCD ê¸°ë³¸ NetworkPolicy ì‚­ì œ (í†µì‹  ì°¨ë‹¨ ë°©ì§€)
  command: kubectl delete networkpolicy --all -n {{ argocd_namespace }}
  register: netpol_deleted
  changed_when: "'deleted' in netpol_deleted.stdout"
  failed_when: false  # NetworkPolicyê°€ ì—†ì„ ìˆ˜ë„ ìˆìŒ

- name: ArgoCD NetworkPolicy ì‚­ì œ ê²°ê³¼
  debug:
    msg: "{{ netpol_deleted.stdout_lines }}"
  when: netpol_deleted.changed
```

**ì»¤ìŠ¤í…€ NetworkPolicy** (í•„ìš” ì‹œ):
```yaml
# ArgoCD ì „ìš© NetworkPolicy (allow-all)
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: argocd-allow-all
  namespace: argocd
spec:
  podSelector: {}
  policyTypes:
    - Ingress
    - Egress
  ingress:
    - {}
  egress:
    - {}
```

**ê²€ì¦**:
```bash
kubectl get networkpolicy -n argocd
# ì˜ˆìƒ: No resources found (ë˜ëŠ” ì»¤ìŠ¤í…€ ì •ì±…ë§Œ)

kubectl logs -n argocd sts/argocd-application-controller --tail=10
# DNS timeout ì—ëŸ¬ ì—†ì–´ì•¼ í•¨
```

---

### 21.6. ArgoCD Application targetRevision ë¶ˆì¼ì¹˜

#### ë¬¸ì œ
**ì¦ìƒ**:
```bash
# ë¡œì»¬ì—ì„œ ìˆ˜ì •í•˜ê³  ì»¤ë°‹í–ˆì§€ë§Œ í´ëŸ¬ìŠ¤í„°ì— ë°˜ì˜ ì•ˆ ë¨
kubectl get deploy auth-api -n auth -o yaml | grep nodeSelector
      nodeSelector:
        node-role.kubernetes.io/api: auth  # âŒ êµ¬ë²„ì „ ë¼ë²¨ (ìˆ˜ì • ì „)
```

**ArgoCD Application ìƒíƒœ**:
```bash
kubectl get application dev-namespaces -n argocd -o jsonpath='{.status.conditions}'
[{"message":"Failed to load target state: workloads/namespaces/dev: app path does not exist","type":"ComparisonError"}]
```

#### ì›ì¸
**ë¸Œëœì¹˜ ë¶ˆì¼ì¹˜**:
- ë¡œì»¬ ë¸Œëœì¹˜: `refactor/gitops-sync-wave` (ìµœì‹  ìˆ˜ì •ì‚¬í•­ í¬í•¨)
- GitHub default: `main` ë˜ëŠ” `develop`
- ArgoCD Application: `targetRevision: HEAD` (GitHub defaultë¥¼ ê°€ë¦¬í‚´)

**ì˜ˆì‹œ**:
```bash
# ë¡œì»¬
git branch --show-current
refactor/gitops-sync-wave

git log -1 --oneline
f191d18 fix: Ansible ë…¸ë“œ ë¼ë²¨ê³¼ Kubernetes manifest ë™ê¸°í™”

# GitHub default ë¸Œëœì¹˜
git log origin/HEAD -1 --oneline
52920f9 Update README.md  # ìˆ˜ì • ì „ ì»¤ë°‹
```

#### í•´ê²°

**1. ì‘ì—… ë¸Œëœì¹˜ push**:
```bash
git push origin refactor/gitops-sync-wave
```

**2. root-appì˜ targetRevision ë³€ê²½**:
```bash
kubectl patch application dev-root -n argocd --type merge \
  -p '{"spec":{"source":{"targetRevision":"refactor/gitops-sync-wave"}}}'
```

**3. ëª¨ë“  child applicationsì˜ targetRevision ë³€ê²½**:

```bash
# ì¼ê´„ ë³€ê²½
find clusters/dev/apps -name "*.yaml" -type f \
  -exec sed -i '' 's/targetRevision: HEAD/targetRevision: refactor\/gitops-sync-wave/g' {} \;

git add clusters/dev/apps/
git commit -m "fix: update all applications targetRevision to working branch"
git push origin refactor/gitops-sync-wave
```

**4. Applications ì¬ìƒì„±**:
```bash
# root-app ì¬ìƒì„±ìœ¼ë¡œ child applicationsë„ ìë™ ì—…ë°ì´íŠ¸
kubectl delete application dev-root -n argocd
kubectl apply -f /tmp/root-app.yaml
kubectl patch application dev-root -n argocd --type merge \
  -p '{"spec":{"source":{"targetRevision":"refactor/gitops-sync-wave"}}}'
```

**ê²€ì¦**:
```bash
# targetRevision í™•ì¸
kubectl get application dev-namespaces -n argocd -o jsonpath='{.spec.source.targetRevision}'
# ì˜ˆìƒ: refactor/gitops-sync-wave

# Sync ìƒíƒœ í™•ì¸
kubectl get applications -n argocd
# Synced ë˜ëŠ” Progressing ìƒíƒœ
```

**ì¥ê¸° í•´ê²°ì±…**: 
- ì‘ì—… ì™„ë£Œ í›„ main/developì— merge
- productionì€ í•­ìƒ `targetRevision: main` ì‚¬ìš©

**ì»¤ë°‹**: `9d5c34b`, `dbe3d6d`, `e82a025`, `a0e7a0b`, `451e5b0`

---

### 21.7. Kustomize ë””ë ‰í† ë¦¬ êµ¬ì¡° ë¬¸ì œ (platform/crds)

#### ë¬¸ì œ
**ArgoCD sync ì—ëŸ¬**:
```
The Kubernetes API could not find kustomize.config.k8s.io/Kustomization 
for requested resource argocd/. Make sure the "Kustomization" CRD is installed 
on the destination cluster.
```

**Application ì„¤ì •**:
```yaml
# clusters/dev/apps/00-crds.yaml (êµ¬ë²„ì „)
source:
  path: platform/crds
  directory:
    recurse: true  # âŒ ë¬¸ì œì˜ ì›ì¸
```

#### ì›ì¸
`directory.recurse: true`ë¡œ ì¸í•´:
1. ArgoCDê°€ `platform/crds/*/kustomization.yaml` íŒŒì¼ì„ **ë¦¬ì†ŒìŠ¤ë¡œ ë°°í¬**í•˜ë ¤ê³  ì‹œë„
2. Kustomization CRDê°€ í´ëŸ¬ìŠ¤í„°ì— ì—†ì–´ì„œ ì‹¤íŒ¨
3. ìƒìœ„ ë””ë ‰í† ë¦¬ì— `kustomization.yaml`ì´ ì—†ì–´ì„œ kustomize build ë¶ˆê°€

**ë””ë ‰í† ë¦¬ êµ¬ì¡°**:
```
platform/crds/
â”œâ”€â”€ (kustomization.yaml ì—†ìŒ!)  # â† ë¬¸ì œ
â”œâ”€â”€ alb-controller/
â”‚   â””â”€â”€ kustomization.yaml
â”œâ”€â”€ external-secrets/
â”‚   â””â”€â”€ kustomization.yaml
â””â”€â”€ postgres-operator/
    â””â”€â”€ kustomization.yaml
```

#### í•´ê²°

**1. ìƒìœ„ kustomization.yaml ìƒì„±**:
```yaml
# platform/crds/kustomization.yaml (ì‹ ê·œ)
apiVersion: kustomize.config.k8s.io/v1beta1
kind: Kustomization

resources:
  - alb-controller
  - external-secrets
  - postgres-operator
  - prometheus-operator
```

**2. Application ì„¤ì • ìˆ˜ì •**:
```yaml
# clusters/dev/apps/00-crds.yaml (ìˆ˜ì •)
source:
  path: platform/crds
  # directory.recurse ì œê±° - kustomize ìë™ ì¸ì‹
```

**ê²€ì¦**:
```bash
# ë¡œì»¬ kustomize build í…ŒìŠ¤íŠ¸
kubectl kustomize platform/crds | head -20
# CRD ë¦¬ì†ŒìŠ¤ë“¤ì´ ì¶œë ¥ë˜ì–´ì•¼ í•¨

# ArgoCD sync í™•ì¸
kubectl get application dev-crds -n argocd
NAME       SYNC STATUS   HEALTH STATUS
dev-crds   Synced        Healthy  # âœ…
```

**ì»¤ë°‹**: `2a8c747`, `dbe3d6d`

---

### 21.8. ApplicationSet í…œí”Œë¦¿ ë”°ì˜´í‘œ ì˜¤ë¥˜ (k8s ì´ë¦„ ê·œì¹™ ìœ„ë°˜)

#### ë¬¸ì œ
**ApplicationSet ì—ëŸ¬**:
```bash
kubectl get applicationset dev-data-operators -n argocd -o yaml
status:
  conditions:
  - message: 'Application.argoproj.io "dev-\"postgres-operator\"" is invalid: 
      metadata.name: Invalid value: "dev-\"postgres-operator\"": 
      a lowercase RFC 1123 subdomain must consist of lower case alphanumeric 
      characters, ''-'' or ''.'', and must start and end with an alphanumeric character'
    type: ErrorOccurred
```

**child applications ë¯¸ìƒì„±**:
```bash
kubectl get applications -n argocd | grep postgres
# ì•„ë¬´ê²ƒë„ ì—†ìŒ
```

#### ì›ì¸
ApplicationSet í…œí”Œë¦¿ì—ì„œ ì´ë¦„ì— ë”°ì˜´í‘œë¥¼ ì˜ëª» ì‚¬ìš©:

```yaml
# clusters/dev/apps/25-data-operators.yaml (ì˜¤ë¥˜)
template:
  metadata:
    name: dev-"{{name}}"  # âŒ ë”°ì˜´í‘œê°€ ë¦¬í„°ëŸ´ë¡œ ë“¤ì–´ê°
    # ê²°ê³¼: dev-"postgres-operator" (ìœ íš¨í•˜ì§€ ì•Šì€ k8s ì´ë¦„)
```

**Kubernetes ì´ë¦„ ê·œì¹™**:
- ì†Œë¬¸ì ì˜ìˆ«ì, `-`, `.`ë§Œ í—ˆìš©
- `"`ëŠ” í—ˆìš©ë˜ì§€ ì•ŠìŒ

#### í•´ê²°
```yaml
# clusters/dev/apps/25-data-operators.yaml (ìˆ˜ì •)
template:
  metadata:
    name: dev-{{name}}  # âœ… ë”°ì˜´í‘œ ì œê±°
    # ê²°ê³¼: dev-postgres-operator (ìœ íš¨í•œ k8s ì´ë¦„)

# clusters/dev/apps/60-apis-appset.yamlë„ ë™ì¼í•˜ê²Œ ìˆ˜ì •
template:
  metadata:
    name: dev-api-{{name}}  # âœ…
  spec:
    destination:
      namespace: "{{name}}"  # âœ… namespaceëŠ” ë”°ì˜´í‘œ OK (ê°’ìœ¼ë¡œ ì‚¬ìš©)
```

**ìˆ˜ì • ì›ì¹™**:
- âŒ `name: "dev-{{name}}"` - ì „ì²´ë¥¼ ë”°ì˜´í‘œë¡œ ê°ì‹¸ë©´ ë¦¬í„°ëŸ´ì´ ë¨
- âœ… `name: dev-{{name}}` - ë³€ìˆ˜ ì¹˜í™˜ ì •ìƒ ì‘ë™
- âœ… `namespace: "{{name}}"` - ê°’ìœ¼ë¡œ ì‚¬ìš© ì‹œ ë”°ì˜´í‘œ OK

**ê²€ì¦**:
```bash
# ApplicationSet ìƒíƒœ í™•ì¸
kubectl get applicationset dev-data-operators -n argocd -o jsonpath='{.status.conditions}'
# ErrorOccurred ì—†ì–´ì•¼ í•¨

# Child applications ìƒì„± í™•ì¸
kubectl get applications -n argocd | grep postgres
dev-postgres-operator   Unknown   Healthy  # âœ… ìƒì„±ë¨
```

**ì˜í–¥ë°›ì€ íŒŒì¼**:
- `clusters/dev/apps/25-data-operators.yaml`
- `clusters/dev/apps/35-data-cr.yaml` (data-clusters)
- `clusters/dev/apps/60-apis-appset.yaml`

**ì»¤ë°‹**: `e82a025`, `451e5b0`

---

### 21.9. CoreDNS Pending ìœ¼ë¡œ ì¸í•œ í´ëŸ¬ìŠ¤í„° ì „ì²´ ì¥ì• 

#### ë¬¸ì œ
**ì¦ìƒ**:
```bash
kubectl get pods -n kube-system | grep coredns
coredns-5dd5756b68-bmdzb   0/1   Pending   0   21m
coredns-5dd5756b68-pz92s   0/1   Pending   0   21m
```

**Pod describe**:
```
Events:
  Warning  FailedScheduling  11m   default-scheduler  
    0/14 nodes are available: 
    1 node(s) had untolerated taint {domain: auth}, 
    1 node(s) had untolerated taint {domain: character}, 
    ...
    4 node(s) had untolerated taint {sesacthon.io/infrastructure: true}.
```

#### ì›ì¸
**ëª¨ë“  ë…¸ë“œì— taintê°€ ì„¤ì •ë˜ì–´ ìˆì–´ì„œ** CoreDNSê°€ ìŠ¤ì¼€ì¤„ë§ë  ìˆ˜ ì—†ìŒ:

- Master: `node-role.kubernetes.io/control-plane:NoSchedule`
- API ë…¸ë“œë“¤: `domain=auth:NoSchedule`, `domain=my:NoSchedule`, etc.
- Infrastructure: `sesacthon.io/infrastructure=true:NoSchedule`

CoreDNSëŠ” íŠ¹ì • tolerationì´ ì—†ì–´ì„œ ì–´ë””ì—ë„ ë°°ì¹˜ë˜ì§€ ëª»í•¨.

**Ansibleì´ ì„¤ì •í•œ taint ì˜ˆì‹œ**:
```bash
# ansible/playbooks/fix-node-labels.yml
node_labels:
  k8s-api-auth: "--node-labels=... --register-with-taints=domain=auth:NoSchedule"
  k8s-postgresql: "--node-labels=... --register-with-taints=sesacthon.io/infrastructure=true:NoSchedule"
```

#### í•´ê²°

**ê¸´ê¸‰ ë³µêµ¬**:
```bash
# Option 1: Master ë…¸ë“œì˜ taint ì¼ì‹œ ì œê±° (CoreDNS í—ˆìš©)
kubectl taint nodes k8s-master node-role.kubernetes.io/control-plane:NoSchedule-

# Option 2: CoreDNSì— ëª¨ë“  taint toleration ì¶”ê°€
kubectl patch deployment coredns -n kube-system --type merge -p '
{
  "spec": {
    "template": {
      "spec": {
        "tolerations": [
          {"key": "node-role.kubernetes.io/control-plane", "operator": "Exists", "effect": "NoSchedule"},
          {"key": "domain", "operator": "Exists", "effect": "NoSchedule"},
          {"key": "sesacthon.io/infrastructure", "operator": "Exists", "effect": "NoSchedule"}
        ]
      }
    }
  }
}'
```

**ê·¼ë³¸ í•´ê²°** - Ansible ê°œì„ :

Master ë…¸ë“œëŠ” taint ì—†ì´ ë˜ëŠ” CoreDNS ë°°í¬ ê°€ëŠ¥í•˜ë„ë¡ ì„¤ì •:

```yaml
# ansible/playbooks/02-master-init.ymlì— ì¶”ê°€
- name: CoreDNS toleration íŒ¨ì¹˜ (Taintëœ í´ëŸ¬ìŠ¤í„° ëŒ€ì‘)
  shell: |
    kubectl patch deployment coredns -n kube-system --type merge -p '
    {
      "spec": {
        "template": {
          "spec": {
            "tolerations": [
              {"key": "node-role.kubernetes.io/control-plane", "operator": "Exists", "effect": "NoSchedule"},
              {"key": "domain", "operator": "Exists", "effect": "NoSchedule"},
              {"key": "sesacthon.io/infrastructure", "operator": "Exists", "effect": "NoSchedule"},
              {"key": "CriticalAddonsOnly", "operator": "Exists"}
            ]
          }
        }
      }
    }'
  register: coredns_patched
  changed_when: "'patched' in coredns_patched.stdout"

- name: CoreDNS Pod ì¬ì‹œì‘ ëŒ€ê¸°
  command: kubectl rollout status deployment coredns -n kube-system --timeout=120s
  when: coredns_patched.changed
```

**ê²€ì¦**:
```bash
kubectl get pods -n kube-system -l k8s-app=kube-dns
NAME                      READY   STATUS    NODE
coredns-xxx-yyy           1/1     Running   k8s-master  # âœ… ì •ìƒ ë°°ì¹˜
```

---

### 21.10. ë² ìŠ¤íŠ¸ í”„ë™í‹°ìŠ¤: Ansible + GitOps ë™ê¸°í™”

#### Ansible Playbook ê°œì„  ì²´í¬ë¦¬ìŠ¤íŠ¸

**1. ArgoCD ì„¤ì¹˜ ì „ ì¤€ë¹„**:
```yaml
# ansible/roles/argocd/tasks/main.yml
- CNI ì„¤ì¹˜ í™•ì¸ ë° ìë™ ì„¤ì¹˜
- ë…¸ë“œ Ready ëŒ€ê¸°
- CoreDNS toleration íŒ¨ì¹˜
```

**2. ArgoCD ì„¤ì¹˜ í›„ ì„¤ì •**:
```yaml
- AppProject ìƒì„± (dev, prod)
- NetworkPolicy ì‚­ì œ
- root-app ê²½ë¡œ ìˆ˜ì • (clusters/{env}/root-app.yaml)
```

**3. ë…¸ë“œ ë¼ë²¨ ì¼ê´€ì„±**:
```yaml
# Ansibleì´ ì„¤ì •í•˜ëŠ” ë¼ë²¨ê³¼ Kubernetes manifestê°€ ì¼ì¹˜í•´ì•¼ í•¨
- sesacthon.io/service=auth
- sesacthon.io/infra-type=postgresql
- sesacthon.io/worker-type=storage
```

**4. GitOps ë°°í¬ ìˆœì„œ**:
```
Wave 0:  CRDs (kustomization.yaml í•„ìˆ˜)
Wave 2:  Namespaces
Wave 5:  Calico CNI
Wave 6:  NetworkPolicies
Wave 10: External Secrets
Wave 15: ALB Controller
...
Wave 60: API Applications
```

**5. ë¬¸ì„œ ë™ê¸°í™”**:
- `docs/infrastructure/k8s-label-annotation-system.md`: ë…¸ë“œ ë¼ë²¨ ì²´ê³„
- `ansible/playbooks/fix-node-labels.yml`: ì‹¤ì œ ë¼ë²¨ ì„¤ì •
- `workloads/apis/*/base/deployment.yaml`: nodeSelector ì„¤ì •

#### ê²€ì¦ ìŠ¤í¬ë¦½íŠ¸

```bash
# ë…¸ë“œ ë¼ë²¨ê³¼ deployment nodeSelector ì¼ì¹˜ í™•ì¸
for service in auth my scan character location info chat; do
  echo "=== $service ==="
  echo "ë…¸ë“œ ë¼ë²¨:"
  kubectl get nodes -l sesacthon.io/service=$service --show-labels | grep sesacthon.io/service
  echo "Deployment nodeSelector:"
  kubectl get deploy -n $service ${service}-api -o jsonpath='{.spec.template.spec.nodeSelector}' 2>/dev/null
  echo ""
done
```

---

**ìµœì¢… ì—…ë°ì´íŠ¸**: 2025-11-16  
**ë²„ì „**: v0.7.4  
**ì•„í‚¤í…ì²˜**: 14-Node GitOps + Ansible Bootstrap

