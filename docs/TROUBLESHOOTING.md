# Troubleshooting Guide - ì´ì½”ì—ì½”(EcoÂ²)

> 13-Node Microservices Architecture + Worker Local SQLite WAL êµ¬ì¶• ê³¼ì •ì—ì„œ ë°œìƒí•œ ë¬¸ì œ ë° í•´ê²° ë°©ì•ˆ

## ğŸ“‹ ëª©ì°¨

- [1. Terraform ê´€ë ¨ ë¬¸ì œ](#1-terraform-ê´€ë ¨-ë¬¸ì œ)
- [2. IAM Policy ì¤‘ë³µ ë¬¸ì œ](#2-iam-policy-ì¤‘ë³µ-ë¬¸ì œ)
- [3. AWS í•œë„ ê´€ë ¨ ë¬¸ì œ](#3-aws-í•œë„-ê´€ë ¨-ë¬¸ì œ)
- [4. ìŠ¤í¬ë¦½íŠ¸ ì‹¤í–‰ ë¬¸ì œ](#4-ìŠ¤í¬ë¦½íŠ¸-ì‹¤í–‰-ë¬¸ì œ)
- [5. GitHub CLI ì¸ì¦ ë¬¸ì œ](#5-github-cli-ì¸ì¦-ë¬¸ì œ)
- [6. CloudFront ê´€ë ¨ ë¬¸ì œ](#6-cloudfront-ê´€ë ¨-ë¬¸ì œ)

---

## 1. Terraform ê´€ë ¨ ë¬¸ì œ

### 1.1. Duplicate Resource Configuration

#### ë¬¸ì œ
```
Error: Duplicate resource "aws_iam_policy" configuration
Error: Duplicate resource "aws_iam_role_policy_attachment" configuration
```

**ì›ì¸**: ë™ì¼í•œ ë¦¬ì†ŒìŠ¤ê°€ ì—¬ëŸ¬ íŒŒì¼ì— ì„ ì–¸ë¨
- `terraform/iam.tf`ì™€ `terraform/alb-controller-iam.tf`ì— `aws_iam_policy.alb_controller` ì¤‘ë³µ

#### í•´ê²°
```bash
# iam.tfì—ì„œ ì¤‘ë³µ ì„ ì–¸ ì œê±°
# alb-controller-iam.tfì—ë§Œ ìœ ì§€
```

**ì ìš© íŒŒì¼**:
- `terraform/iam.tf` (ì¤‘ë³µ ì œê±°)
- `terraform/alb-controller-iam.tf` (ìœ ì§€)

**ì»¤ë°‹**: `feat: Remove duplicate IAM policy declarations`

---

### 1.2. Provider Configuration Not Present

#### ë¬¸ì œ
```
Error: Provider configuration not present
To work with aws_acm_certificate.cdn its original provider configuration at 
provider["registry.terraform.io/hashicorp/aws"].us_east_1 is required
```

**ì›ì¸**: CloudFront ACM ì¸ì¦ì„œëŠ” `us-east-1` ë¦¬ì „ì— ìˆì–´ì•¼ í•˜ì§€ë§Œ provider ì„¤ì • ëˆ„ë½

#### í•´ê²°
`terraform/main.tf`ì— `us-east-1` provider ì¶”ê°€:

```hcl
# CloudFront requires ACM certificate in us-east-1
provider "aws" {
  alias  = "us_east_1"
  region = "us-east-1"
  
  default_tags {
    tags = {
      Project     = "SeSACTHON"
      ManagedBy   = "Terraform"
      Environment = var.environment
      Team        = "Backend"
    }
  }
}
```

**ì»¤ë°‹**: `fix: Add us-east-1 provider for CloudFront ACM certificates`

---

### 1.3. Reference to Undeclared Resource

#### ë¬¸ì œ
```
Error: Reference to undeclared resource
A managed resource "aws_iam_role" "ec2_ssm_role" has not been declared
```

**ì›ì¸**: ì˜ëª»ëœ IAM role ì°¸ì¡°
- `alb-controller-iam.tf`ì™€ `s3.tf`ì—ì„œ `aws_iam_role.ec2_ssm_role` ì°¸ì¡°
- ì‹¤ì œ ë¦¬ì†ŒìŠ¤ ì´ë¦„ì€ `aws_iam_role.k8s_node`

#### í•´ê²°
```hcl
# Before
role = aws_iam_role.ec2_ssm_role.name

# After
role = aws_iam_role.k8s_node.name
```

**ì ìš© íŒŒì¼**:
- `terraform/alb-controller-iam.tf`
- `terraform/s3.tf`

**ì»¤ë°‹**: `fix: Correct IAM role reference from ec2_ssm_role to k8s_node`

---

### 1.4. Missing Resource Instance Key

#### ë¬¸ì œ
```
Error: Missing resource instance key
Because data.aws_route53_zone.main has "count" set, its attributes must be 
accessed on specific instances.
```

**ì›ì¸**: `count` ê¸°ë°˜ ë¦¬ì†ŒìŠ¤ë¥¼ ì¸ë±ìŠ¤ ì—†ì´ ì°¸ì¡°

#### í•´ê²°
```hcl
# Before
zone_id = data.aws_route53_zone.main.zone_id

# After
zone_id = data.aws_route53_zone.main[0].zone_id
```

**ì ìš© íŒŒì¼**: `terraform/cloudfront.tf`

**ì»¤ë°‹**: `fix: Add index to count-based Route53 zone reference`

---

### 1.5. Invalid Attribute Combination (S3 Lifecycle)

#### ë¬¸ì œ
```
Warning: Invalid Attribute Combination
No attribute specified when one (and only one) of [rule[0].filter,rule[0].prefix] is required
```

**ì›ì¸**: S3 lifecycle ruleì— `filter` ë˜ëŠ” `prefix` í•„ìˆ˜

#### í•´ê²°
```hcl
resource "aws_s3_bucket_lifecycle_configuration" "images" {
  bucket = aws_s3_bucket.images.id

  rule {
    id     = "cleanup-old-images"
    status = "Enabled"

    filter {
      prefix = ""  # ëª¨ë“  ê°ì²´ì— ì ìš©
    }

    transition {
      days          = 30
      storage_class = "STANDARD_IA"
    }

    expiration {
      days = 90
    }
  }
}
```

**ì ìš© íŒŒì¼**: `terraform/s3.tf`

**ì»¤ë°‹**: `fix: Add filter block to S3 lifecycle configuration`

---

### 1.6. No Configuration Files

#### ë¬¸ì œ
```
Error: No configuration files
Apply requires configuration to be present.
```

**ì›ì¸**: `auto-rebuild.sh`ì˜ Step 2ì—ì„œ terraform ë””ë ‰í† ë¦¬ë¡œ ì´ë™í•˜ì§€ ì•ŠìŒ

#### í•´ê²°
```bash
# Step 2 ì‹œì‘ ì‹œ ëª…ì‹œì ìœ¼ë¡œ ë””ë ‰í† ë¦¬ ì´ë™
cd "$PROJECT_ROOT/terraform"

echo "ğŸ”§ Terraform ì´ˆê¸°í™” (ì¬í™•ì¸)..."
terraform init -migrate-state -upgrade
```

**ì ìš© íŒŒì¼**: `scripts/cluster/auto-rebuild.sh`

**ì»¤ë°‹**: `fix: Add missing cd to terraform directory in auto-rebuild.sh Step 2`

---

## 2. IAM Policy ì¤‘ë³µ ë¬¸ì œ

### 2.1. EntityAlreadyExists - IAM Policy

#### ë¬¸ì œ
```
Error: creating IAM Policy (prod-alb-controller-policy): EntityAlreadyExists
Error: creating IAM Policy (prod-s3-presigned-url-policy): EntityAlreadyExists
```

**ì›ì¸**: ì´ì „ ë°°í¬ì˜ IAM Policyê°€ Terraform stateì— ì—†ì§€ë§Œ AWSì— ë‚¨ì•„ìˆìŒ

#### í•´ê²°
`scripts/maintenance/destroy-with-cleanup.sh`ì— IAM Policy ê°•ì œ ì •ë¦¬ ì¶”ê°€:

```bash
# IAM Policy ê°•ì œ ì •ë¦¬
echo "ğŸ” IAM Policy ê°•ì œ ì •ë¦¬..."
POLICY_ARNS=$(aws iam list-policies --scope Local \
    --query "Policies[?contains(PolicyName, 'alb-controller') || contains(PolicyName, 'ecoeco') || contains(PolicyName, 's3-presigned-url')].Arn" \
    --output text)

if [ -n "$POLICY_ARNS" ]; then
    for policy_arn in $POLICY_ARNS; do
        # Roleì—ì„œ detach
        ATTACHED_ROLES=$(aws iam list-entities-for-policy \
            --policy-arn "$policy_arn" \
            --entity-filter Role \
            --query 'PolicyRoles[*].RoleName' \
            --output text)
        
        for role in $ATTACHED_ROLES; do
            aws iam detach-role-policy --role-name "$role" --policy-arn "$policy_arn"
        done
        
        # Policy ì‚­ì œ
        aws iam delete-policy --policy-arn "$policy_arn"
    done
fi
```

**ì»¤ë°‹**: `feat: Enhance destroy-with-cleanup.sh with comprehensive AWS resource cleanup`

---

### 2.2. auto-rebuild.sh í†µí•©

#### ë¬¸ì œ
`auto-rebuild.sh`ê°€ ë³µì¡í•œ ì •ë¦¬ ë¡œì§ì„ ì§ì ‘ êµ¬í˜„í•˜ì—¬ ìœ ì§€ë³´ìˆ˜ ì–´ë ¤ì›€

#### í•´ê²°
`destroy-with-cleanup.sh`ë¥¼ í˜¸ì¶œí•˜ë„ë¡ ë¦¬íŒ©í† ë§:

```bash
# Step 1: Terraform Destroy (destroy-with-cleanup.sh í˜¸ì¶œ)
if [ -f "$PROJECT_ROOT/scripts/maintenance/destroy-with-cleanup.sh" ]; then
    echo "ğŸ”§ destroy-with-cleanup.sh ì‹¤í–‰ ì¤‘..."
    export AUTO_MODE=true
    bash "$PROJECT_ROOT/scripts/maintenance/destroy-with-cleanup.sh"
else
    # Fallback: ê°„ë‹¨í•œ destroy
    terraform destroy -auto-approve
fi
```

**íš¨ê³¼**:
- 200+ ì¤„ ì½”ë“œ ì œê±°
- ë‹¨ì¼ ì •ë¦¬ ë¡œì§ìœ¼ë¡œ í†µí•©
- IAM, S3, CloudFront, Route53, ACM ëª¨ë‘ ìë™ ì •ë¦¬

**ì»¤ë°‹**: `feat: Integrate destroy-with-cleanup.sh into auto-rebuild.sh + S3 Policy cleanup`

---

## 3. AWS í•œë„ ê´€ë ¨ ë¬¸ì œ

### 3.1. VcpuLimitExceeded

#### ë¬¸ì œ
```
Error: creating EC2 Instance: VcpuLimitExceeded
You have requested more vCPU capacity than your current vCPU limit of 16 allows
```

**ì›ì¸**: 13-Node ì•„í‚¤í…ì²˜ í•„ìš” vCPU (26) > ê³„ì • í•œë„ (16)

**vCPU ê³„ì‚°**:
| ë…¸ë“œ íƒ€ì… | ê°œìˆ˜ | ì¸ìŠ¤í„´ìŠ¤ | vCPU/ë…¸ë“œ | ì´ vCPU |
|----------|------|---------|-----------|---------|
| Master | 1 | t3a.large | 2 | 2 |
| API | 6 | t3a.medium | 2 | 12 |
| Worker | 2 | t3a.large | 2 | 4 |
| Infrastructure | 4 | t3a.medium | 2 | 8 |
| **í•©ê³„** | **13** | | | **26** |

#### í•´ê²° ë°©ë²• 1: vCPU í•œë„ ì¦ê°€ ìš”ì²­ (ê¶Œì¥)

**ìë™ ìŠ¤í¬ë¦½íŠ¸**:
```bash
./scripts/utilities/request-vcpu-increase.sh
```

**ìˆ˜ë™ ìš”ì²­**:
```bash
aws service-quotas request-service-quota-increase \
    --service-code ec2 \
    --quota-code L-1216C47A \
    --desired-value 32 \
    --region ap-northeast-2
```

**AWS Console**: Service Quotas â†’ EC2 â†’ "Running On-Demand Standard instances"

**ìŠ¹ì¸ ì‹œê°„**: 15ë¶„-2ì‹œê°„ (ì¼ë°˜ì )

**í•œë„ í™•ì¸**:
```bash
aws service-quotas get-service-quota \
    --service-code ec2 \
    --quota-code L-1216C47A \
    --region ap-northeast-2 \
    --query 'Quota.Value'
```

#### í•´ê²° ë°©ë²• 2: ì¸ìŠ¤í„´ìŠ¤ íƒ€ì… ë‹¤ìš´ê·¸ë ˆì´ë“œ (ì„ì‹œ)

âš ï¸ **ì„±ëŠ¥ ì €í•˜ ìˆìŒ** - ê°œë°œ/í…ŒìŠ¤íŠ¸ìš©ìœ¼ë¡œë§Œ ê¶Œì¥

`terraform.tfvars` ìƒì„±:
```hcl
master_instance_type = "t3a.small"   # 2 vCPU (ê¸°ì¡´: large)
api_instance_type = "t3a.micro"      # 2 vCPU (ê¸°ì¡´: medium)
worker_instance_type = "t3a.medium"  # 2 vCPU (ê¸°ì¡´: large)
infra_instance_type = "t3a.micro"    # 2 vCPU (ê¸°ì¡´: medium)
```

**ì´ vCPU**: 16 (í•œë„ ë‚´)

**ì»¤ë°‹**: `feat: Add vCPU quota increase request script`

---

### 3.2. ResourceAlreadyExistsException

#### ë¬¸ì œ
```
Error: Only one open service quota increase request is allowed per quota.
```

**ì›ì¸**: ì´ë¯¸ ì§„í–‰ ì¤‘ì¸ í•œë„ ì¦ê°€ ìš”ì²­ì´ ìˆìŒ

#### í™•ì¸
```bash
aws service-quotas list-requested-service-quota-change-history-by-quota \
    --service-code ec2 \
    --quota-code L-1216C47A \
    --region ap-northeast-2 \
    --query 'RequestedQuotas[?Status==`PENDING` || Status==`CASE_OPENED`]'
```

**Status**:
- `CASE_OPENED`: AWSê°€ ê²€í†  ì¤‘ (ì¢‹ì€ ì‹ í˜¸!)
- `PENDING`: ëŒ€ê¸° ì¤‘
- `APPROVED`: ìŠ¹ì¸ ì™„ë£Œ
- `DENIED`: ê±°ì ˆ (ë“œë¬¼ìŒ)

#### í•´ê²°
ê¸°ì¡´ ìš”ì²­ì´ ì²˜ë¦¬ë  ë•Œê¹Œì§€ ëŒ€ê¸° (5-10ë¶„ë§ˆë‹¤ í•œë„ í™•ì¸)

---

## 4. ìŠ¤í¬ë¦½íŠ¸ ì‹¤í–‰ ë¬¸ì œ

### 4.1. InvalidKeyPair.Duplicate

#### ë¬¸ì œ
```
Error: importing EC2 Key Pair (k8s-cluster-key): InvalidKeyPair.Duplicate
```

**ì›ì¸**: ì´ì „ Key Pairê°€ ì‚­ì œë˜ì§€ ì•ŠìŒ

#### í•´ê²°
`destroy-with-cleanup.sh`ì— Key Pair ì •ë¦¬ ì¶”ê°€:

```bash
KEY_NAME="k8s-cluster-key"
if aws ec2 describe-key-pairs --key-names "$KEY_NAME" --region "$AWS_REGION" >/dev/null 2>&1; then
    aws ec2 delete-key-pair --key-name "$KEY_NAME" --region "$AWS_REGION"
fi
```

---

### 4.2. BucketAlreadyOwnedByYou

#### ë¬¸ì œ
```
Error: creating S3 Bucket (prod-sesacthon-images): BucketAlreadyOwnedByYou
```

**ì›ì¸**: S3 ë²„í‚·ì´ ì‚­ì œë˜ì§€ ì•Šê³  ë‚¨ì•„ìˆìŒ

#### í•´ê²°
`destroy-with-cleanup.sh`ì— S3 Bucket ì •ë¦¬ ì¶”ê°€:

```bash
BUCKETS=$(aws s3api list-buckets \
    --query "Buckets[?starts_with(Name, 'prod-sesacthon')].Name" \
    --output text)

for bucket in $BUCKETS; do
    # ë²„í‚· ë‚´ìš©ë¬¼ ì‚­ì œ
    aws s3 rm "s3://$bucket" --recursive
    # ë²„í‚· ì‚­ì œ
    aws s3api delete-bucket --bucket "$bucket" --region "$AWS_REGION"
done
```

---

## 5. GitHub CLI ì¸ì¦ ë¬¸ì œ

### 5.1. Missing Required Scope

#### ë¬¸ì œ
```
error validating token: missing required scope 'read:org'
```

**ì›ì¸**: GitHub Personal Access Tokenì— í•„ìš”í•œ scope ëˆ„ë½

#### í•´ê²° ë°©ë²• 1: ëŒ€í™”í˜• ë¡œê·¸ì¸ (ê¶Œì¥)

```bash
gh auth login
# GitHub.com ì„ íƒ
# HTTPS ì„ íƒ
# Login with a web browser ì„ íƒ
# ë¸Œë¼ìš°ì €ì—ì„œ ì½”ë“œ ì…ë ¥
```

#### í•´ê²° ë°©ë²• 2: ìƒˆ PAT ìƒì„±

1. https://github.com/settings/tokens ì ‘ì†
2. "Generate new token (classic)" í´ë¦­
3. í•„ìš”í•œ scopes ì„ íƒ:
   - âœ… `repo` (ì „ì²´)
   - âœ… `read:org`
   - âœ… `write:packages`
4. í† í° ìƒì„± í›„:
```bash
echo "<your-token>" | gh auth login --with-token
```

#### í•´ê²° ë°©ë²• 3: GitHub Web UI (ìˆ˜ë™)

1. Repository â†’ Settings â†’ Secrets and variables â†’ Actions
2. Secret ë“±ë¡:
   - `GITHUB_USERNAME`: `mangowhoiscloud`
3. Variable ë“±ë¡:
   - `VERSION`: `v0.6.0`

âš ï¸ **ì£¼ì˜**: `GITHUB_TOKEN`ì€ GitHub Actionsì—ì„œ ìë™ìœ¼ë¡œ ì œê³µë˜ë¯€ë¡œ ë“±ë¡ ë¶ˆí•„ìš”

---

## 6. CloudFront ê´€ë ¨ ë¬¸ì œ

### 6.1. CloudFront ìƒì„± ì‹œê°„

#### í˜„ìƒ
```
aws_cloudfront_distribution.images: Still creating... [6m0s elapsed]
```

**ì›ì¸**: CloudFront distribution ìƒì„±ì€ ì „ì„¸ê³„ edge locationì— ë°°í¬ë˜ë¯€ë¡œ ì‹œê°„ ì†Œìš”

**ì •ìƒ ë²”ìœ„**: 5-15ë¶„

**í™•ì¸**:
```bash
aws cloudfront get-distribution --id <distribution-id> \
    --query 'Distribution.Status'
```

**Status**:
- `InProgress`: ë°°í¬ ì¤‘ (ì •ìƒ)
- `Deployed`: ë°°í¬ ì™„ë£Œ

---

### 6.2. CloudFront ì‚­ì œ í•„ìš”ì„±

#### ë¬¸ì œ
CloudFrontê°€ ë‚¨ì•„ìˆìœ¼ë©´:
- S3 ë²„í‚· ì‚­ì œ ë¶ˆê°€
- ë¹„ìš© ì§€ì† ë°œìƒ ($0.085/GB + ìš”ì²­ ë¹„ìš©)
- Route53 ë ˆì½”ë“œ ì¶©ëŒ

#### í•´ê²°
`destroy-with-cleanup.sh`ì— CloudFront ì •ë¦¬ ì¶”ê°€:

```bash
# 1. Distribution disable
CONFIG=$(aws cloudfront get-distribution-config --id "$dist_id" --output json)
ETAG=$(echo "$CONFIG" | jq -r '.ETag')
NEW_CONFIG=$(echo "$CONFIG" | jq '.DistributionConfig | .Enabled = false')

aws cloudfront update-distribution \
    --id "$dist_id" \
    --if-match "$ETAG" \
    --distribution-config "$NEW_CONFIG"

# 2. Disabled ìƒíƒœ ëŒ€ê¸° (2ë¶„)
sleep 120

# 3. Distribution ì‚­ì œ
FINAL_CONFIG=$(aws cloudfront get-distribution-config --id "$dist_id" --output json)
FINAL_ETAG=$(echo "$FINAL_CONFIG" | jq -r '.ETag')

aws cloudfront delete-distribution --id "$dist_id" --if-match "$FINAL_ETAG"
```

**ì†Œìš” ì‹œê°„**: 5-10ë¶„

---

## 7. ë² ìŠ¤íŠ¸ í”„ë™í‹°ìŠ¤

### 7.1. ì¬êµ¬ì¶• ì „ ì²´í¬ë¦¬ìŠ¤íŠ¸

```bash
# 1. vCPU í•œë„ í™•ì¸
aws service-quotas get-service-quota \
    --service-code ec2 \
    --quota-code L-1216C47A \
    --region ap-northeast-2 \
    --query 'Quota.Value'
# ê²°ê³¼: 32.0 ì´ìƒì´ì–´ì•¼ í•¨

# 2. ì´ì „ ë¦¬ì†ŒìŠ¤ ì™„ì „ ì •ë¦¬
./scripts/maintenance/destroy-with-cleanup.sh

# 3. í™˜ê²½ ë³€ìˆ˜ ì„¤ì •
export GITHUB_TOKEN="<your-token>"
export GITHUB_USERNAME="<your-username>"
export VERSION="v0.6.0"

# 4. ì¬êµ¬ì¶• ì‹¤í–‰
./scripts/cluster/auto-rebuild.sh
```

---

### 7.2. ë””ë²„ê¹… ëª…ë ¹ì–´ ëª¨ìŒ

```bash
# Terraform ìƒíƒœ í™•ì¸
terraform state list
terraform output -json

# AWS ë¦¬ì†ŒìŠ¤ í™•ì¸
aws ec2 describe-instances --region ap-northeast-2
aws iam list-policies --scope Local
aws s3api list-buckets

# Kubernetes ìƒíƒœ í™•ì¸
kubectl get nodes -o wide
kubectl get pods -A
kubectl get svc -A

# GitHub ì¸ì¦ ìƒíƒœ
gh auth status

# ë¡œê·¸ í™•ì¸
tail -f /var/log/cloud-init-output.log  # Master ë…¸ë“œì—ì„œ
journalctl -u kubelet -f                 # Worker ë…¸ë“œì—ì„œ
```

---

### 7.3. ë¬¸ì œ ë°œìƒ ì‹œ ëŒ€ì‘ ìˆœì„œ

1. **ì—ëŸ¬ ë©”ì‹œì§€ í™•ì¸**: ì •í™•í•œ ì—ëŸ¬ ë‚´ìš© íŒŒì•…
2. **ì´ ë¬¸ì„œ ê²€ìƒ‰**: ìœ ì‚¬í•œ ë¬¸ì œ í•´ê²° ë°©ë²• í™•ì¸
3. **AWS ìƒíƒœ í™•ì¸**: ë¦¬ì†ŒìŠ¤ê°€ ì‹¤ì œë¡œ ë‚¨ì•„ìˆëŠ”ì§€ í™•ì¸
4. **ì •ë¦¬ ìŠ¤í¬ë¦½íŠ¸ ì‹¤í–‰**: `destroy-with-cleanup.sh`
5. **í•œë„ í™•ì¸**: vCPU, IAM Policy ë“±
6. **ì¬ì‹œë„**: ì •ë¦¬ í›„ ì¬êµ¬ì¶•

---

## 8. ì°¸ê³  ë¬¸ì„œ

- [AWS Service Quotas Documentation](https://docs.aws.amazon.com/servicequotas/)
- [Terraform AWS Provider](https://registry.terraform.io/providers/hashicorp/aws/latest/docs)
- [GitHub CLI Authentication](https://cli.github.com/manual/gh_auth_login)
- [CloudFront Developer Guide](https://docs.aws.amazon.com/cloudfront/)

---

## 9. ì§€ì›

ë¬¸ì œê°€ í•´ê²°ë˜ì§€ ì•Šìœ¼ë©´:
- GitHub Issues: https://github.com/mangowhoiscloud/backend/issues
- AWS Support: https://console.aws.amazon.com/support/
- Terraform Registry: https://discuss.hashicorp.com/

---

**ìµœì¢… ì—…ë°ì´íŠ¸**: 2025-11-07  
**ë²„ì „**: v0.6.0  
**ì•„í‚¤í…ì²˜**: 13-Node Microservices + Worker Local SQLite WAL

