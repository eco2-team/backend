# Worker Layer ì•„í‚¤í…ì²˜

ì‘ì„± ì¼ì‹œ: 2025-11-06
ì‹œìŠ¤í…œ: Growbin Backend (13-Node Cluster)
ì°¸ê³ : [Celery ë™ì‘ ë°©ì‹](./CELERY_ARCHITECTURE.md), Instagram Architecture

---

## ğŸ“š ëª©ì°¨

1. [Worker Layer ê°œìš”](#1-worker-layer-ê°œìš”)
2. [Growbin Worker êµ¬ì¡°](#2-growbin-worker-êµ¬ì¡°)
3. [Worker íƒ€ì…ë³„ ìƒì„¸ ì„¤ëª…](#3-worker-íƒ€ì…ë³„-ìƒì„¸-ì„¤ëª…)
4. [WAL í†µí•© ì•„í‚¤í…ì²˜](#4-wal-í†µí•©-ì•„í‚¤í…ì²˜)
5. [Worker ë°°í¬ ì „ëµ](#5-worker-ë°°í¬-ì „ëµ)
6. [ëª¨ë‹ˆí„°ë§ ë° ì¥ì•  ì²˜ë¦¬](#6-ëª¨ë‹ˆí„°ë§-ë°-ì¥ì• -ì²˜ë¦¬)
7. [ì„±ëŠ¥ ìµœì í™”](#7-ì„±ëŠ¥-ìµœì í™”)
8. [Best Practices](#8-best-practices)

---

## 1. Worker Layer ê°œìš”

### 1.1 ì •ì˜

```
Worker Layer = Celery Workerë¥¼ ì‹¤í–‰í•˜ëŠ” ì¸í”„ë¼ ê³„ì¸µ
  - ë¹„ë™ê¸° ì‘ì—… ì‹¤í–‰
  - I/O ì§‘ì•½ì  ì‘ì—… (S3, API í˜¸ì¶œ)
  - AI ëª¨ë¸ ì¶”ë¡  (GPT-5, GPT-4o mini)
```

### 1.2 Growbinì—ì„œì˜ ì—­í• 

```mermaid
graph TB
    subgraph API["API Layer (ë™ê¸° ì²˜ë¦¬)"]
        Request["ì‚¬ìš©ì ìš”ì²­ ìˆ˜ì‹ "]
        Auth["ì¸ì¦/ì¸ê°€"]
        Publish["Task ë°œí–‰ (RabbitMQ)"]
        Response["ì¦‰ì‹œ ì‘ë‹µ<br/>(task_id ë°˜í™˜)"]
    end
    
    subgraph Worker["Worker Layer (ë¹„ë™ê¸° ì²˜ë¦¬) â­"]
        S3["S3 ì—…ë¡œë“œ/ë‹¤ìš´ë¡œë“œ"]
        GPT5["GPT-5 Vision API í˜¸ì¶œ<br/>(30ì´ˆ)"]
        GPT4["GPT-4o mini API í˜¸ì¶œ<br/>(10ì´ˆ)"]
        Rules["JSON ê·œì¹™ ì¡°íšŒ"]
        Save["ê²°ê³¼ ì €ì¥ (PostgreSQL)"]
        WAL["ë¡œì»¬ WAL ì˜ì†í™”"]
    end
    
    Request --> Auth
    Auth --> Publish
    Publish --> Response
    Publish -.->|Async| S3
    S3 --> GPT5
    GPT5 --> Rules
    Rules --> GPT4
    GPT4 --> Save
    GPT4 --> WAL
    
    style Request fill:#cce5ff,stroke:#007bff,stroke-width:2px,color:#000
    style Auth fill:#FFE066,stroke:#F59F00,stroke-width:2px,color:#000
    style Publish fill:#F39C12,stroke:#C87F0A,stroke-width:3px,color:#000
    style Response fill:#51CF66,stroke:#2F9E44,stroke-width:2px,color:#fff
    style S3 fill:#9370DB,stroke:#5A478A,stroke-width:2px,color:#fff
    style GPT5 fill:#7B68EE,stroke:#4B3C8C,stroke-width:3px,color:#fff
    style GPT4 fill:#7B68EE,stroke:#4B3C8C,stroke-width:3px,color:#fff
    style Rules fill:#9370DB,stroke:#5A478A,stroke-width:2px,color:#fff
    style Save fill:#3498DB,stroke:#2874A6,stroke-width:2px,color:#fff
    style WAL fill:#E74C3C,stroke:#C0392B,stroke-width:3px,color:#fff
```

**ì¥ì **:
- âœ… API ì‘ë‹µ ì†ë„ í–¥ìƒ (ì¦‰ì‹œ ë°˜í™˜)
- âœ… ë¦¬ì†ŒìŠ¤ ê²©ë¦¬ (API vs Worker)
- âœ… ìŠ¤ì¼€ì¼ ë…ë¦½ (Workerë§Œ ì¦ì„¤)
- âœ… ì¥ì•  ê²©ë¦¬ (Worker ë‹¤ìš´ â†’ API ì •ìƒ)

---

## 2. Growbin Worker êµ¬ì¡°

### 2.1 ì „ì²´ ì•„í‚¤í…ì²˜

```mermaid
graph TB
    subgraph Storage["Worker-Storage (t3.medium, 4GB, 40GB)"]
        IU["image-uploader<br/>(4 processes)"]
        RR["rule-retriever<br/>(2 processes)"]
        TS["task-scheduler<br/>(Celery Beat, 1 process)"]
        SWAL[("ë¡œì»¬ SQLite WAL â­<br/>/var/lib/growbin/worker-storage/task_queue.db<br/>â”œâ”€ task_wal<br/>â”œâ”€ sync_log<br/>â””â”€ file_cache")]
        SProm["Prometheus Exporter<br/>:9090/metrics"]
    end
    
    subgraph AI["Worker-AI (t3.medium, 4GB, 40GB)"]
        GPT5["gpt5-analyzer<br/>(3 processes)"]
        GPT4["response-generator<br/>(3 processes)"]
        AWAL[("ë¡œì»¬ SQLite WAL â­<br/>/var/lib/growbin/worker-ai/task_queue.db<br/>â”œâ”€ task_wal<br/>â”œâ”€ gpt_cache<br/>â”œâ”€ retry_queue<br/>â””â”€ rate_limit_log")]
        AProm["Prometheus Exporter<br/>:9090/metrics"]
    end
    
    IU -.-> SWAL
    RR -.-> SWAL
    TS -.-> SWAL
    GPT5 -.-> AWAL
    GPT4 -.-> AWAL
    
    style IU fill:#9370DB,stroke:#5A478A,stroke-width:3px,color:#fff
    style RR fill:#9370DB,stroke:#5A478A,stroke-width:2px,color:#fff
    style TS fill:#51CF66,stroke:#2F9E44,stroke-width:2px,color:#fff
    style SWAL fill:#E74C3C,stroke:#C0392B,stroke-width:3px,color:#fff
    style SProm fill:#2ECC71,stroke:#27AE60,stroke-width:2px,color:#fff
    style GPT5 fill:#7B68EE,stroke:#4B3C8C,stroke-width:3px,color:#fff
    style GPT4 fill:#7B68EE,stroke:#4B3C8C,stroke-width:3px,color:#fff
    style AWAL fill:#E74C3C,stroke:#C0392B,stroke-width:3px,color:#fff
    style AProm fill:#2ECC71,stroke:#27AE60,stroke-width:2px,color:#fff
```

### 2.2 Worker ë¶„ë¦¬ ê¸°ì¤€

```mermaid
graph LR
    subgraph StorageW["Worker-Storage (I/O Bound)"]
        S3["S3 ì—…ë¡œë“œ/ë‹¤ìš´ë¡œë“œ"]
        File["íŒŒì¼ ì½ê¸°/ì“°ê¸°"]
        DBQ["DB ì¿¼ë¦¬ (ê²½ëŸ‰)"]
        Pool1["Eventlet Pool<br/>(1000 concurrency)"]
    end
    
    subgraph AIW["Worker-AI (Network Bound)"]
        GPT5API["GPT-5 Vision API<br/>(30ì´ˆ)"]
        GPT4API["GPT-4o mini API<br/>(10ì´ˆ)"]
        RateLimit["ì™¸ë¶€ API Rate Limit ê´€ë¦¬"]
        Pool2["Prefork Pool<br/>(3-4 concurrency)"]
    end
    
    S3 --> Pool1
    File --> Pool1
    DBQ --> Pool1
    GPT5API --> Pool2
    GPT4API --> Pool2
    RateLimit --> Pool2
    
    style S3 fill:#9370DB,stroke:#5A478A,stroke-width:2px,color:#fff
    style File fill:#9370DB,stroke:#5A478A,stroke-width:2px,color:#fff
    style DBQ fill:#9370DB,stroke:#5A478A,stroke-width:2px,color:#fff
    style Pool1 fill:#51CF66,stroke:#2F9E44,stroke-width:3px,color:#fff
    style GPT5API fill:#7B68EE,stroke:#4B3C8C,stroke-width:3px,color:#fff
    style GPT4API fill:#7B68EE,stroke:#4B3C8C,stroke-width:3px,color:#fff
    style RateLimit fill:#F39C12,stroke:#C87F0A,stroke-width:2px,color:#000
    style Pool2 fill:#E74C3C,stroke:#C0392B,stroke-width:3px,color:#fff
```

---

## 3. Worker íƒ€ì…ë³„ ìƒì„¸ ì„¤ëª…

### 3.1 image-uploader (Worker-Storage)

**ì—­í• **: S3 ì´ë¯¸ì§€ ì—…ë¡œë“œ ë° ì „ì²˜ë¦¬

```python
# workers/storage/tasks/image_uploader.py
from celery import Task
from celery_app import app
import boto3
from PIL import Image

class ImageUploaderTask(Task):
    """ì´ë¯¸ì§€ ì—…ë¡œë“œ Task Base"""
    
    def on_success(self, retval, task_id, args, kwargs):
        """ì—…ë¡œë“œ ì„±ê³µ ì‹œ"""
        # ë¡œì»¬ WAL ì—…ë°ì´íŠ¸
        local_wal.update_status(task_id, 'completed')
        
        # PostgreSQL ë¹„ë™ê¸° ë™ê¸°í™”
        sync_to_postgres.delay(task_id)
    
    def on_failure(self, exc, task_id, args, kwargs, einfo):
        """ì—…ë¡œë“œ ì‹¤íŒ¨ ì‹œ"""
        # ë¡œì»¬ WALì— ì—ëŸ¬ ê¸°ë¡
        local_wal.update_status(task_id, 'failed', error=str(exc))

@app.task(
    base=ImageUploaderTask,
    bind=True,
    autoretry_for=(S3Error, NetworkError),
    retry_kwargs={'max_retries': 3, 'countdown': 10},
    time_limit=300,  # 5ë¶„ íƒ€ì„ì•„ì›ƒ
    acks_late=True,
)
def image_upload_task(self, user_id, image_path, analysis_id):
    """
    ì´ë¯¸ì§€ S3 ì—…ë¡œë“œ
    
    Args:
        user_id: ì‚¬ìš©ì ID
        image_path: ë¡œì»¬ ì´ë¯¸ì§€ ê²½ë¡œ
        analysis_id: ë¶„ì„ ID
    
    Returns:
        dict: {"s3_path": "s3://...", "thumbnail_path": "s3://..."}
    """
    task_id = self.request.id
    
    # 1. ë¡œì»¬ WALì— Task ê¸°ë¡
    local_wal.save_task(
        task_id=task_id,
        task_name='image_upload',
        payload={'user_id': user_id, 'analysis_id': analysis_id},
        status='running'
    )
    
    try:
        # 2. ì´ë¯¸ì§€ ì „ì²˜ë¦¬
        image = Image.open(image_path)
        
        # ë¦¬ì‚¬ì´ì¦ˆ (ìµœëŒ€ 2048x2048)
        max_size = (2048, 2048)
        image.thumbnail(max_size, Image.Resampling.LANCZOS)
        
        # 3. S3 ì—…ë¡œë“œ (ì›ë³¸)
        s3_key = f"uploads/{user_id}/{analysis_id}/original.jpg"
        s3_client.upload_file(
            image_path,
            bucket='prod-growbin-images',
            key=s3_key,
            ExtraArgs={
                'ContentType': 'image/jpeg',
                'CacheControl': 'max-age=31536000',  # 1ë…„
            }
        )
        s3_path = f"s3://prod-growbin-images/{s3_key}"
        
        # 4. ì¸ë„¤ì¼ ìƒì„± ë° ì—…ë¡œë“œ
        thumbnail = image.copy()
        thumbnail.thumbnail((300, 300), Image.Resampling.LANCZOS)
        
        thumbnail_key = f"uploads/{user_id}/{analysis_id}/thumbnail.jpg"
        # ... (ì¸ë„¤ì¼ ì—…ë¡œë“œ)
        
        # 5. ê²°ê³¼ ë°˜í™˜
        result = {
            "s3_path": s3_path,
            "thumbnail_path": f"s3://prod-growbin-images/{thumbnail_key}"
        }
        
        # 6. ë‹¤ìŒ Task ë°œí–‰ (Vision ë¶„ì„)
        gpt5_analysis_task.apply_async(
            args=[analysis_id, s3_path],
            queue='vision_analysis'
        )
        
        return result
    
    except Exception as e:
        logger.error(f"Image upload failed: {e}")
        raise

# Celery Worker ì„¤ì •
# $ celery -A celery_app worker \
#     -Q user_input \
#     --pool=eventlet \
#     --concurrency=1000 \
#     --hostname=image-uploader@%h
```

---

### 3.2 gpt5-analyzer (Worker-AI)

**ì—­í• **: GPT-5 Vision API í˜¸ì¶œ ë° ì´ë¯¸ì§€ ë¶„ì„

```python
# workers/ai/tasks/gpt5_analyzer.py
from celery import Task
from celery_app import app
import openai
import hashlib

class GPT5AnalyzerTask(Task):
    """GPT-5 ë¶„ì„ Task Base"""
    
    def on_success(self, retval, task_id, args, kwargs):
        # WAL ì—…ë°ì´íŠ¸ + ìºì‹±
        local_wal.update_status(task_id, 'completed')
        
        # GPT-5 ì‘ë‹µ ìºì‹±
        cache_key = hashlib.sha256(str(args).encode()).hexdigest()
        local_wal.save_cache(cache_key, retval, ttl=3600)  # 1ì‹œê°„

@app.task(
    base=GPT5AnalyzerTask,
    bind=True,
    autoretry_for=(openai.error.RateLimitError, openai.error.APIError),
    retry_kwargs={'max_retries': 5, 'countdown': 30},
    retry_backoff=True,  # Exponential Backoff
    retry_jitter=True,   # Jitter ì¶”ê°€
    time_limit=60,       # 1ë¶„ íƒ€ì„ì•„ì›ƒ
    soft_time_limit=50,  # 50ì´ˆ Soft Timeout
    rate_limit='100/m',  # ë¶„ë‹¹ 100ê°œ (GPT-5 Rate Limit)
    acks_late=True,
)
def gpt5_analysis_task(self, analysis_id, s3_path):
    """
    GPT-5 Visionìœ¼ë¡œ ì´ë¯¸ì§€ ë¶„ì„
    
    Args:
        analysis_id: ë¶„ì„ ID
        s3_path: S3 ì´ë¯¸ì§€ ê²½ë¡œ
    
    Returns:
        dict: {
            "category": "plastic",
            "item": "pet_bottle",
            "state": "clean",
            "confidence": 0.95
        }
    """
    task_id = self.request.id
    
    # 1. ë¡œì»¬ WALì— Task ê¸°ë¡
    local_wal.save_task(
        task_id=task_id,
        task_name='gpt5_analysis',
        payload={'analysis_id': analysis_id},
        status='running'
    )
    
    # 2. ìºì‹œ í™•ì¸
    cache_key = hashlib.sha256(s3_path.encode()).hexdigest()
    cached = local_wal.get_cache(cache_key)
    if cached:
        logger.info(f"Cache hit for {s3_path}")
        return cached
    
    try:
        # 3. Rate Limit ì²´í¬
        rate_limit_key = f"rate_limit:gpt5:{self.request.id}"
        if redis.exists(rate_limit_key):
            raise self.retry(countdown=60)
        
        redis.setex(rate_limit_key, 60, 1)
        
        # 4. GPT-5 Vision API í˜¸ì¶œ
        response = openai.ChatCompletion.create(
            model="gpt-5-vision-preview",
            messages=[
                {
                    "role": "system",
                    "content": """ë‹¹ì‹ ì€ íê¸°ë¬¼ ë¶„ë¥˜ ì „ë¬¸ê°€ì…ë‹ˆë‹¤.
                    ì´ë¯¸ì§€ë¥¼ ë¶„ì„í•˜ì—¬ ë‹¤ìŒ ì •ë³´ë¥¼ JSON í˜•ì‹ìœ¼ë¡œ ë°˜í™˜í•˜ì„¸ìš”:
                    - category: íê¸°ë¬¼ ì¢…ë¥˜ (plastic, paper, metal, glass, general)
                    - item: êµ¬ì²´ì  í’ˆëª© (pet_bottle, cardboard, aluminum_can ë“±)
                    - state: ìƒíƒœ (clean, dirty, mixed)
                    - confidence: ì‹ ë¢°ë„ (0.0-1.0)
                    """
                },
                {
                    "role": "user",
                    "content": [
                        {"type": "text", "text": "ì´ íê¸°ë¬¼ì„ ë¶„ì„í•´ì£¼ì„¸ìš”."},
                        {"type": "image_url", "image_url": {"url": s3_path}}
                    ]
                }
            ],
            max_tokens=500,
            temperature=0.2,  # ì¼ê´€ì„± ìˆëŠ” ì‘ë‹µ
        )
        
        # 5. ì‘ë‹µ íŒŒì‹±
        result = json.loads(response.choices[0].message.content)
        
        # 6. ë¡œì»¬ WAL ìºì‹œì— ì €ì¥
        local_wal.save_cache(cache_key, result, ttl=3600)
        
        # 7. ë‹¤ìŒ Task ë°œí–‰ (ê·œì¹™ ì¡°íšŒ)
        rule_retrieval_task.apply_async(
            args=[analysis_id, result['item']],
            queue='rule_retrieval'
        )
        
        return result
    
    except openai.error.RateLimitError as e:
        # Rate Limit ì´ˆê³¼ â†’ ì¬ì‹œë„
        logger.warning(f"GPT-5 Rate Limit: {e}")
        raise self.retry(exc=e, countdown=60)
    
    except Exception as e:
        logger.error(f"GPT-5 analysis failed: {e}")
        raise

# Celery Worker ì„¤ì •
# $ celery -A celery_app worker \
#     -Q vision_analysis \
#     --pool=prefork \
#     --concurrency=3 \
#     --hostname=gpt5-analyzer@%h
```

---

### 3.3 rule-retriever (Worker-Storage)

**ì—­í• **: JSON ê·œì¹™ íŒŒì¼ ì¡°íšŒ

```python
# workers/storage/tasks/rule_retriever.py
from celery import Task
from celery_app import app
import json
from pathlib import Path

@app.task(
    bind=True,
    time_limit=10,  # 10ì´ˆ íƒ€ì„ì•„ì›ƒ (ë¡œì»¬ íŒŒì¼ ì¡°íšŒ)
    acks_late=True,
)
def rule_retrieval_task(self, analysis_id, item):
    """
    JSON ê·œì¹™ ì¡°íšŒ
    
    Args:
        analysis_id: ë¶„ì„ ID
        item: í’ˆëª© (pet_bottle, cardboard ë“±)
    
    Returns:
        dict: {"rules": [...], "disposal_method": "..."}
    """
    task_id = self.request.id
    
    # 1. ë¡œì»¬ WALì— Task ê¸°ë¡
    local_wal.save_task(
        task_id=task_id,
        task_name='rule_retrieval',
        payload={'analysis_id': analysis_id, 'item': item},
        status='running'
    )
    
    try:
        # 2. JSON íŒŒì¼ ì¡°íšŒ (ë¡œì»¬)
        rule_path = Path(f"/app/rules/{item}.json")
        
        if not rule_path.exists():
            # Fallback to general rule
            rule_path = Path("/app/rules/general.json")
        
        with open(rule_path, 'r', encoding='utf-8') as f:
            rules = json.load(f)
        
        # 3. ë‹¤ìŒ Task ë°œí–‰ (ì‘ë‹µ ìƒì„±)
        response_generation_task.apply_async(
            args=[analysis_id, rules],
            queue='response_generation'
        )
        
        return rules
    
    except Exception as e:
        logger.error(f"Rule retrieval failed: {e}")
        raise

# Celery Worker ì„¤ì •
# $ celery -A celery_app worker \
#     -Q rule_retrieval \
#     --pool=prefork \
#     --concurrency=2 \
#     --hostname=rule-retriever@%h
```

---

### 3.4 response-generator (Worker-AI)

**ì—­í• **: GPT-4o minië¡œ ìµœì¢… ì‘ë‹µ ìƒì„±

```python
# workers/ai/tasks/response_generator.py
from celery import Task
from celery_app import app
import openai

@app.task(
    bind=True,
    autoretry_for=(openai.error.RateLimitError,),
    retry_kwargs={'max_retries': 3, 'countdown': 20},
    time_limit=30,  # 30ì´ˆ íƒ€ì„ì•„ì›ƒ
    rate_limit='200/m',  # ë¶„ë‹¹ 200ê°œ (GPT-4o mini)
    acks_late=True,
)
def response_generation_task(self, analysis_id, rules):
    """
    GPT-4o minië¡œ ìµœì¢… ì‘ë‹µ ìƒì„±
    
    Args:
        analysis_id: ë¶„ì„ ID
        rules: ë¶„ë¦¬ë°°ì¶œ ê·œì¹™
    
    Returns:
        str: ì‚¬ìš©ìì—ê²Œ ë³´ì—¬ì¤„ ìµœì¢… ì‘ë‹µ
    """
    task_id = self.request.id
    
    # 1. ë¡œì»¬ WALì— Task ê¸°ë¡
    local_wal.save_task(
        task_id=task_id,
        task_name='response_generation',
        payload={'analysis_id': analysis_id},
        status='running'
    )
    
    try:
        # 2. GPT-4o mini API í˜¸ì¶œ
        response = openai.ChatCompletion.create(
            model="gpt-4o-mini",
            messages=[
                {
                    "role": "system",
                    "content": """ë‹¹ì‹ ì€ ì¹œì ˆí•œ ë¶„ë¦¬ë°°ì¶œ ì•ˆë‚´ ë„ìš°ë¯¸ì…ë‹ˆë‹¤.
                    ì‚¬ìš©ìê°€ ì´í•´í•˜ê¸° ì‰½ê²Œ ì„¤ëª…í•´ì£¼ì„¸ìš”."""
                },
                {
                    "role": "user",
                    "content": f"""ë‹¤ìŒ ê·œì¹™ì„ ë°”íƒ•ìœ¼ë¡œ ë¶„ë¦¬ë°°ì¶œ ë°©ë²•ì„ ì•ˆë‚´í•´ì£¼ì„¸ìš”:
                    {json.dumps(rules, ensure_ascii=False)}
                    """
                }
            ],
            max_tokens=300,
            temperature=0.7,
        )
        
        final_response = response.choices[0].message.content
        
        # 3. PostgreSQLì— ìµœì¢… ê²°ê³¼ ì €ì¥
        with postgres_session('growbin_waste') as db:
            analysis = db.query(WasteAnalysis).get(analysis_id)
            analysis.status = 'completed'
            analysis.response = final_response
            analysis.completed_at = datetime.now()
            db.commit()
        
        # 4. ë¡œì»¬ WAL ì—…ë°ì´íŠ¸
        local_wal.update_status(task_id, 'completed')
        
        return final_response
    
    except Exception as e:
        logger.error(f"Response generation failed: {e}")
        raise

# Celery Worker ì„¤ì •
# $ celery -A celery_app worker \
#     -Q response_generation \
#     --pool=prefork \
#     --concurrency=3 \
#     --hostname=response-generator@%h
```

---

### 3.5 task-scheduler (Celery Beat)

**ì—­í• **: ì£¼ê¸°ì  ì‘ì—… ìŠ¤ì¼€ì¤„ë§

```python
# workers/storage/tasks/scheduler.py
from celery_app import app
from celery.schedules import crontab

# Celery Beat ìŠ¤ì¼€ì¤„ ì •ì˜
app.conf.beat_schedule = {
    # 5ë¶„ë§ˆë‹¤: WAL â†’ PostgreSQL ë™ê¸°í™”
    'sync-wal-every-5-minutes': {
        'task': 'workers.storage.tasks.sync_wal_to_postgres',
        'schedule': 300.0,
    },
    
    # 1ì‹œê°„ë§ˆë‹¤: WAL ì²´í¬í¬ì¸íŠ¸
    'wal-checkpoint-hourly': {
        'task': 'workers.storage.tasks.wal_checkpoint',
        'schedule': 3600.0,
    },
    
    # ë§¤ì¼ ìì •: ë¡œê·¸ ì •ë¦¬
    'cleanup-logs-daily': {
        'task': 'workers.storage.tasks.cleanup_logs',
        'schedule': crontab(hour=0, minute=0),
    },
    
    # ë§¤ì£¼ ì›”ìš”ì¼ ì˜¤ì „ 9ì‹œ: ì£¼ê°„ ë¦¬í¬íŠ¸
    'weekly-report-monday-9am': {
        'task': 'workers.analytics.tasks.generate_weekly_report',
        'schedule': crontab(hour=9, minute=0, day_of_week=1),
    },
}

@app.task
def sync_wal_to_postgres():
    """WAL â†’ PostgreSQL ë™ê¸°í™”"""
    # ìƒì„¸ ì„¤ëª…ì€ WAL ì•„í‚¤í…ì²˜ ì°¸ê³ 
    pass

@app.task
def wal_checkpoint():
    """WAL ì²´í¬í¬ì¸íŠ¸ (WAL â†’ DB ë™ê¸°í™”)"""
    local_wal.conn.execute("PRAGMA wal_checkpoint(FULL)")

# Celery Beat ì‹œì‘
# $ celery -A celery_app beat --loglevel=info
```

---

## 4. WAL í†µí•© ì•„í‚¤í…ì²˜

> ìƒì„¸ ì„¤ëª…: [RabbitMQ + WAL ì•„í‚¤í…ì²˜](./RABBITMQ_WAL_ARCHITECTURE.md)

### 4.1 Worker ë¡œì»¬ WAL êµ¬ì¡°

```sql
-- /var/lib/growbin/worker-storage/task_queue.db
CREATE TABLE task_wal (
    task_id TEXT PRIMARY KEY,
    task_name TEXT NOT NULL,
    payload JSON,
    status TEXT DEFAULT 'pending',  -- pending, running, completed, failed
    rabbitmq_delivery_tag INTEGER,
    created_at INTEGER,
    started_at INTEGER,
    completed_at INTEGER,
    error TEXT,
    retry_count INTEGER DEFAULT 0
);

CREATE INDEX idx_task_status ON task_wal(status, created_at);

-- PostgreSQL ë™ê¸°í™” ì´ë ¥
CREATE TABLE sync_log (
    id INTEGER PRIMARY KEY AUTOINCREMENT,
    task_id TEXT NOT NULL,
    synced_at INTEGER,
    status TEXT,  -- success, failed
    error TEXT
);

-- íŒŒì¼ ìºì‹œ (Worker-Storage)
CREATE TABLE file_cache (
    key TEXT PRIMARY KEY,
    file_path TEXT,
    size INTEGER,
    created_at INTEGER,
    ttl INTEGER
);
```

```sql
-- /var/lib/growbin/worker-ai/task_queue.db
-- (Worker-Storageì™€ ë™ì¼í•œ task_wal í…Œì´ë¸”)

-- GPT API ì‘ë‹µ ìºì‹œ
CREATE TABLE gpt_cache (
    cache_key TEXT PRIMARY KEY,
    prompt TEXT,
    response JSON,
    model TEXT,  -- gpt-5-vision, gpt-4o-mini
    created_at INTEGER,
    ttl INTEGER DEFAULT 3600
);

CREATE INDEX idx_gpt_cache_ttl ON gpt_cache(created_at + ttl);

-- ì¬ì‹œë„ í
CREATE TABLE retry_queue (
    task_id TEXT PRIMARY KEY,
    retry_count INTEGER,
    next_retry_at INTEGER,
    error TEXT,
    payload JSON
);

-- Rate Limit ë¡œê·¸
CREATE TABLE rate_limit_log (
    timestamp INTEGER,
    api TEXT,  -- gpt5, gpt4o
    count INTEGER
);
```

### 4.2 WAL ì´ˆê¸°í™” (Worker ì‹œì‘ ì‹œ)

```python
# workers/common/wal.py
import sqlite3
import time

class LocalWALQueue:
    def __init__(self, db_path, worker_type='storage'):
        self.db_path = db_path
        self.worker_type = worker_type
        self.conn = None
        self._init_connection()
    
    def _init_connection(self):
        """WAL ëª¨ë“œë¡œ ì—°ê²°"""
        self.conn = sqlite3.connect(
            self.db_path,
            isolation_level=None,  # Autocommit
            check_same_thread=False,
            timeout=30.0
        )
        
        # WAL ëª¨ë“œ í™œì„±í™” (Robin ë°©ì‹)
        self.conn.execute("PRAGMA journal_mode=WAL")
        self.conn.execute("PRAGMA synchronous=NORMAL")  # ì„±ëŠ¥ ìµœì í™”
        self.conn.execute("PRAGMA wal_autocheckpoint=1000")  # 1000 í˜ì´ì§€ë§ˆë‹¤
        
        # í…Œì´ë¸” ìƒì„±
        self._create_tables()
    
    def _create_tables(self):
        """í…Œì´ë¸” ìƒì„±"""
        # task_wal (ê³µí†µ)
        self.conn.execute("""
            CREATE TABLE IF NOT EXISTS task_wal (
                task_id TEXT PRIMARY KEY,
                task_name TEXT NOT NULL,
                payload JSON,
                status TEXT DEFAULT 'pending',
                created_at INTEGER,
                started_at INTEGER,
                completed_at INTEGER,
                error TEXT,
                retry_count INTEGER DEFAULT 0
            )
        """)
        
        if self.worker_type == 'ai':
            # GPT ìºì‹œ
            self.conn.execute("""
                CREATE TABLE IF NOT EXISTS gpt_cache (
                    cache_key TEXT PRIMARY KEY,
                    response JSON,
                    model TEXT,
                    created_at INTEGER,
                    ttl INTEGER DEFAULT 3600
                )
            """)
    
    def save_task(self, task_id, task_name, payload, status='pending'):
        """Task WALì— ì €ì¥"""
        self.conn.execute("""
            INSERT INTO task_wal (task_id, task_name, payload, status, created_at)
            VALUES (?, ?, ?, ?, ?)
        """, (task_id, task_name, json.dumps(payload), status, int(time.time())))
    
    def update_status(self, task_id, status, error=None):
        """Task ìƒíƒœ ì—…ë°ì´íŠ¸"""
        if status == 'completed':
            self.conn.execute("""
                UPDATE task_wal
                SET status = ?, completed_at = ?
                WHERE task_id = ?
            """, (status, int(time.time()), task_id))
        else:
            self.conn.execute("""
                UPDATE task_wal
                SET status = ?, error = ?
                WHERE task_id = ?
            """, (status, error, task_id))

# Worker ì „ì—­ ë³€ìˆ˜
local_wal = LocalWALQueue(
    db_path="/var/lib/growbin/task_queue.db",
    worker_type=os.getenv('WORKER_TYPE', 'storage')
)
```

---

## 5. Worker ë°°í¬ ì „ëµ

### 5.1 Kubernetes Deployment

```yaml
# k8s/workers/worker-storage-deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: worker-storage
  namespace: workers
spec:
  replicas: 2
  selector:
    matchLabels:
      app: worker-storage
  template:
    metadata:
      labels:
        app: worker-storage
    spec:
      nodeSelector:
        node-role: worker-storage
      
      containers:
      - name: celery-worker
        image: ghcr.io/sesacthon/worker-storage:latest
        command: ["/bin/sh", "-c"]
        args:
          - |
            celery -A celery_app worker \
              -Q user_input,rule_retrieval \
              --pool=eventlet \
              --concurrency=1000 \
              --hostname=image-uploader@%h \
              --loglevel=info
        
        env:
        - name: CELERY_BROKER_URL
          value: "amqp://rabbitmq:5672/growbin"
        - name: DATABASE_URL
          value: "postgresql://postgres:5432/growbin_waste"
        - name: WORKER_TYPE
          value: "storage"
        
        resources:
          requests:
            cpu: "1"
            memory: "2Gi"
          limits:
            cpu: "2"
            memory: "4Gi"
        
        volumeMounts:
        - name: wal-storage
          mountPath: /var/lib/growbin
        
        livenessProbe:
          exec:
            command:
              - celery
              - -A
              - celery_app
              - inspect
              - ping
          initialDelaySeconds: 30
          periodSeconds: 30
        
        readinessProbe:
          exec:
            command:
              - celery
              - -A
              - celery_app
              - inspect
              - active
          initialDelaySeconds: 10
          periodSeconds: 10
      
      volumes:
      - name: wal-storage
        persistentVolumeClaim:
          claimName: worker-storage-wal-pvc

---
apiVersion: v1
kind: Service
metadata:
  name: worker-storage
  namespace: workers
spec:
  selector:
    app: worker-storage
  ports:
  - name: metrics
    port: 9090
    targetPort: 9090
```

### 5.2 Horizontal Pod Autoscaler

```yaml
# k8s/workers/worker-storage-hpa.yaml
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: worker-storage-hpa
  namespace: workers
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: worker-storage
  minReplicas: 2
  maxReplicas: 10
  metrics:
  - type: Resource
    resource:
      name: cpu
      target:
        type: Utilization
        averageUtilization: 70
  - type: Pods
    pods:
      metric:
        name: celery_queue_length
      target:
        type: AverageValue
        averageValue: "100"
  
  behavior:
    scaleUp:
      stabilizationWindowSeconds: 60
      policies:
      - type: Percent
        value: 50
        periodSeconds: 60
    scaleDown:
      stabilizationWindowSeconds: 300
      policies:
      - type: Pods
        value: 1
        periodSeconds: 60
```

---

## 6. ëª¨ë‹ˆí„°ë§ ë° ì¥ì•  ì²˜ë¦¬

> ìƒì„¸ ì„¤ëª…: [í†µí•© ì•„í‚¤í…ì²˜ ì¥ì•  ì§€ì ](./COMBINED_ARCHITECTURE_WAL_DOMAIN.md#3-ì¥ì• -ì§€ì -ë¶„ì„-ë°-ëŒ€ì‘)

### 6.1 Prometheus ë©”íŠ¸ë¦­

```python
# workers/common/metrics.py
from prometheus_client import Counter, Gauge, Histogram

# Task ë©”íŠ¸ë¦­
celery_tasks_total = Counter(
    'celery_tasks_total',
    'Total Celery tasks',
    ['worker', 'task_name', 'status']
)

celery_task_duration = Histogram(
    'celery_task_duration_seconds',
    'Task execution time',
    ['worker', 'task_name'],
    buckets=[0.1, 0.5, 1, 5, 10, 30, 60, 120, 300]
)

# WAL ë©”íŠ¸ë¦­
wal_pending_tasks = Gauge(
    'wal_pending_tasks',
    'Current pending tasks in WAL',
    ['worker']
)

wal_sync_delay = Histogram(
    'wal_sync_delay_seconds',
    'WAL â†’ PostgreSQL sync delay',
    ['worker']
)

# GPT API ë©”íŠ¸ë¦­
gpt_api_calls_total = Counter(
    'gpt_api_calls_total',
    'Total GPT API calls',
    ['model', 'status']  # model: gpt5, gpt4o
)

gpt_cache_hits_total = Counter(
    'gpt_cache_hits_total',
    'GPT cache hits',
    ['model']
)
```

---

## 7. ì„±ëŠ¥ ìµœì í™”

### 7.1 Task ì²´ì´ë‹ ìµœì í™”

```python
# âŒ Bad: ìˆœì°¨ ì²´ì´ë‹ (ëŠë¦¼)
result1 = task1.delay(args)
result2 = task2.delay(result1.get())  # Blocking!
result3 = task3.delay(result2.get())  # Blocking!

# âœ… Good: Celery Chain (ë¹ ë¦„)
from celery import chain

result = chain(
    task1.s(args),
    task2.s(),
    task3.s()
).apply_async()
```

### 7.2 Prefetch ìµœì í™”

```python
# Worker-Storage (I/O Bound)
# - Eventlet Pool
# - ë†’ì€ Prefetch (ë¹ ë¥¸ Task ì²˜ë¦¬)
app.conf.worker_prefetch_multiplier = 10

# Worker-AI (Network Bound)
# - Prefork Pool
# - ë‚®ì€ Prefetch (ê¸´ Task ì²˜ë¦¬)
app.conf.worker_prefetch_multiplier = 1
```

---

## 8. Best Practices

### 8.1 Task ì„¤ê³„

- âœ… **ì‘ê³  ë‹¨ì¼ ì±…ì„**: í•œ TaskëŠ” í•œ ê°€ì§€ ì¼ë§Œ
- âœ… **ë©±ë“±ì„±**: ì—¬ëŸ¬ ë²ˆ ì‹¤í–‰í•´ë„ ê°™ì€ ê²°ê³¼
- âœ… **íƒ€ì„ì•„ì›ƒ ì„¤ì •**: `time_limit`, `soft_time_limit`
- âœ… **ì¬ì‹œë„ ì „ëµ**: `autoretry_for`, Exponential Backoff

### 8.2 ì—ëŸ¬ ì²˜ë¦¬

- âœ… **ëª…ì‹œì  ì˜ˆì™¸ ì²˜ë¦¬**: ì¬ì‹œë„ ê°€ëŠ¥ vs ë¶ˆê°€ëŠ¥ êµ¬ë¶„
- âœ… **DLQ ì„¤ì •**: ìµœëŒ€ ì¬ì‹œë„ ì´ˆê³¼ ì‹œ DLQë¡œ
- âœ… **ë¡œê¹…**: ì—ëŸ¬ ë°œìƒ ì‹œ ìƒì„¸ ë¡œê·¸

### 8.3 WAL ê´€ë¦¬

- âœ… **ì£¼ê¸°ì  ì²´í¬í¬ì¸íŠ¸**: 1ì‹œê°„ë§ˆë‹¤ `PRAGMA wal_checkpoint`
- âœ… **TTL ì„¤ì •**: ìºì‹œ ë°ì´í„°ëŠ” TTL ì„¤ì •
- âœ… **ë””ìŠ¤í¬ ëª¨ë‹ˆí„°ë§**: WAL íŒŒì¼ í¬ê¸° ëª¨ë‹ˆí„°ë§

---

## ğŸ“š ì°¸ê³  ë¬¸ì„œ

- [Celery ë™ì‘ ë°©ì‹](./CELERY_ARCHITECTURE.md) â­
- [RabbitMQ + WAL ì•„í‚¤í…ì²˜](./RABBITMQ_WAL_ARCHITECTURE.md)
- [í†µí•© ì•„í‚¤í…ì²˜ (WAL + ë„ë©”ì¸ ë¶„ë¦¬)](./COMBINED_ARCHITECTURE_WAL_DOMAIN.md)
- [DB ì•„í‚¤í…ì²˜ ë¶„ì„](./DB_ARCHITECTURE_ANALYSIS.md)

---

**ì‘ì„± ì¼ì‹œ**: 2025-11-06
**ì‹œìŠ¤í…œ**: Growbin Backend (13-Node Cluster)

