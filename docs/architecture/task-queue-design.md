# ğŸ° RabbitMQ + Celery Task Queue ì„¤ê³„

> **ëª©í‘œ**: í•œ í í­ì£¼ ë°©ì§€ + SLO ë¶„ë¦¬ + ì¥ì•  ê²©ë¦¬  
> **ê¸°ë°˜**: RabbitMQ Topic Exchange + Celery Best Practices  
> **ë‚ ì§œ**: 2025-10-30

## ğŸ“‹ ëª©ì°¨

1. [ì„¤ê³„ ì›ì¹™](#ì„¤ê³„-ì›ì¹™)
2. [í ì„¤ê³„ (5ê°œ)](#í-ì„¤ê³„-5ê°œ)
3. [ì‹¤ì œ Task ë§¤í•‘](#ì‹¤ì œ-task-ë§¤í•‘)
4. [Celery ì„¤ì •](#celery-ì„¤ì •)
5. [K8s Worker ë°°ì¹˜](#k8s-worker-ë°°ì¹˜)
6. [ìš´ì˜ ê°€ì´ë“œ](#ìš´ì˜-ê°€ì´ë“œ)

---

## ğŸ¯ ì„¤ê³„ ì›ì¹™

### 3ëŒ€ ëª©í‘œ

```mermaid
graph TB
    A[RabbitMQ + Celery] --> B{ì„¤ê³„ ëª©í‘œ}
    
    B --> C1[ğŸ›¡ï¸ í•œ í í­ì£¼ ë°©ì§€<br/>TTL + max-length + DLX]
    B --> C2[âš¡ SLO ë¶„ë¦¬<br/>ì§§ì€ ì‘ì—… vs ê¸´ ì‘ì—…]
    B --> C3[ğŸ”’ ì¥ì•  ê²©ë¦¬<br/>ì™¸ë¶€ API ì¥ì•  ì‹œ ë‹¤ë¥¸ í ì •ìƒ]
    
    C1 --> D[ì•ˆì •ì ì¸<br/>ì„œë¹„ìŠ¤]
    C2 --> D
    C3 --> D
    
    style A fill:#ffe0b3,stroke:#fd7e14,stroke-width:4px,color:#000
    style C1 fill:#ffd1d1,stroke:#dc3545,stroke-width:3px,color:#000
    style C2 fill:#cce5ff,stroke:#007bff,stroke-width:3px,color:#000
    style C3 fill:#d1f2eb,stroke:#28a745,stroke-width:3px,color:#000
    style D fill:#e6d5ff,stroke:#8844ff,stroke-width:4px,color:#000
```

### í•µì‹¬ ì „ëµ

```
1. ì§§ì€ ì‘ì—…ê³¼ ê¸´ ì‘ì—… ë¶„ë¦¬
   âœ… ì§§ì€ ì‘ì—…(< 1ì´ˆ): q.fast (prefetch ë†’ê²Œ)
   âœ… ê¸´ ì‘ì—…(5ì´ˆ+): q.bulk (prefetch=1, ê³µí‰ì„±â†‘)

2. ì™¸ë¶€ API ê²©ë¦¬
   âœ… ë¶ˆì•ˆì •í•œ ì™¸ë¶€ API: q.external (ì¬ì‹œë„ ì—„ê²©, DLX í•„ìˆ˜)
   âœ… ì¥ì•  ì‹œ ë‹¤ë¥¸ í ì˜í–¥ ì—†ìŒ

3. ì˜ˆì•½ ì‘ì—… ë¶„ë¦¬
   âœ… Celery Beat ì „ìš© í: q.sched
   âœ… ëŒ€ê·œëª¨ ì˜ˆì•½ íŠ¸ë˜í”½ ë¸Œë¡œì»¤ ë³´í˜¸

4. DLX(Dead Letter Exchange) ê³µí†µ
   âœ… ëª¨ë“  í â†’ q.dlqë¡œ ì‹¤íŒ¨ ë©”ì‹œì§€ ê²©ë¦¬
   âœ… ìˆ˜ë™ ì¬ì²˜ë¦¬ ë˜ëŠ” ë¶„ì„ìš©
```

---

## ğŸ“¦ í ì„¤ê³„ (5ê°œ)

### ì „ì²´ êµ¬ì¡°

```mermaid
graph TB
    subgraph Producer["FastAPI Services"]
        API[waste-service<br/>recycling-service<br/>etc.]
    end
    
    subgraph RabbitMQ["RabbitMQ Broker"]
        Exchange[Topic Exchange<br/>'tasks']
        DLX[Direct Exchange<br/>'dlx']
        
        Q1[q.fast<br/>Priority: ë†’ìŒ<br/>ì§§ì€ ì‘ì—…]
        Q2[q.bulk<br/>Priority: ë³´í†µ<br/>ê¸´ ì‘ì—…]
        Q3[q.external<br/>Priority: ë†’ìŒ<br/>ì™¸ë¶€ API]
        Q4[q.sched<br/>Priority: ë³´í†µ<br/>ì˜ˆì•½ ì‘ì—…]
        Q5[q.dlq<br/>Dead Letter<br/>ì‹¤íŒ¨ ë©”ì‹œì§€]
    end
    
    subgraph Workers["Celery Workers"]
        W1[Fast Workers Ã—5<br/>concurrency: 10<br/>prefetch: ë†’ìŒ]
        W2[Bulk Workers Ã—2<br/>concurrency: 4<br/>prefetch: 1]
        W3[External Workers Ã—3<br/>concurrency: 4<br/>prefetch: 2]
        W4[Sched Workers Ã—1<br/>concurrency: 4]
    end
    
    API --> Exchange
    Exchange -->|*.high.*| Q1
    Exchange -->|*.low.*| Q2
    Exchange -->|external.#| Q3
    Exchange -->|sched.#| Q4
    
    Q1 -.->|ì‹¤íŒ¨/TTL| DLX
    Q2 -.->|ì‹¤íŒ¨/TTL| DLX
    Q3 -.->|ì‹¤íŒ¨/TTL| DLX
    Q4 -.->|ì‹¤íŒ¨/TTL| DLX
    DLX --> Q5
    
    Q1 --> W1
    Q2 --> W2
    Q3 --> W3
    Q4 --> W4
    
    style Exchange fill:#ffe0b3,stroke:#fd7e14,stroke-width:4px,color:#000
    style DLX fill:#ffd1d1,stroke:#dc3545,stroke-width:3px,color:#000
    style Q1 fill:#cce5ff,stroke:#007bff,stroke-width:3px,color:#000
    style Q2 fill:#ffe0b3,stroke:#fd7e14,stroke-width:2px,color:#000
    style Q3 fill:#ffd1d1,stroke:#dc3545,stroke-width:3px,color:#000
    style Q4 fill:#d1f2eb,stroke:#28a745,stroke-width:2px,color:#000
    style Q5 fill:#ffb3b3,stroke:#dc3545,stroke-width:4px,color:#000
```

---

## ğŸ“‹ íë³„ ìƒì„¸ ì„¤ê³„

### Queue 1: **q.fast** (ì‚¬ìš©ì ì‘ë‹µ ì§ê²°)

```yaml
í ì´ë¦„: q.fast
ë¼ìš°íŒ… í‚¤: *.high.*
ëª©ì : ì‚¬ìš©ìê°€ ëŒ€ê¸° ì¤‘ì¸ ì§§ì€ ì‘ì—…

ë‹´ë‹¹ ì‘ì—…:
â”œâ”€ ì´ë¯¸ì§€ ë‹¤ìš´ë¡œë“œ (S3)
â”œâ”€ ì´ë¯¸ì§€ í•´ì‹œ ê³„ì‚° (pHash)
â”œâ”€ ìºì‹œ ì¡°íšŒ (Redis)
â”œâ”€ ê²°ê³¼ ì €ì¥ (DB)
â””â”€ ì´ë¯¸ì§€ ì „ì²˜ë¦¬ (ë¦¬ì‚¬ì´ì¦ˆ)

ì‘ì—… íŠ¹ì„±:
â”œâ”€ ì‹¤í–‰ ì‹œê°„: < 1ì´ˆ
â”œâ”€ ì¤‘ìš”ë„: Critical (ì‚¬ìš©ì ëŒ€ê¸°)
â”œâ”€ CPU: ë†’ìŒ (ì´ë¯¸ì§€ ì²˜ë¦¬)
â””â”€ ì‹¤íŒ¨ ì˜í–¥: ì „ì²´ í”„ë¡œì„¸ìŠ¤ ì¤‘ë‹¨

Worker í”„ë¡œíŒŒì¼:
â”œâ”€ Concurrency: 10 (ë©€í‹°í”„ë¡œì„¸ì‹±)
â”œâ”€ Prefetch Multiplier: 4 (ë¹ ë¥¸ ì²˜ë¦¬)
â”œâ”€ Pool: processes
â””â”€ Replicas: 5ê°œ (K8s)

ì •ì±… (RabbitMQ):
â”œâ”€ TTL: 60ì´ˆ (ì§§ê²Œ, ë¹ ë¥¸ ì‹¤íŒ¨)
â”œâ”€ max-length: 5,000 (í­ì£¼ ë°©ì§€)
â”œâ”€ DLX: dlx Exchangeë¡œ ì´ë™
â”œâ”€ Priority: ì§€ì› (0-10)
â””â”€ Overflow: reject-publish (ê¸¸ì´ ì´ˆê³¼ ì‹œ ê±°ë¶€)

ì¬ì‹œë„/íƒ€ì„ì•„ì›ƒ:
â”œâ”€ Task Time Limit: 60ì´ˆ
â”œâ”€ Soft Time Limit: 50ì´ˆ
â”œâ”€ Max Retries: 3íšŒ
â”œâ”€ Retry Backoff: ì§€ìˆ˜ ë°±ì˜¤í”„ (1s, 2s, 4s)
â””â”€ acks_late: False (ë¹ ë¥¸ ACK)
```

### Queue 2: **q.bulk** (ë°°ì¹˜/ê¸´ ì‘ì—…)

```yaml
í ì´ë¦„: q.bulk
ë¼ìš°íŒ… í‚¤: *.low.*
ëª©ì : ì‹œê°„ì´ ê±¸ë¦¬ëŠ” ë°°ì¹˜ ì‘ì—…

ë‹´ë‹¹ ì‘ì—…:
â”œâ”€ ì¼ì¼ í†µê³„ ë¦¬í¬íŠ¸ ìƒì„±
â”œâ”€ CSV ë‚´ë³´ë‚´ê¸°
â”œâ”€ ëŒ€ëŸ‰ ë°ì´í„° ì²˜ë¦¬
â””â”€ ë¶„ì„ ì´ë ¥ ì§‘ê³„ (ë°°ì¹˜)

ì‘ì—… íŠ¹ì„±:
â”œâ”€ ì‹¤í–‰ ì‹œê°„: 10ì´ˆ ~ ìˆ˜ ë¶„
â”œâ”€ ì¤‘ìš”ë„: Medium (ë°°ì¹˜ ì²˜ë¦¬)
â”œâ”€ I/O: DB ì§‘ì•½ì 
â””â”€ ì‹¤íŒ¨ ì˜í–¥: ë‚®ìŒ

Worker í”„ë¡œíŒŒì¼:
â”œâ”€ Concurrency: 4
â”œâ”€ Prefetch Multiplier: 1 (ê³µí‰ì„±â†‘, í—¤ë“œì˜¤ë¸Œë¼ì¸ ë°©ì§€)
â”œâ”€ Pool: gevent (I/O ëŒ€ê¸°)
â””â”€ Replicas: 2ê°œ

ì •ì±… (RabbitMQ):
â”œâ”€ TTL: 3600ì´ˆ (1ì‹œê°„, ë„‰ë„‰íˆ)
â”œâ”€ max-length: 1,000
â”œâ”€ DLX: dlx
â””â”€ Priority: ë‚®ìŒ (1-3)

ì¬ì‹œë„/íƒ€ì„ì•„ì›ƒ:
â”œâ”€ Task Time Limit: 600ì´ˆ (10ë¶„)
â”œâ”€ Soft Time Limit: 540ì´ˆ
â”œâ”€ Max Retries: 2íšŒ
â”œâ”€ Retry Backoff: True
â””â”€ acks_late: True (ì²˜ë¦¬ ì™„ë£Œ í›„ ACK)

íŠ¹ì´ì‚¬í•­:
âš ï¸ prefetch=1ë¡œ ê¸´ ì‘ì—…ì´ ì§§ì€ ì‘ì—… êµ¶ê¸°ì§€ ì•Šë„ë¡!
âš ï¸ ì²´í¬í¬ì¸íŒ… ê¶Œì¥ (ì¤‘ê°„ ì €ì¥)
```

### Queue 3: **q.external** (ì™¸ë¶€ API/ë¶ˆì•ˆì •)

```yaml
í ì´ë¦„: q.external
ë¼ìš°íŒ… í‚¤: external.#
ëª©ì : ì™¸ë¶€ API í˜¸ì¶œ (AI Vision, LLM, Map)

ë‹´ë‹¹ ì‘ì—…:
â”œâ”€ AI Vision API (Roboflow, HuggingFace)
â”œâ”€ LLM API (OpenAI GPT, Claude)
â”œâ”€ ì§€ë„ API (Kakao Map)
â””â”€ ê¸°íƒ€ ì„œë“œíŒŒí‹° ì—°ë™

ì‘ì—… íŠ¹ì„±:
â”œâ”€ ì‹¤í–‰ ì‹œê°„: 2-10ì´ˆ (API ì‘ë‹µ ì‹œê°„)
â”œâ”€ ì¤‘ìš”ë„: Critical/High
â”œâ”€ ë„¤íŠ¸ì›Œí¬: ë§¤ìš° ë†’ìŒ
â”œâ”€ ë¶ˆì•ˆì •ì„±: ë†’ìŒ (ì™¸ë¶€ API ì¥ì•  ê°€ëŠ¥)
â””â”€ ì‚¬ì´ë“œ ì´í™íŠ¸: ì£¼ì˜ (ì¤‘ë³µ í˜¸ì¶œ ë¹„ìš©)

Worker í”„ë¡œíŒŒì¼:
â”œâ”€ Concurrency: 4 (API Rate Limit ì¤€ìˆ˜)
â”œâ”€ Prefetch Multiplier: 1-2 (ì†Œìˆ˜, ê³¼ë¶€í•˜ ë°©ì§€)
â”œâ”€ Pool: gevent (ë„¤íŠ¸ì›Œí¬ ëŒ€ê¸°)
â””â”€ Replicas: 3-5ê°œ (ì™¸ë¶€ APIë³„)

ì •ì±… (RabbitMQ):
â”œâ”€ TTL: 300ì´ˆ (5ë¶„, í•„ìˆ˜!)
â”œâ”€ max-length: 2,000
â”œâ”€ DLX: dlx (í•„ìˆ˜!)
â””â”€ Priority: ë†’ìŒ (7-10)

ì¬ì‹œë„/íƒ€ì„ì•„ì›ƒ:
â”œâ”€ Task Time Limit: 300ì´ˆ (5ë¶„)
â”œâ”€ Soft Time Limit: 240ì´ˆ
â”œâ”€ Max Retries: 3íšŒ (ì—„ê²©, ì‚¬ì´ë“œ ì´í™íŠ¸ ì£¼ì˜)
â”œâ”€ Retry Backoff: True (ì§€ìˆ˜ ë°±ì˜¤í”„)
â”œâ”€ acks_late: True (API ì„±ê³µ í›„ ACK)
â””â”€ ë©±ë“±ì„±: í•„ìˆ˜! (ì¤‘ë³µ í˜¸ì¶œ ëŒ€ë¹„)

íŠ¹ì´ì‚¬í•­:
ğŸ”´ ì™¸ë¶€ API ì¥ì•  ì‹œ ì´ íë§Œ ì˜í–¥
ğŸ”´ DLX í•„ìˆ˜ (íƒ€ì„ì•„ì›ƒ/ì‹¤íŒ¨ ë©”ì‹œì§€ ê²©ë¦¬)
ğŸ”´ Rate Limiting ì—„ê²©íˆ ì ìš©
```

### Queue 4: **q.sched** (ì˜ˆì•½/ì£¼ê¸° ì‘ì—…)

```yaml
í ì´ë¦„: q.sched
ë¼ìš°íŒ… í‚¤: sched.#
ëª©ì : Celery Beat ì˜ˆì•½ ì‘ì—…

ë‹´ë‹¹ ì‘ì—…:
â”œâ”€ ì¼ì¼ í†µê³„ ì§‘ê³„ (ë§¤ì¼ 02:00)
â”œâ”€ ìºì‹œ ì •ë¦¬ (ë§¤ì‹œê°„)
â”œâ”€ ì˜¤ë˜ëœ ì´ë¯¸ì§€ ì‚­ì œ (ë§¤ì¼ 03:00)
â””â”€ ì£¼ê°„ ë¦¬í¬íŠ¸ (ë§¤ì£¼ ì›”ìš”ì¼)

ì‘ì—… íŠ¹ì„±:
â”œâ”€ ì‹¤í–‰ ì‹œê°„: ë‹¤ì–‘ (1ì´ˆ ~ 10ë¶„)
â”œâ”€ ì¤‘ìš”ë„: Medium
â”œâ”€ ë¹ˆë„: ì£¼ê¸°ì  (cron)
â””â”€ ETA/countdown ì‚¬ìš©

Worker í”„ë¡œíŒŒì¼:
â”œâ”€ Concurrency: 4
â”œâ”€ Prefetch Multiplier: 1
â”œâ”€ Pool: gevent
â””â”€ Replicas: 1ê°œ (Beatë„ 1ê°œ!)

ì •ì±… (RabbitMQ):
â”œâ”€ TTL: 3600ì´ˆ
â”œâ”€ max-length: 500
â”œâ”€ DLX: dlx
â””â”€ Priority: ì¤‘ê°„ (5)

Celery Beat:
â”œâ”€ Replicas: 1ê°œ (ì¤‘ë³µ ì‹¤í–‰ ë°©ì§€)
â”œâ”€ Scheduler: DatabaseScheduler (ë¶„ì‚° í™˜ê²½)
â””â”€ Lock: Redis Lock (HA ì‹œ)

íŠ¹ì´ì‚¬í•­:
âš ï¸ BeatëŠ” ë°˜ë“œì‹œ 1ê°œë§Œ ì‹¤í–‰!
âš ï¸ ëŒ€ê·œëª¨ ì˜ˆì•½ì€ ë³„ë„ ì›Œí¬í”Œë¡œ ì—”ì§„ ê²€í† 
âš ï¸ ì˜ˆì•½ íŠ¸ë˜í”½ì´ ë¸Œë¡œì»¤ ë³‘ëª© ìœ ë°œ ê°€ëŠ¥
```

### Queue 5: **q.dlq** (Dead Letter Queue)

```yaml
í ì´ë¦„: q.dlq
ë¼ìš°íŒ… í‚¤: dlq (Direct)
ëª©ì : ì‹¤íŒ¨/ë§Œë£Œ ë©”ì‹œì§€ ìˆ˜ì§‘ ë° ë¶„ì„

ìœ ì… ê²½ë¡œ:
â”œâ”€ q.fast ì‹¤íŒ¨ (3íšŒ ì¬ì‹œë„ í›„)
â”œâ”€ q.external íƒ€ì„ì•„ì›ƒ (5ë¶„ ì´ˆê³¼)
â”œâ”€ q.bulk ì‹¤íŒ¨
â”œâ”€ q.sched ì‹¤íŒ¨
â””â”€ max-length ì´ˆê³¼ë¡œ rejectëœ ë©”ì‹œì§€

Worker í”„ë¡œíŒŒì¼:
â”œâ”€ ê¸°ë³¸: ì†Œë¹„ ì•ˆ í•¨ (ìˆ˜ë™ ì¬ì²˜ë¦¬)
â”œâ”€ ì„ íƒ: ì•ŒëŒ ì „ìš© Worker (1ê°œ)
â””â”€ ë¡œê·¸/ë¶„ì„ë§Œ ìˆ˜í–‰

ì •ì±…:
â”œâ”€ TTL: ì—†ìŒ (ì˜êµ¬ ë³´ê´€)
â”œâ”€ max-length: 10,000
â””â”€ Overflow: drop-head (ì˜¤ë˜ëœ ê²ƒë¶€í„° ì‚­ì œ)

ëª¨ë‹ˆí„°ë§:
âœ… DLQ ê¸¸ì´ ëª¨ë‹ˆí„°ë§ (Prometheus)
âœ… ê¸¸ì´ > 100 â†’ Slack ì•ŒëŒ
âœ… ì£¼ê¸°ì  ë¶„ì„ (ì‹¤íŒ¨ ì›ì¸ íŒŒì•…)
```

---

## ğŸ¯ ì‹¤ì œ Task ë§¤í•‘

### ìš°ë¦¬ í”„ë¡œì íŠ¸ Task ëª©ë¡

| Task í•¨ìˆ˜ëª… | ì˜ˆìƒ ì‹œê°„ | ì™¸ë¶€ ì˜ì¡´ì„± | SLO | í | ë¼ìš°íŒ… í‚¤ |
|------------|----------|------------|-----|-----|----------|
| `download_image` | 0.5ì´ˆ | S3 | 1ì´ˆ | q.fast | waste.high.download |
| `calculate_hash` | 0.3ì´ˆ | ì—†ìŒ | 1ì´ˆ | q.fast | waste.high.hash |
| `check_cache` | 0.1ì´ˆ | Redis | 1ì´ˆ | q.fast | waste.high.cache |
| `preprocess_image` | 0.8ì´ˆ | ì—†ìŒ | 2ì´ˆ | q.fast | waste.high.preprocess |
| `save_result` | 0.2ì´ˆ | DB | 1ì´ˆ | q.fast | waste.high.save |
| `ai_vision_classify` | 2-5ì´ˆ | Roboflow API | 10ì´ˆ | q.external | external.ai.vision |
| `llm_generate_feedback` | 3-8ì´ˆ | OpenAI API | 15ì´ˆ | q.external | external.llm.feedback |
| `search_nearby_bins` | 0.5-1ì´ˆ | Kakao Map API | 5ì´ˆ | q.external | external.map.location |
| `save_analytics` | 1-2ì´ˆ | DB | ì—†ìŒ | q.bulk | analytics.low.history |
| `daily_stats_report` | 30-60ì´ˆ | DB | ì—†ìŒ | q.sched | sched.daily.stats |
| `cleanup_old_images` | 10-30ì´ˆ | S3 | ì—†ìŒ | q.sched | sched.daily.cleanup |

### ë¼ìš°íŒ… í‚¤ íŒ¨í„´

```
íŒ¨í„´: {ë„ë©”ì¸}.{ìš°ì„ ìˆœìœ„}.{ì‘ì—…}

ë„ë©”ì¸:
â”œâ”€ waste: ì“°ë ˆê¸° ë¶„ì„ ê´€ë ¨
â”œâ”€ recycling: ì¬í™œìš© ì •ë³´
â”œâ”€ external: ì™¸ë¶€ API
â”œâ”€ analytics: í†µê³„
â””â”€ sched: ì˜ˆì•½ ì‘ì—…

ìš°ì„ ìˆœìœ„:
â”œâ”€ high: ì‚¬ìš©ì ëŒ€ê¸°, ì¦‰ì‹œ ì²˜ë¦¬
â”œâ”€ low: ë°°ì¹˜, ë°±ê·¸ë¼ìš´ë“œ
â””â”€ (external, schedëŠ” ìš°ì„ ìˆœìœ„ ì—†ìŒ)

ì‘ì—…:
â”œâ”€ download, hash, cache, preprocess, save
â”œâ”€ ai.vision, llm.feedback
â””â”€ stats, cleanup
```

---

## âš™ï¸ Celery ì„¤ì •

### ê³µí†µ ì„¤ì •

```python
# app/core/celery_config.py
from kombu import Exchange, Queue

# Exchange ì •ì˜
TASKS_EXCHANGE = Exchange("tasks", type="topic")
DLX_EXCHANGE = Exchange("dlx", type="direct")

# Queue ì •ì˜
task_queues = (
    # q.fast: ì§§ê³  ê¸´ê¸‰í•œ ì‘ì—…
    Queue(
        "q.fast",
        TASKS_EXCHANGE,
        routing_key="*.high.*",
        queue_arguments={
            "x-dead-letter-exchange": "dlx",
            "x-dead-letter-routing-key": "dlq",
            "x-message-ttl": 60_000,  # 60ì´ˆ
            "x-max-length": 5_000,
            "x-overflow": "reject-publish",
            "x-max-priority": 10,
        },
    ),
    
    # q.bulk: ê¸´ ë°°ì¹˜ ì‘ì—…
    Queue(
        "q.bulk",
        TASKS_EXCHANGE,
        routing_key="*.low.*",
        queue_arguments={
            "x-dead-letter-exchange": "dlx",
            "x-dead-letter-routing-key": "dlq",
            "x-message-ttl": 3_600_000,  # 1ì‹œê°„
            "x-max-length": 1_000,
            "x-overflow": "reject-publish",
        },
    ),
    
    # q.external: ì™¸ë¶€ API (í•„ìˆ˜ DLX)
    Queue(
        "q.external",
        TASKS_EXCHANGE,
        routing_key="external.#",
        queue_arguments={
            "x-dead-letter-exchange": "dlx",
            "x-dead-letter-routing-key": "dlq",
            "x-message-ttl": 300_000,  # 5ë¶„ (í•„ìˆ˜!)
            "x-max-length": 2_000,
            "x-overflow": "reject-publish",
            "x-max-priority": 10,
        },
    ),
    
    # q.sched: ì˜ˆì•½ ì‘ì—…
    Queue(
        "q.sched",
        TASKS_EXCHANGE,
        routing_key="sched.#",
        queue_arguments={
            "x-dead-letter-exchange": "dlx",
            "x-dead-letter-routing-key": "dlq",
            "x-message-ttl": 3_600_000,
            "x-max-length": 500,
        },
    ),
    
    # q.dlq: Dead Letter Queue
    Queue("q.dlq", DLX_EXCHANGE, routing_key="dlq"),
)

# Task ë¼ìš°íŒ…
task_routes = {
    # Fast ì‘ì—… (ì§§ê³  ê¸´ê¸‰)
    "tasks.image.download": {
        "queue": "q.fast",
        "routing_key": "waste.high.download",
        "priority": 10,
    },
    "tasks.image.hash": {
        "queue": "q.fast",
        "routing_key": "waste.high.hash",
        "priority": 10,
    },
    "tasks.cache.check": {
        "queue": "q.fast",
        "routing_key": "waste.high.cache",
        "priority": 10,
    },
    "tasks.image.preprocess": {
        "queue": "q.fast",
        "routing_key": "waste.high.preprocess",
        "priority": 9,
    },
    "tasks.result.save": {
        "queue": "q.fast",
        "routing_key": "waste.high.save",
        "priority": 10,
    },
    
    # External API (ê²©ë¦¬)
    "tasks.ai.vision": {
        "queue": "q.external",
        "routing_key": "external.ai.vision",
        "priority": 10,
    },
    "tasks.llm.feedback": {
        "queue": "q.external",
        "routing_key": "external.llm.feedback",
        "priority": 7,
    },
    "tasks.location.search": {
        "queue": "q.external",
        "routing_key": "external.map.location",
        "priority": 5,
    },
    
    # Bulk ì‘ì—… (ë°°ì¹˜)
    "tasks.analytics.save": {
        "queue": "q.bulk",
        "routing_key": "analytics.low.history",
        "priority": 1,
    },
    
    # Scheduled ì‘ì—…
    "tasks.daily.stats": {
        "queue": "q.sched",
        "routing_key": "sched.daily.stats",
    },
    "tasks.cleanup.images": {
        "queue": "q.sched",
        "routing_key": "sched.daily.cleanup",
    },
}

# ê³µí†µ ì„¤ì •
broker_url = "amqp://admin:password@rabbitmq.messaging.svc.cluster.local:5672//"
result_backend = "redis://redis.default.svc.cluster.local:6379/1"

# ê³µí‰ì„± & ì•ˆì •ì„±
worker_prefetch_multiplier = 1  # ê¸°ë³¸ê°’, Workerë³„ë¡œ override
task_acks_late = True  # ì²˜ë¦¬ ì™„ë£Œ í›„ ACK (ì¬ì‹œë„ ì•ˆì „)
task_reject_on_worker_lost = True
task_queue_max_priority = 10

# ì¬ì‹œë„ ê¸°ë³¸ê°’
task_autoretry_for = (Exception,)
task_retry_kwargs = {"max_retries": 3}
task_retry_backoff = True
task_retry_backoff_max = 60
```

---

## ğŸ–¥ï¸ Worker ë³„ ì„¤ì •

### Fast Worker (q.fast ì „ìš©)

```python
# workers/fast_worker.py
from celery import Celery
from app.core.celery_config import *

app = Celery("fast_worker")

app.conf.update(
    broker_url=broker_url,
    result_backend=result_backend,
    task_queues=task_queues,
    
    # Fast Queueë§Œ ì†Œë¹„
    task_default_queue="q.fast",
    worker_queues=["q.fast"],
    
    # CPU ìµœì í™”
    worker_pool="processes",  # ë©€í‹°í”„ë¡œì„¸ì‹±
    worker_concurrency=10,    # CPU ì½”ì–´ í™œìš©
    worker_prefetch_multiplier=4,  # ë¹ ë¥¸ ì†Œë¹„
    
    # ë¹ ë¥¸ ACK
    task_acks_late=False,
    
    # íƒ€ì„ì•„ì›ƒ
    task_time_limit=60,
    task_soft_time_limit=50,
)

if __name__ == "__main__":
    app.start()
```

### External Worker (q.external ì „ìš©)

```python
# workers/external_worker.py
app = Celery("external_worker")

app.conf.update(
    broker_url=broker_url,
    result_backend=result_backend,
    
    # External Queueë§Œ
    worker_queues=["q.external"],
    
    # ë„¤íŠ¸ì›Œí¬ ìµœì í™”
    worker_pool="gevent",  # ë¹„ë™ê¸° I/O
    worker_concurrency=20,  # ë„¤íŠ¸ì›Œí¬ ëŒ€ê¸° í™œìš©
    worker_prefetch_multiplier=2,  # ì†Œìˆ˜ (ê³¼ë¶€í•˜ ë°©ì§€)
    
    # Late ACK (API ì„±ê³µ í›„)
    task_acks_late=True,
    
    # ê¸´ íƒ€ì„ì•„ì›ƒ
    task_time_limit=300,  # 5ë¶„
    task_soft_time_limit=240,
    
    # ì¬ì‹œë„ ì •ì±…
    task_autoretry_for=(ConnectionError, TimeoutError),
    task_retry_kwargs={"max_retries": 3},
    task_retry_backoff=True,
    task_retry_backoff_max=120,
)
```

### Bulk Worker (q.bulk ì „ìš©)

```python
# workers/bulk_worker.py
app = Celery("bulk_worker")

app.conf.update(
    broker_url=broker_url,
    result_backend=result_backend,
    
    worker_queues=["q.bulk"],
    
    # I/O ìµœì í™”
    worker_pool="gevent",
    worker_concurrency=4,
    worker_prefetch_multiplier=1,  # â­ ê³µí‰ì„±! (í—¤ë“œì˜¤ë¸Œë¼ì¸ ë°©ì§€)
    
    # Late ACK
    task_acks_late=True,
    
    # ë„‰ë„‰í•œ íƒ€ì„ì•„ì›ƒ
    task_time_limit=600,  # 10ë¶„
    task_soft_time_limit=540,
)
```

### Scheduled Worker (q.sched ì „ìš©)

```python
# workers/sched_worker.py
app = Celery("sched_worker")

app.conf.update(
    broker_url=broker_url,
    result_backend=result_backend,
    
    worker_queues=["q.sched"],
    
    worker_pool="gevent",
    worker_concurrency=4,
    worker_prefetch_multiplier=1,
    
    task_acks_late=True,
    task_time_limit=600,
)

# Celery Beat Schedule
app.conf.beat_schedule = {
    "daily-stats": {
        "task": "tasks.daily.stats",
        "schedule": crontab(hour=2, minute=0),  # ë§¤ì¼ 02:00
        "options": {
            "queue": "q.sched",
            "routing_key": "sched.daily.stats",
        },
    },
    "hourly-cache-cleanup": {
        "task": "tasks.cleanup.cache",
        "schedule": crontab(minute=0),  # ë§¤ì‹œê°„
        "options": {
            "queue": "q.sched",
            "routing_key": "sched.hourly.cleanup",
        },
    },
    "daily-image-cleanup": {
        "task": "tasks.cleanup.images",
        "schedule": crontab(hour=3, minute=0),  # ë§¤ì¼ 03:00
        "options": {
            "queue": "q.sched",
            "routing_key": "sched.daily.cleanup",
        },
    },
}
```

---

## ğŸ—ï¸ K8s Worker ë°°ì¹˜

### Deployment êµ¬ì¡°

```mermaid
graph TB
    subgraph Master["Master Node (t3.medium)"]
        M[k3s Server<br/>+ RabbitMQ Pod]
    end
    
    subgraph Worker1["Worker 1 (t3.medium) - CPU"]
        W1[Fast Worker Pods Ã—5<br/>q.fast ì†Œë¹„<br/>processes pool<br/>concurrency: 10]
    end
    
    subgraph Worker2["Worker 2 (t3.medium) - Network"]
        W2a[External-AI Worker Ã—3<br/>q.external (AI)<br/>gevent pool]
        W2b[External-LLM Worker Ã—2<br/>q.external (LLM)<br/>gevent pool]
    end
    
    subgraph Worker3["Worker 3 (t3.small) - I/O & Sched"]
        W3a[Bulk Worker Ã—2<br/>q.bulk<br/>gevent pool]
        W3b[Sched Worker Ã—1<br/>q.sched<br/>gevent pool]
        W3c[Beat Ã—1<br/>ìŠ¤ì¼€ì¤„ëŸ¬]
        W3d[API Services<br/>auth, users, locations]
    end
    
    M -.-> W1
    M -.-> W2a
    M -.-> W2b
    M -.-> W3a
    
    style M fill:#ffd1d1,stroke:#dc3545,stroke-width:4px,color:#000
    style W1 fill:#ffdddd,stroke:#ff4444,stroke-width:3px,color:#000
    style W2a fill:#cce5ff,stroke:#007bff,stroke-width:3px,color:#000
    style W2b fill:#e6d5ff,stroke:#8844ff,stroke-width:3px,color:#000
    style W3a fill:#ffe0b3,stroke:#fd7e14,stroke-width:2px,color:#000
    style W3b fill:#d1f2eb,stroke:#28a745,stroke-width:2px,color:#000
    style W3c fill:#ccf5f0,stroke:#20c997,stroke-width:3px,color:#000
    style W3d fill:#fff4dd,stroke:#ffc107,stroke-width:2px,color:#000
```

### K8s Deployment YAML

```yaml
# k8s/waste/fast-worker-deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: fast-worker
  namespace: waste
spec:
  replicas: 5
  selector:
    matchLabels:
      app: fast-worker
  template:
    metadata:
      labels:
        app: fast-worker
        queue: fast
    spec:
      nodeSelector:
        workload: cpu  # Worker 1
      containers:
      - name: worker
        image: waste-service:latest
        command:
        - celery
        - -A
        - workers.fast_worker
        - worker
        - --loglevel=info
        - --queues=q.fast
        - --concurrency=10
        - --pool=processes
        - --prefetch-multiplier=4
        env:
        - name: CELERY_BROKER_URL
          value: "amqp://admin:password@rabbitmq.messaging:5672//"
        - name: CELERY_RESULT_BACKEND
          value: "redis://redis.default:6379/1"
        resources:
          requests:
            cpu: 500m
            memory: 512Mi
          limits:
            cpu: 2000m
            memory: 1Gi

---
# k8s/waste/external-worker-deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: external-ai-worker
  namespace: waste
spec:
  replicas: 3
  template:
    spec:
      nodeSelector:
        workload: network  # Worker 2
      containers:
      - name: worker
        command:
        - celery
        - -A
        - workers.external_worker
        - worker
        - --loglevel=info
        - --queues=q.external
        - --concurrency=20
        - --pool=gevent
        - --prefetch-multiplier=2
        env:
        - name: AI_VISION_API_URL
          value: "https://api.roboflow.com/..."
        - name: OPENAI_API_KEY
          valueFrom:
            secretKeyRef:
              name: llm-secrets
              key: openai-api-key
        resources:
          requests:
            cpu: 200m
            memory: 256Mi
          limits:
            cpu: 1000m
            memory: 512Mi

---
# k8s/analytics/beat-deployment.yaml (âš ï¸ Replicas: 1)
apiVersion: apps/v1
kind: Deployment
metadata:
  name: celery-beat
  namespace: analytics
spec:
  replicas: 1  # âš ï¸ ë°˜ë“œì‹œ 1ê°œ!
  template:
    spec:
      containers:
      - name: beat
        command:
        - celery
        - -A
        - workers.sched_worker
        - beat
        - --loglevel=info
        - --scheduler=celery.beat:PersistentScheduler
```

---

## ğŸ“Š ë…¸ë“œë³„ ë¦¬ì†ŒìŠ¤ ë°°ì¹˜

### 3ë…¸ë“œ êµ¬ì„± ($91/ì›”)

```
Master (t3.medium, $30/ì›”):
â”œâ”€ k3s Control Plane
â”œâ”€ RabbitMQ (1 Pod)
â”œâ”€ ArgoCD
â””â”€ Prometheus

Worker 1 (t3.medium, $30/ì›”) - CPU ì§‘ì•½:
â”œâ”€ Fast Workers Ã—5 (q.fast)
â”‚   â””â”€ CPU: 2 cores ê±°ì˜ í’€ ì‚¬ìš©
â””â”€ ë¦¬ì†ŒìŠ¤: CPU 90%, Memory 70%

Worker 2 (t3.medium, $30/ì›”) - Network ì§‘ì•½:
â”œâ”€ External-AI Workers Ã—3 (q.external - AI)
â”œâ”€ External-LLM Workers Ã—2 (q.external - LLM)
â””â”€ ë¦¬ì†ŒìŠ¤: CPU 30%, Memory 40%, Network 80%

Worker 3 (t3.small, $15/ì›”) - I/O & Sched:
â”œâ”€ Bulk Workers Ã—2 (q.bulk)
â”œâ”€ Sched Worker Ã—1 (q.sched)
â”œâ”€ Beat Ã—1 (ìŠ¤ì¼€ì¤„ëŸ¬)
â””â”€ API Services (auth, users, locations)
â””â”€ ë¦¬ì†ŒìŠ¤: CPU 50%, Memory 60%

ì´: $105/ì›” (RabbitMQ í¬í•¨)
```

---

## âœ… ìš´ì˜ ì²´í¬ë¦¬ìŠ¤íŠ¸

### ë°°í¬ ì „ ê²€ì¦

```bash
# 1. RabbitMQ í ìƒì„± í™•ì¸
rabbitmqctl list_queues name messages consumers

# ì˜ˆìƒ ì¶œë ¥:
# q.fast         0    5
# q.bulk         0    2
# q.external     0    5
# q.sched        0    1
# q.dlq          0    0

# 2. Exchange í™•ì¸
rabbitmqctl list_exchanges

# 3. Binding í™•ì¸
rabbitmqctl list_bindings

# 4. Worker ì—°ê²° í™•ì¸
celery -A workers.fast_worker inspect active_queues

# 5. DLQ ê¸¸ì´ ëª¨ë‹ˆí„°ë§
watch -n 5 'rabbitmqctl list_queues name messages | grep dlq'
```

### ìš´ì˜ ëª¨ë‹ˆí„°ë§

```python
# Prometheus ë©”íŠ¸ë¦­
rabbitmq_queue_messages{queue="q.fast"}
rabbitmq_queue_messages{queue="q.dlq"}  # âš ï¸ ì¦ê°€ ì‹œ ì•ŒëŒ
celery_task_duration_seconds{queue="q.fast"}
celery_task_failures_total{queue="q.external"}

# ì•ŒëŒ ì¡°ê±´:
- q.dlq ê¸¸ì´ > 100 â†’ Critical
- q.fast ê¸¸ì´ > 1,000 â†’ Warning
- q.external ì‹¤íŒ¨ìœ¨ > 10% â†’ Warning
```

---

## ğŸ¯ í•µì‹¬ í¬ì¸íŠ¸

### ì™œ ì´ë ‡ê²Œ ì„¤ê³„í–ˆëŠ”ê°€?

```
1. í—¤ë“œì˜¤ë¸Œë¼ì¸ ë°©ì§€ (ê³µí‰ì„±)
   âœ… ê¸´ ì‘ì—…(q.bulk): prefetch=1
   âœ… ì§§ì€ ì‘ì—…(q.fast): prefetch=4
   â†’ ê¸´ ì‘ì—…ì´ ì§§ì€ ì‘ì—… êµ¶ê¸°ì§€ ì•ŠìŒ!

2. ë¸Œë¡œì»¤ ë³´í˜¸
   âœ… TTL: ë©”ì‹œì§€ ë§Œë£Œ â†’ DLQ ì´ë™
   âœ… max-length: í ê¸¸ì´ ì œí•œ â†’ í­ì£¼ ë°©ì§€
   âœ… overflow: reject-publish â†’ ê±°ë¶€

3. ì¥ì•  ê²©ë¦¬
   âœ… ì™¸ë¶€ API ì¥ì•  â†’ q.externalë§Œ ì˜í–¥
   âœ… q.fast, q.bulkëŠ” ê³„ì† ì •ìƒ ì‘ë™

4. ë©±ë“±ì„± (ì¤‘ë³µ ì‹¤í–‰ ëŒ€ë¹„)
   âœ… acks_late=True â†’ ì²˜ë¦¬ ì™„ë£Œ í›„ ACK
   âœ… TaskëŠ” ë°˜ë“œì‹œ ë©±ë“±í•˜ê²Œ ì„¤ê³„
   âœ… ì¤‘ë³µ í˜¸ì¶œë˜ì–´ë„ ì•ˆì „
```

---

## ğŸ“š ì°¸ê³  ìë£Œ

- [Celery ê³µì‹ ê°€ì´ë“œ - Routing](https://docs.celeryq.dev/en/stable/userguide/routing.html)
- [RabbitMQ - Dead Letter Exchanges](https://www.rabbitmq.com/dlx.html)
- [Celery - Task Retry](https://docs.celeryq.dev/en/stable/userguide/tasks.html#retrying)
- [RabbitMQ - TTL and Expiry](https://www.rabbitmq.com/ttl.html)

---

**ì‘ì„±ì¼**: 2025-10-30  
**ìƒíƒœ**: ğŸ”„ ìŠ¹ì¸ ëŒ€ê¸°  
**ë¹„ìš©**: $105/ì›” (3ë…¸ë“œ + RabbitMQ)

