# ğŸ—ï¸ ìµœì¢… Kubernetes ì•„í‚¤í…ì²˜

> **â™»ï¸ ì´ì½”ì—ì½”(EcoÂ²) Backend - í”„ë¡œë•ì…˜ê¸‰ K8s ì¸í”„ë¼**  
> **ìµœì¢… ì—…ë°ì´íŠ¸**: 2025-11-11  
> **ìƒíƒœ**: âœ… í”„ë¡œë•ì…˜ ë°°í¬ ì™„ë£Œ (14-Node í´ëŸ¬ìŠ¤í„°)

## ğŸ“‹ ëª©ì°¨

1. [ì „ì²´ ì•„í‚¤í…ì²˜](#ì „ì²´-ì•„í‚¤í…ì²˜)
2. [í´ëŸ¬ìŠ¤í„° êµ¬ì„±](#í´ëŸ¬ìŠ¤í„°-êµ¬ì„±)
3. [ë§ˆì´í¬ë¡œì„œë¹„ìŠ¤ ë°°ì¹˜](#ë§ˆì´í¬ë¡œì„œë¹„ìŠ¤-ë°°ì¹˜)
4. [GitOps íŒŒì´í”„ë¼ì¸](#gitops-íŒŒì´í”„ë¼ì¸)
5. [ë°ì´í„° íë¦„](#ë°ì´í„°-íë¦„)
6. [ë„¤íŠ¸ì›Œí¬ êµ¬ì¡°](#ë„¤íŠ¸ì›Œí¬-êµ¬ì¡°)

---

## ğŸŒ ì „ì²´ ì•„í‚¤í…ì²˜

```mermaid
graph TB
    subgraph Internet["ğŸŒ ì¸í„°ë„·"]
        Users[ì‚¬ìš©ì<br/>Mobile App]
    end
    
    subgraph GitHub["GitHub"]
        Code[Code Repository<br/>services/]
        Charts[Helm Charts<br/>charts/]
        GHA[GitHub Actions<br/>CI Pipeline]
        GHCR[GitHub Container Registry<br/>ghcr.io]
    end
    
    subgraph AWS["AWS í´ë¼ìš°ë“œ"]
        Route53[Route 53<br/>DNS]
        ACM[ACM<br/>SSL Certificate]
        ALB[Application Load Balancer<br/>ecoeco.app]
        S3[S3 Bucket<br/>ì´ë¯¸ì§€ ì €ì¥]
        CloudFront[CloudFront<br/>CDN]
    end
    
    subgraph K8s["Kubernetes Cluster (14 Nodes, Self-Managed)"]
        subgraph Master["Master Node (t3.large)"]
            CP[Control Plane<br/>API Server<br/>etcd<br/>Scheduler]
            ArgoCD[ArgoCD<br/>GitOps Engine]
            Atlantis[Atlantis<br/>Terraform PR Automation]
        end
        
        subgraph API["API Nodes (7ê°œ, ê° ì „ìš© ë…¸ë“œ)"]
            AuthSvc[auth-service<br/>t3.micro]
            MySvc[my-service<br/>t3.micro]
            ScanSvc[scan-service<br/>t3.small]
            CharSvc[character-service<br/>t3.micro]
            LocSvc[location-service<br/>t3.micro]
            InfoSvc[info-service<br/>t3.micro]
            ChatSvc[chat-service<br/>t3.small]
        end
        
        subgraph Workers["Worker Nodes (2ê°œ)"]
            StorageW[storage-worker<br/>t3.small<br/>S3 ì—…ë¡œë“œ]
            AIW[ai-worker<br/>t3.small<br/>AI ì¶”ë¡ ]
        end
        
        subgraph Infra["Infrastructure Nodes (4ê°œ)"]
            DB[(PostgreSQL<br/>t3.small)]
            Redis[(Redis<br/>t3.micro)]
            RabbitMQ[RabbitMQ<br/>t3.small]
            Monitoring[Prometheus<br/>Grafana<br/>t3.small]
        end
        
        ALBC[AWS Load Balancer<br/>Controller]
    end
    
    subgraph External["ì™¸ë¶€ ì„œë¹„ìŠ¤"]
        Roboflow[Roboflow API<br/>AI Vision]
        OpenAI[OpenAI API<br/>LLM]
        KakaoMap[Kakao Map API<br/>ìœ„ì¹˜ ê²€ìƒ‰]
    end
    
    Users --> Route53
    Route53 --> ALB
    ACM -.->|SSL Cert| ALB
    ALB --> ALBC
    
    ALBC -->|/api/v1/auth| AuthSvc
    ALBC -->|/api/v1/my| MySvc
    ALBC -->|/api/v1/scan| ScanSvc
    ALBC -->|/api/v1/character| CharSvc
    ALBC -->|/api/v1/location| LocSvc
    ALBC -->|/api/v1/info| InfoSvc
    ALBC -->|/api/v1/chat| ChatSvc
    ALBC -->|/argocd| ArgoCD
    ALBC -->|/atlantis| Atlantis
    ALBC -->|/grafana| Monitoring
    
    Code --> GHA
    GHA --> GHCR
    GHA --> Charts
    Charts --> ArgoCD
    ArgoCD -.->|ë°°í¬| AuthSvc
    ArgoCD -.->|ë°°í¬| MySvc
    ArgoCD -.->|ë°°í¬| ScanSvc
    
    ScanSvc --> S3
    StorageW --> S3
    S3 --> CloudFront
    
    AuthSvc --> DB
    MySvc --> DB
    ScanSvc --> DB
    CharSvc --> DB
    LocSvc --> DB
    InfoSvc --> DB
    ChatSvc --> DB
    
    AuthSvc --> Redis
    MySvc --> Redis
    
    AIW --> Roboflow
    ChatSvc --> OpenAI
    LocSvc --> KakaoMap
    
    GHCR -.->|Pull Image| AuthSvc
    GHCR -.->|Pull Image| MySvc
    GHCR -.->|Pull Image| ScanSvc
    
    style Users fill:#1e40af,stroke:#3b82f6,stroke-width:4px,color:#fff
    style GHA fill:#b91c1c,stroke:#dc2626,stroke-width:3px,color:#fff
    style ALB fill:#c2410c,stroke:#ea580c,stroke-width:4px,color:#fff
    style Master fill:#1e3a8a,stroke:#2563eb,stroke-width:3px,color:#fff
    style API fill:#166534,stroke:#16a34a,stroke-width:2px,color:#fff
    style Workers fill:#a16207,stroke:#ca8a04,stroke-width:2px,color:#fff
    style Infra fill:#991b1b,stroke:#dc2626,stroke-width:3px,color:#fff
    style ArgoCD fill:#6b21a8,stroke:#9333ea,stroke-width:3px,color:#fff
    style DB fill:#0e7490,stroke:#06b6d4,stroke-width:3px,color:#fff
    style Redis fill:#be123c,stroke:#e11d48,stroke-width:2px,color:#fff
```

---

## ğŸ–¥ï¸ í´ëŸ¬ìŠ¤í„° êµ¬ì„±

### 14-Node ì•„í‚¤í…ì²˜ (Self-Managed Kubernetes)

```mermaid
graph TB
    subgraph "14-Node Production Cluster"
        subgraph "Master (1)"
            M[k8s-master<br/>t3.large<br/>2 vCPU, 8GB<br/>Control Plane + ArgoCD]
        end
        
        subgraph "API Services (7)"
            A1[auth-node<br/>t3.micro<br/>2 vCPU, 1GB]
            A2[my-node<br/>t3.micro<br/>2 vCPU, 1GB]
            A3[scan-node<br/>t3.small<br/>2 vCPU, 2GB]
            A4[character-node<br/>t3.micro<br/>2 vCPU, 1GB]
            A5[location-node<br/>t3.micro<br/>2 vCPU, 1GB]
            A6[info-node<br/>t3.micro<br/>2 vCPU, 1GB]
            A7[chat-node<br/>t3.small<br/>2 vCPU, 2GB]
    end
    
        subgraph "Workers (2)"
            W1[storage-node<br/>t3.small<br/>2 vCPU, 2GB]
            W2[ai-node<br/>t3.small<br/>2 vCPU, 2GB]
    end
    
        subgraph "Infrastructure (4)"
            I1[postgresql-node<br/>t3.small<br/>2 vCPU, 2GB]
            I2[redis-node<br/>t3.micro<br/>2 vCPU, 1GB]
            I3[rabbitmq-node<br/>t3.small<br/>2 vCPU, 2GB]
            I4[monitoring-node<br/>t3.small<br/>2 vCPU, 2GB]
        end
    end
    
    Total["ğŸ“Š Total: 14 nodes, 30 vCPU, 22GB RAM"]
    
    style M fill:#b91c1c,color:#fff
    style A1 fill:#0e7490,color:#fff
    style A2 fill:#0e7490,color:#fff
    style A3 fill:#0e7490,color:#fff
    style A4 fill:#0e7490,color:#fff
    style A5 fill:#0e7490,color:#fff
    style A6 fill:#0e7490,color:#fff
    style A7 fill:#0e7490,color:#fff
    style W1 fill:#a16207,color:#fff
    style W2 fill:#a16207,color:#fff
    style I1 fill:#166534,color:#fff
    style I2 fill:#166534,color:#fff
    style I3 fill:#166534,color:#fff
    style I4 fill:#166534,color:#fff
```

### ë…¸ë“œë³„ ì—­í•  ë° ë¦¬ì†ŒìŠ¤

```yaml
Master Node (1):
  - t3.large: 2 vCPU, 8GB RAM
  - Control Plane (kube-apiserver, etcd, scheduler, controller-manager)
  - ArgoCD (GitOps CD)
  - Atlantis (Terraform PR Automation)
  - AWS ALB Ingress Controller
  - Cert-Manager
  - ë¹„ìš©: $60/ì›”

API Nodes (7):
  auth-service:
    - t3.micro: 2 vCPU, 1GB RAM
    - OAuth 2.0, JWT í† í° ê´€ë¦¬
    - ë¹„ìš©: $7/ì›”
  
  my-service:
    - t3.micro: 2 vCPU, 1GB RAM
    - ì‚¬ìš©ì í”„ë¡œí•„, ì´ë ¥ ê´€ë¦¬
    - ë¹„ìš©: $7/ì›”
  
  scan-service:
    - t3.small: 2 vCPU, 2GB RAM
    - ì“°ë ˆê¸° ì´ë¯¸ì§€ ìŠ¤ìº” ë° ë¶„ì„
    - ë¹„ìš©: $15/ì›”
  
  character-service:
    - t3.micro: 2 vCPU, 1GB RAM
    - ìºë¦­í„° ì‹œìŠ¤í…œ ê´€ë¦¬
    - ë¹„ìš©: $7/ì›”
  
  location-service:
    - t3.micro: 2 vCPU, 1GB RAM
    - ìˆ˜ê±°í•¨ ìœ„ì¹˜ ê²€ìƒ‰
    - ë¹„ìš©: $7/ì›”
  
  info-service:
    - t3.micro: 2 vCPU, 1GB RAM
    - ì •ë³´ ì¡°íšŒ ì„œë¹„ìŠ¤
    - ë¹„ìš©: $7/ì›”
  
  chat-service:
    - t3.small: 2 vCPU, 2GB RAM
    - AI ì±—ë´‡ (LLM ì—°ë™)
    - ë¹„ìš©: $15/ì›”

Worker Nodes (2):
  storage-worker:
    - t3.small: 2 vCPU, 2GB RAM
    - S3 ì´ë¯¸ì§€ ì—…ë¡œë“œ ì²˜ë¦¬
    - ë¹„ìš©: $15/ì›”
  
  ai-worker:
    - t3.small: 2 vCPU, 2GB RAM
    - AI ì¶”ë¡  ì‘ì—… (Roboflow)
    - ë¹„ìš©: $15/ì›”

Infrastructure Nodes (4):
  postgresql:
    - t3.small: 2 vCPU, 2GB RAM
    - ë©”ì¸ ë°ì´í„°ë² ì´ìŠ¤
    - 30GB EBS (gp3)
    - ë¹„ìš©: $15/ì›” + $2.4/ì›” (storage)
  
  redis:
    - t3.micro: 2 vCPU, 1GB RAM
    - JWT Blacklist, ìºì‹œ
    - ë¹„ìš©: $7/ì›”
  
  rabbitmq:
    - t3.small: 2 vCPU, 2GB RAM
    - ë©”ì‹œì§€ ë¸Œë¡œì»¤
    - ë¹„ìš©: $15/ì›”
  
  monitoring:
    - t3.small: 2 vCPU, 2GB RAM
    - Prometheus + Grafana
    - 50GB EBS (gp3)
    - ë¹„ìš©: $15/ì›” + $4/ì›” (storage)

ì´ ë¹„ìš©: ~$218/ì›” (EC2 + EBS)
```

---

## ğŸ”„ GitOps íŒŒì´í”„ë¼ì¸

### CI/CD ì „ì²´ íë¦„

```mermaid
sequenceDiagram
    actor Dev as ê°œë°œì
    participant GH as GitHub<br/>Repository
    participant GHA as GitHub Actions<br/>CI Pipeline
    participant GHCR as GHCR<br/>ghcr.io
    participant Helm as Helm Charts<br/>(Git)
    participant Argo as ArgoCD<br/>(Master Node)
    participant K8s as Kubernetes<br/>14-Node Cluster
    
    Dev->>GH: 1. services/auth/ ì½”ë“œ ìˆ˜ì • & Push
    GH->>GHA: 2. .github/workflows/ci-auth.yml íŠ¸ë¦¬ê±°
    
    activate GHA
    GHA->>GHA: 3. Lint & Format Check
    GHA->>GHA: 4. Unit Tests
    GHA->>GHA: 5. Docker Build
    GHA->>GHCR: 6. Push auth-service:abc123
    GHA->>Helm: 7. charts/ecoeco-backend/<br/>values-14nodes.yaml ì—…ë°ì´íŠ¸<br/>auth.image.tag: abc123
    deactivate GHA
    
    Note over Argo: 8. Git í´ë§ (3ë¶„ë§ˆë‹¤)
    
    activate Argo
    Argo->>Helm: 9. ë³€ê²½ ê°ì§€!
    Argo->>Argo: 10. Helm Template ë Œë”ë§
    Argo->>Argo: 11. Diff ê³„ì‚°
    Argo->>K8s: 12. kubectl apply (ìë™ Sync)
    deactivate Argo
    
    activate K8s
    K8s->>GHCR: 13. Pull auth-service:abc123
    K8s->>K8s: 14. Rolling Update (ë¬´ì¤‘ë‹¨)
    K8s->>K8s: 15. Health Check (readiness/liveness)
    deactivate K8s
    
    K8s-->>Argo: 16. Sync ì™„ë£Œ
    Argo-->>Dev: 17. Slack ì•Œë¦¼: âœ… ë°°í¬ ì„±ê³µ
```

### ArgoCD Application êµ¬ì¡°

```yaml
apiVersion: argoproj.io/v1alpha1
kind: Application
metadata:
  name: ecoeco-backend-14nodes
  namespace: argocd
spec:
  project: default
  
  # Git Source
  source:
    repoURL: https://github.com/SeSACTHON/backend.git
    targetRevision: develop
    path: charts/ecoeco-backend
    helm:
      valueFiles:
        - values-14nodes.yaml
  
  # Destination Cluster
  destination:
    server: https://kubernetes.default.svc
    namespace: default
  
  # Sync Policy
  syncPolicy:
    automated:
      prune: true        # ì‚­ì œëœ ë¦¬ì†ŒìŠ¤ ìë™ ì œê±°
      selfHeal: true     # Drift ìë™ ìˆ˜ì •
      allowEmpty: false
    syncOptions:
      - CreateNamespace=true
    retry:
      limit: 5
      backoff:
        duration: 5s
        factor: 2
        maxDuration: 3m
```

---

## ğŸ“Š ë§ˆì´í¬ë¡œì„œë¹„ìŠ¤ ë°°ì¹˜

### Namespaceë³„ ì„œë¹„ìŠ¤ êµ¬ì„±

```yaml
Namespaces:
  argocd:
    - argocd-server
    - argocd-repo-server
    - argocd-application-controller
    - argocd-dex-server
    - argocd-redis
  
  atlantis:
    - atlantis (Terraform PR Automation)
  
  kube-system:
    - aws-load-balancer-controller
    - cert-manager
    - ebs-csi-controller
    - coredns
    - kube-proxy
  
  monitoring:
    - prometheus-server
    - grafana
    - node-exporter (DaemonSet, 14 pods)
  
  default:
    - auth-service (2 replicas)
    - my-service (2 replicas)
    - scan-service (2 replicas)
    - character-service (2 replicas)
    - location-service (2 replicas)
    - info-service (2 replicas)
    - chat-service (2 replicas)
    - storage-worker (1 replica)
    - ai-worker (1 replica)
    - postgresql (StatefulSet)
    - redis (StatefulSet)
    - rabbitmq (StatefulSet)
```

### NodeSelectorë¥¼ í†µí•œ Pod ë°°ì¹˜

```yaml
# auth-service â†’ auth-node
apiVersion: apps/v1
kind: Deployment
metadata:
  name: auth-service
spec:
  replicas: 2
  template:
    spec:
      nodeSelector:
        node-role: auth
      containers:
        - name: auth
          image: ghcr.io/sesacthon/auth-service:latest
          resources:
            requests:
              memory: "256Mi"
              cpu: "100m"
            limits:
              memory: "512Mi"
              cpu: "500m"

# scan-service â†’ scan-node
apiVersion: apps/v1
kind: Deployment
metadata:
  name: scan-service
spec:
  replicas: 2
  template:
    spec:
      nodeSelector:
        node-role: scan
      containers:
        - name: scan
          image: ghcr.io/sesacthon/scan-service:latest
          resources:
            requests:
              memory: "512Mi"
              cpu: "200m"
            limits:
              memory: "1Gi"
              cpu: "1000m"
```

---

## ğŸ—ºï¸ ë°ì´í„° íë¦„

### ì“°ë ˆê¸° ìŠ¤ìº” ìš”ì²­ íë¦„

```mermaid
sequenceDiagram
    actor User as ì‚¬ìš©ì
    participant App as Mobile App
    participant ALB as AWS ALB
    participant Scan as scan-service<br/>(scan-node)
    participant DB as PostgreSQL<br/>(postgresql-node)
    participant Redis as Redis<br/>(redis-node)
    participant S3 as AWS S3
    participant CF as CloudFront
    participant AIW as ai-worker<br/>(ai-node)
    participant AI as Roboflow API
    
    User->>App: ì“°ë ˆê¸° ì‚¬ì§„ ì´¬ì˜
    App->>ALB: POST /api/v1/scan/analyze
    ALB->>Scan: ë¼ìš°íŒ…
    
    Scan->>Scan: Job ID ìƒì„±
    Scan->>App: S3 Presigned URL ë°˜í™˜
    
    App->>S3: ì´ë¯¸ì§€ ì§ì ‘ ì—…ë¡œë“œ
    App->>Scan: POST /scan/complete/{job_id}
    
    Scan->>Redis: ìºì‹œ í™•ì¸ (ì´ë¯¸ì§€ í•´ì‹œ)
    
    alt ìºì‹œ íˆíŠ¸ (70%)
        Redis-->>Scan: ë¶„ì„ ê²°ê³¼ ë°˜í™˜
        Scan-->>App: ì¦‰ì‹œ ì‘ë‹µ (< 1ì´ˆ)
    else ìºì‹œ ë¯¸ìŠ¤ (30%)
        Scan->>AIW: ë¹„ë™ê¸° ì‘ì—… ìš”ì²­
        
        activate AIW
        AIW->>S3: ì´ë¯¸ì§€ ë‹¤ìš´ë¡œë“œ
        AIW->>AI: AI Vision API í˜¸ì¶œ
        AI-->>AIW: ë¶„ë¥˜ ê²°ê³¼
        AIW->>DB: ê²°ê³¼ ì €ì¥
        AIW->>Redis: ìºì‹± (7ì¼)
        deactivate AIW
        
        loop í´ë§ (0.5ì´ˆë§ˆë‹¤)
            App->>Scan: GET /scan/status/{job_id}
            Scan->>Redis: ì§„í–‰ë¥  ì¡°íšŒ
            Redis-->>App: progress: 60%
    end    
        App->>Scan: GET /scan/status/{job_id}
        Scan->>Redis: ìµœì¢… ê²°ê³¼ ì¡°íšŒ
        Redis-->>App: ë¶„ì„ ì™„ë£Œ!
    end
    
    App->>CF: GET /{image_key}
    CF-->>App: ì´ë¯¸ì§€ í‘œì‹œ (CDN)
    App->>User: ê²°ê³¼ í™”ë©´ í‘œì‹œ
```

---

## ğŸŒ ë„¤íŠ¸ì›Œí¬ êµ¬ì¡°

### Ingress ë¼ìš°íŒ…

```yaml
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: ecoeco-api-ingress
  annotations:
    kubernetes.io/ingress.class: alb
    alb.ingress.kubernetes.io/scheme: internet-facing
    alb.ingress.kubernetes.io/target-type: ip
    alb.ingress.kubernetes.io/certificate-arn: arn:aws:acm:...
    alb.ingress.kubernetes.io/listen-ports: '[{"HTTP": 80}, {"HTTPS": 443}]'
    alb.ingress.kubernetes.io/ssl-redirect: '443'
spec:
  rules:
    - host: api.ecoeco.app
      http:
        paths:
          - path: /api/v1/auth
            pathType: Prefix
            backend:
              service:
                name: auth-service
                port:
                  number: 8000
          
          - path: /api/v1/my
            pathType: Prefix
            backend:
              service:
                name: my-service
                port:
                  number: 8000
          
          - path: /api/v1/scan
            pathType: Prefix
            backend:
              service:
                name: scan-service
                port:
                  number: 8000
          
          - path: /api/v1/character
            pathType: Prefix
            backend:
              service:
                name: character-service
                port:
                  number: 8000
          
          - path: /api/v1/location
            pathType: Prefix
            backend:
              service:
                name: location-service
                port:
                  number: 8000
          
          - path: /api/v1/info
            pathType: Prefix
            backend:
              service:
                name: info-service
                port:
                  number: 8000
          
          - path: /api/v1/chat
            pathType: Prefix
            backend:
              service:
                name: chat-service
                port:
                  number: 8000
```

---

## ğŸ“ˆ í™•ì¥ ê³„íš

### Horizontal Pod Autoscaler (HPA)

```yaml
# scan-service HPA (íŠ¸ë˜í”½ ëŒ€ì‘)
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: scan-service-hpa
  namespace: default
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: scan-service
  minReplicas: 2
  maxReplicas: 10
  metrics:
  - type: Resource
    resource:
      name: cpu
      target:
        type: Utilization
        averageUtilization: 70
  - type: Resource
    resource:
      name: memory
      target:
        type: Utilization
        averageUtilization: 80
  behavior:
    scaleUp:
      stabilizationWindowSeconds: 60
      policies:
        - type: Percent
          value: 100
          periodSeconds: 60
    scaleDown:
      stabilizationWindowSeconds: 300
      policies:
        - type: Percent
          value: 50
          periodSeconds: 60
```

### Cluster Autoscaler (ìˆ˜ë™ í™•ì¥)

```bash
# íŠ¸ë˜í”½ ì¦ê°€ ì‹œ Worker ë…¸ë“œ ì¶”ê°€
# 1. Terraformìœ¼ë¡œ ìƒˆ EC2 ì¸ìŠ¤í„´ìŠ¤ ìƒì„±
terraform apply -var="worker_count=4"

# 2. ìë™ìœ¼ë¡œ í´ëŸ¬ìŠ¤í„° ì¡°ì¸ (user-data)
# 3. NodeSelector ë¼ë²¨ ìë™ ì„¤ì •

# Spot Instance í™œìš© ê°€ëŠ¥
# t3.small Spot: $4.5/ì›” (70% í• ì¸)
```

---

## ğŸ”’ ë³´ì•ˆ

### Network Policies

```yaml
# auth-service ë„¤íŠ¸ì›Œí¬ ê²©ë¦¬
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: auth-service-policy
  namespace: default
spec:
  podSelector:
    matchLabels:
      app: auth-service
  policyTypes:
  - Ingress
  - Egress
  ingress:
    # ALB Ingress Controllerë§Œ í—ˆìš©
  - from:
    - namespaceSelector:
        matchLabels:
          name: kube-system
    ports:
    - protocol: TCP
      port: 8000
  egress:
    # PostgreSQL ì ‘ê·¼ í—ˆìš©
    - to:
        - podSelector:
            matchLabels:
              app: postgresql
      ports:
        - protocol: TCP
          port: 5432
    
    # Redis ì ‘ê·¼ í—ˆìš©
    - to:
        - podSelector:
            matchLabels:
              app: redis
      ports:
        - protocol: TCP
          port: 6379
    
    # DNS í—ˆìš©
  - to:
    - namespaceSelector:
        matchLabels:
              name: kube-system
    ports:
        - protocol: UDP
          port: 53
```

### Secret ê´€ë¦¬

```bash
# Sealed Secrets (GitOps ì¹œí™”ì )
# Secretì„ ì•”í˜¸í™”í•˜ì—¬ Gitì— ì•ˆì „í•˜ê²Œ ì €ì¥

# 1. Sealed Secrets ì„¤ì¹˜
kubectl apply -f https://github.com/bitnami-labs/sealed-secrets/releases/download/v0.24.0/controller.yaml

# 2. Secret ì•”í˜¸í™”
echo -n 'my-secret-password' | \
  kubectl create secret generic db-password \
    --dry-run=client \
    --from-file=password=/dev/stdin \
    -o yaml | \
  kubeseal -o yaml > sealed-secret.yaml

# 3. Gitì— ì»¤ë°‹ (ì•”í˜¸í™”ë¨!)
git add sealed-secret.yaml
git commit -m "Add encrypted database password"
```

---

## ğŸ“Š ëª¨ë‹ˆí„°ë§

### Prometheus ë©”íŠ¸ë¦­

```yaml
ëª¨ë‹ˆí„°ë§ ëŒ€ìƒ:
  ë…¸ë“œ (14ê°œ):
    - CPU ì‚¬ìš©ë¥ 
    - Memory ì‚¬ìš©ë¥ 
    - Disk I/O
    - Network Traffic
  
  Pods (30+):
    - Running/Pending/Failed ìƒíƒœ
    - Restart Count
    - CPU/Memory ì‚¬ìš©ëŸ‰
  
  Services (7ê°œ API):
    - Request Rate (req/s)
    - Latency (P50, P95, P99)
    - Error Rate (5xx)
  
  Ingress:
    - ALB Target Health
    - Request Distribution
  
  Database:
    - Connection Pool
    - Query Performance
    - Replication Lag (ì—†ìŒ, ë‹¨ì¼ ì¸ìŠ¤í„´ìŠ¤)

ì•ŒëŒ ê·œì¹™:
  Critical:
    - Node CPU > 90% (5ë¶„)
    - Node Memory > 90% (5ë¶„)
    - Pod CrashLoopBackOff
    - Disk Usage > 85%
  
  Warning:
    - API Latency P95 > 2ì´ˆ
    - Error Rate > 5%
    - PostgreSQL Connection > 80%
```

### Grafana ëŒ€ì‹œë³´ë“œ

```yaml
ëŒ€ì‹œë³´ë“œ êµ¬ì„±:
  1. Cluster Overview:
     - 14ê°œ ë…¸ë“œ ìƒíƒœ
     - ì „ì²´ ë¦¬ì†ŒìŠ¤ ì‚¬ìš©ë¥ 
     - Pod ë¶„í¬
  
  2. API Performance:
     - 7ê°œ API ë³„ Request Rate
     - Latency Distribution
     - Error Rate
  
  3. Node Resources:
     - CPU/Memory/Disk ì‚¬ìš©ë¥ 
     - Network Traffic
  
  4. Database:
     - Connection Pool
     - Query Duration
     - Cache Hit Ratio
  
  5. GitOps:
     - ArgoCD Sync Status
     - Application Health
     - Deployment History
```

---

## ğŸ¯ í•µì‹¬ ì‚¬ì–‘ ìš”ì•½

### í´ëŸ¬ìŠ¤í„°

```yaml
Kubernetes (kubeadm, Self-Managed):
  Version: v1.28.x
  CNI: Calico
  Nodes: 14ê°œ (1 Master + 7 API + 2 Worker + 4 Infra)
  HA: non-HA (ë‹¨ì¼ Master)
  
ì´ ë¦¬ì†ŒìŠ¤:
  vCPU: 30 cores
  Memory: 22GB
  Storage: 80GB EBS (gp3)
  ë¹„ìš©: ~$218/ì›”

ë„¤íŠ¸ì›Œí¬:
  VPC: 10.0.0.0/16
  Subnets: 
    - Public: 10.0.1.0/24, 10.0.2.0/24
    - Private: 10.0.10.0/24, 10.0.11.0/24
  Ingress: AWS ALB
  SSL: ACM Certificate
  DNS: Route53
```

### ë§ˆì´í¬ë¡œì„œë¹„ìŠ¤

```yaml
7ê°œ ë…ë¦½ API ì„œë¹„ìŠ¤:
  - auth-service: 2 replicas (OAuth, JWT)
  - my-service: 2 replicas (í”„ë¡œí•„, ì´ë ¥)
  - scan-service: 2 replicas (ì´ë¯¸ì§€ ë¶„ì„)
  - character-service: 2 replicas (ìºë¦­í„°)
  - location-service: 2 replicas (ìœ„ì¹˜ ê²€ìƒ‰)
  - info-service: 2 replicas (ì •ë³´ ì¡°íšŒ)
  - chat-service: 2 replicas (AI ì±—ë´‡)

2ê°œ Worker ì„œë¹„ìŠ¤:
  - storage-worker: 1 replica (S3 ì—…ë¡œë“œ)
  - ai-worker: 1 replica (AI ì¶”ë¡ )

ì´ Pods: ~16ê°œ (API) + 2ê°œ (Worker)
```

### GitOps

```yaml
CI/CD Pipeline:
  CI: GitHub Actions
    - Lint & Format
    - Unit Tests
    - Docker Build & Push (GHCR)
    - Helm Values Update
  
  CD: ArgoCD
    - Git í´ë§ (3ë¶„)
    - ìë™ Sync
    - Rollback ì§€ì›
    - Multi-Environment (dev, staging, prod)
  
  IaC: Terraform + Atlantis
    - PR ê¸°ë°˜ Plan/Apply
    - State Lock (S3 + DynamoDB)
    - ìë™ ë¬¸ì„œí™”

ì„±ëŠ¥:
  ë™ì‹œ ì‚¬ìš©ì: 1,000ëª…+
  ì²˜ë¦¬ ì‹œê°„: < 2ì´ˆ (P95)
  ìºì‹œ íˆíŠ¸ìœ¨: 70%
  ê°€ìš©ì„±: 99.5%+
```

---

## ğŸ“š ê´€ë ¨ ë¬¸ì„œ

- [K8s í´ëŸ¬ìŠ¤í„° êµ¬ì¶• ê°€ì´ë“œ](../deployment/K8S_CLUSTER_SETUP.md) - ìƒì„¸ ì„¤ì¹˜ ëª…ë ¹ì–´
- [GitOps ë°°í¬ ê°€ì´ë“œ](../deployment/GITOPS_DEPLOYMENT.md) - ArgoCD + Helm
- [Terraform ì¸í”„ë¼ êµ¬ì„±](../../terraform/README.md) - IaC ì„¤ì •
- [ëª¨ë‹ˆí„°ë§ ì„¤ì •](../deployment/MONITORING_SETUP.md) - Prometheus + Grafana
- [ë„¤íŠ¸ì›Œí¬ ì •ì±…](../deployment/NETWORK_POLICIES.md) - ë³´ì•ˆ ì„¤ì •

---

**ë¬¸ì„œ ë²„ì „**: v0.7.0  
**ìµœì¢… ì—…ë°ì´íŠ¸**: 2025-11-11  
**êµ¬ì„±**: Kubernetes (Self-Managed, 14-Node) + ArgoCD + Helm + GHCR + Atlantis  
**ì´ ë¹„ìš©**: ~$218/ì›”  
**ìƒíƒœ**: âœ… í”„ë¡œë•ì…˜ ë°°í¬ ì™„ë£Œ
