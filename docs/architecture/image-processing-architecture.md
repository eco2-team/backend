# ğŸ—ï¸ ì´ë¯¸ì§€ ì²˜ë¦¬ ì•„í‚¤í…ì²˜ ì„¤ê³„

> **AI ê¸°ë°˜ ì“°ë ˆê¸° ë¶„ë¥˜ ì„œë¹„ìŠ¤ì˜ ë°±ì—”ë“œ ì•„í‚¤í…ì²˜**  
> **ë‚ ì§œ**: 2025-10-30  
> **ë²„ì „**: 1.0

## ğŸ“‹ ëª©ì°¨

1. [ì‹œìŠ¤í…œ ê°œìš”](#ì‹œìŠ¤í…œ-ê°œìš”)
2. [ì „ì²´ ì•„í‚¤í…ì²˜](#ì „ì²´-ì•„í‚¤í…ì²˜)
3. [í•µì‹¬ ì»´í¬ë„ŒíŠ¸](#í•µì‹¬-ì»´í¬ë„ŒíŠ¸)
4. [ë°ì´í„° íë¦„](#ë°ì´í„°-íë¦„)
5. [ìµœì í™” ì „ëµ](#ìµœì í™”-ì „ëµ)
6. [í™•ì¥ì„± ê³ ë ¤ì‚¬í•­](#í™•ì¥ì„±-ê³ ë ¤ì‚¬í•­)

---

## ğŸ¯ ì‹œìŠ¤í…œ ê°œìš”

### ì„œë¹„ìŠ¤ ëª©í‘œ

```
ì‚¬ìš©ìê°€ ì“°ë ˆê¸° ì‚¬ì§„ì„ ì°ìœ¼ë©´:
1. AI ë¹„ì „ ëª¨ë¸ì´ ì¬ì§ˆ/í˜•íƒœ/í˜¼í•© ì—¬ë¶€ ë¶„ì„
2. LLMì´ "ì–´ë–»ê²Œ, ì™œ ê·¸ë ‡ê²Œ ë²„ë ¤ì•¼ í•˜ëŠ”ì§€" ì„¤ëª…
3. ìœ„ì¹˜ ê¸°ë°˜ìœ¼ë¡œ ê°€ì¥ ê°€ê¹Œìš´ ì¬í™œìš© ìˆ˜ê±°í•¨ ì¶”ì²œ
```

### ì˜ˆìƒ ë¶€í•˜

```
ë™ì‹œ ì ‘ì†ì: 100-500ëª…
ì´ë¯¸ì§€ í¬ê¸°: 2-5MB
ì²˜ë¦¬ ì‹œê°„: 5-10ì´ˆ
ì¼ì¼ ìš”ì²­: 10,000-50,000ê±´
```

---

## ğŸ—ï¸ ì „ì²´ ì•„í‚¤í…ì²˜

```mermaid
graph TB
    subgraph Client["ğŸ“± í´ë¼ì´ì–¸íŠ¸ ê³„ì¸µ"]
        FE["Frontend<br/>(React Native/Flutter)<br/>- ì¹´ë©”ë¼ ì´¬ì˜<br/>- Progress Bar<br/>- ê²°ê³¼ í‘œì‹œ"]
    end
    
    subgraph CDN["ğŸŒ CDN & ë¡œë“œë°¸ëŸ°ì„œ"]
        CF["CloudFront<br/>- SSL Termination<br/>- Lambda@Edge<br/>- Gzip/Brotli"]
        ALB["AWS ALB<br/>- Health Check<br/>- Round Robin<br/>- Auto Scaling"]
    end
    
    subgraph Backend["âš¡ ë°±ì—”ë“œ ì„œë²„ (Stateless)"]
        API1["FastAPI #1<br/>(ECS Task)"]
        API2["FastAPI #2<br/>(ECS Task)"]
        API3["FastAPI #3<br/>(ECS Task)"]
        APIN["FastAPI #N<br/>(Auto Scale)"]
    end
    
    subgraph Data["ğŸ’¾ ë°ì´í„° ì €ì¥ì†Œ"]
        Redis["Redis Cluster<br/>(ElastiCache)<br/>- Job Queue<br/>- ì§„í–‰ë¥  ì €ì¥<br/>- ê²°ê³¼ ìºì‹±"]
        DB[("PostgreSQL RDS<br/>- ì‚¬ìš©ì ì •ë³´<br/>- ë¶„ì„ ì´ë ¥<br/>- ìœ„ì¹˜ ë°ì´í„°")]
    end
    
    subgraph Workers["ğŸ”„ ë¹„ë™ê¸° ì²˜ë¦¬"]
        Celery["Celery Workers<br/>(ECS Fargate Auto Scaling)<br/>Min: 2, Max: 20<br/><br/>ğŸ“Š íŒŒì´í”„ë¼ì¸:<br/>10% - ì´ë¯¸ì§€ ë‹¤ìš´ë¡œë“œ<br/>20% - í•´ì‹œ ê³„ì‚°<br/>30% - ìºì‹œ í™•ì¸<br/>40% - ì „ì²˜ë¦¬<br/>50-60% - AI ë¹„ì „<br/>70-80% - LLM í”¼ë“œë°±<br/>90% - ìœ„ì¹˜ ê²€ìƒ‰<br/>100% - ì €ì¥ ì™„ë£Œ"]
    end
    
    subgraph External["ğŸŒ ì™¸ë¶€ ì„œë¹„ìŠ¤"]
        S3["S3 Bucket<br/>(ì´ë¯¸ì§€ ì €ì¥)<br/>Lifecycle: 30ì¼"]
        AI["ì™¸ë¶€ AI API<br/>- Roboflow<br/>- HuggingFace<br/>- OpenAI<br/>- Claude"]
        Map["Kakao Map API<br/>- ìˆ˜ê±°í•¨ ê²€ìƒ‰<br/>- ë„¤ë¹„ê²Œì´ì…˜"]
    end
    
    FE -->|HTTPS| CF
    CF --> ALB
    ALB -->|íŠ¸ë˜í”½ ë¶„ì‚°| API1
    ALB --> API2
    ALB --> API3
    ALB --> APIN
    
    API1 --> Redis
    API2 --> Redis
    API3 --> Redis
    APIN --> Redis
    
    API1 --> DB
    API2 --> DB
    API3 --> DB
    APIN --> DB
    
    Redis -->|Bull Queue| Celery
    
    Celery --> S3
    Celery --> AI
    Celery --> Map
    Celery --> Redis
    
    style FE fill:#cce5ff,stroke:#007bff,stroke-width:4px,color:#000
    style CF fill:#ffe0b3,stroke:#fd7e14,stroke-width:3px,color:#000
    style ALB fill:#ffe0b3,stroke:#fd7e14,stroke-width:3px,color:#000
    style API1 fill:#d1f2eb,stroke:#28a745,stroke-width:2px,color:#000
    style API2 fill:#d1f2eb,stroke:#28a745,stroke-width:2px,color:#000
    style API3 fill:#d1f2eb,stroke:#28a745,stroke-width:2px,color:#000
    style APIN fill:#d1f2eb,stroke:#28a745,stroke-width:2px,color:#000
    style Redis fill:#ffd1d1,stroke:#dc3545,stroke-width:3px,color:#000
    style DB fill:#ccf5f0,stroke:#20c997,stroke-width:3px,color:#000
    style Celery fill:#e6d5ff,stroke:#8844ff,stroke-width:3px,color:#000
    style S3 fill:#fff4dd,stroke:#ffc107,stroke-width:2px,color:#000
    style AI fill:#ffe0f0,stroke:#e83e8c,stroke-width:2px,color:#000
    style Map fill:#d1f2eb,stroke:#20c997,stroke-width:2px,color:#000
```

---

## ğŸ”§ í•µì‹¬ ì»´í¬ë„ŒíŠ¸

### 1. FastAPI Backend (Stateless)

```python
# main.py
from fastapi import FastAPI, BackgroundTasks
from app.tasks import process_waste_image

app = FastAPI()

@router.post("/api/v1/waste/analyze")
async def create_analysis(background_tasks: BackgroundTasks):
    """
    ì—­í• :
    1. Job ID ìƒì„±
    2. S3 Presigned URL ë°œê¸‰ (í´ë¼ì´ì–¸íŠ¸ê°€ ì§ì ‘ ì—…ë¡œë“œ)
    3. Redisì— Job ì´ˆê¸° ìƒíƒœ ì €ì¥
    4. ì¦‰ì‹œ ì‘ë‹µ (0.1ì´ˆ ì´ë‚´)
    """
    job_id = str(uuid.uuid4())
    
    # S3 Presigned URL (5ë¶„ ìœ íš¨)
    upload_url = s3_client.generate_presigned_url(
        'put_object',
        Params={
            'Bucket': 'waste-images',
            'Key': f'{job_id}.jpg',
            'ContentType': 'image/jpeg'
        },
        ExpiresIn=300
    )
    
    # Redis ì´ˆê¸° ìƒíƒœ
    await redis.setex(
        f"job:{job_id}:progress",
        3600,
        json.dumps({"progress": 0, "status": "pending"})
    )
    
    return {
        "job_id": job_id,
        "upload_url": upload_url,
        "status_url": f"/api/v1/waste/status/{job_id}"
    }

@router.post("/api/v1/waste/upload-complete/{job_id}")
async def upload_complete(job_id: str, background_tasks: BackgroundTasks):
    """
    S3 ì—…ë¡œë“œ ì™„ë£Œ ì•Œë¦¼ â†’ Celery Task ì‹œì‘
    """
    background_tasks.add_task(process_waste_image, job_id)
    return {"status": "processing"}

@router.get("/api/v1/waste/status/{job_id}")
async def get_status(job_id: str):
    """
    ì§„í–‰ë¥  ì¡°íšŒ (Polling)
    - 0.5ì´ˆë§ˆë‹¤ í˜¸ì¶œë¨
    - ì–´ëŠ FastAPI ì¸ìŠ¤í„´ìŠ¤ë¡œ ì™€ë„ ë™ì¼í•œ ì‘ë‹µ
    """
    progress_data = await redis.get(f"job:{job_id}:progress")
    
    if not progress_data:
        raise HTTPException(404, "Job not found")
    
    data = json.loads(progress_data)
    
    return {
        "job_id": job_id,
        "progress": data["progress"],
        "message": data["message"],
        "result": data.get("result") if data["progress"] == 100 else None
    }
```

### 2. Celery Worker (ë¹„ë™ê¸° ì²˜ë¦¬)

```python
# tasks.py
from celery import Celery
import imagehash
from PIL import Image

celery_app = Celery('waste_processor', broker='redis://redis:6379/0')

@celery_app.task(bind=True)
def process_waste_image(self, job_id: str):
    """
    ì´ë¯¸ì§€ ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸
    ê° ë‹¨ê³„ë§ˆë‹¤ Redisì— ì§„í–‰ë¥  ì—…ë°ì´íŠ¸
    """
    try:
        # 10% - S3ì—ì„œ ì´ë¯¸ì§€ ë‹¤ìš´ë¡œë“œ
        update_progress(job_id, 10, "ì´ë¯¸ì§€ ë‹¤ìš´ë¡œë“œ ì¤‘...")
        image_path = download_from_s3(f"{job_id}.jpg")
        
        # 20% - ì´ë¯¸ì§€ í•´ì‹œ ê³„ì‚° (ì¤‘ë³µ ê°ì§€)
        update_progress(job_id, 20, "ìºì‹œ í™•ì¸ ì¤‘...")
        img_hash = calculate_image_hash(image_path)
        
        # ìºì‹œ í™•ì¸ (ë™ì¼ ì´ë¯¸ì§€ëŠ” AI í˜¸ì¶œ ìŠ¤í‚µ)
        cached_result = redis.get(f"cache:hash:{img_hash}")
        if cached_result:
            update_progress(job_id, 100, "ìºì‹œì—ì„œ ê²°ê³¼ ë¡œë“œ!")
            save_result(job_id, json.loads(cached_result))
            return
        
        # 30% - ì „ì²˜ë¦¬
        update_progress(job_id, 30, "ì´ë¯¸ì§€ ì „ì²˜ë¦¬ ì¤‘...")
        processed_image = preprocess_image(image_path)
        
        # 50% - AI ë¹„ì „ í˜¸ì¶œ
        update_progress(job_id, 50, "AI ë¹„ì „ ë¶„ì„ ì¤‘...")
        vision_result = call_roboflow_api(processed_image)
        
        # 70% - LLM í”¼ë“œë°±
        update_progress(job_id, 70, "AI í”¼ë“œë°± ìƒì„± ì¤‘...")
        feedback = generate_llm_feedback(vision_result)
        
        # 90% - ìœ„ì¹˜ ê²€ìƒ‰
        update_progress(job_id, 90, "ê·¼ì²˜ ìˆ˜ê±°í•¨ ê²€ìƒ‰ ì¤‘...")
        nearby_bins = find_nearby_bins(vision_result, user_location)
        
        # ê²°ê³¼ ì €ì¥ ë° ìºì‹±
        final_result = {
            "waste_type": vision_result["class"],
            "confidence": vision_result["confidence"],
            "feedback": feedback,
            "nearby_bins": nearby_bins
        }
        
        # 7ì¼ê°„ ìºì‹± (ê°™ì€ ì´ë¯¸ì§€ëŠ” ì¬ì‚¬ìš©)
        redis.setex(f"cache:hash:{img_hash}", 86400 * 7, json.dumps(final_result))
        
        # 100% - ì™„ë£Œ
        update_progress(job_id, 100, "ë¶„ì„ ì™„ë£Œ!")
        save_result(job_id, final_result)
        
    except Exception as e:
        update_progress(job_id, -1, f"ì˜¤ë¥˜: {str(e)}")
        raise

def update_progress(job_id: str, progress: int, message: str):
    """Redisì— ì§„í–‰ë¥  ì €ì¥"""
    redis.setex(
        f"job:{job_id}:progress",
        3600,
        json.dumps({
            "progress": progress,
            "message": message,
            "updated_at": datetime.utcnow().isoformat()
        })
    )

def calculate_image_hash(image_path: str) -> str:
    """Perceptual Hash (ìœ ì‚¬ ì´ë¯¸ì§€ ê°ì§€)"""
    image = Image.open(image_path)
    return str(imagehash.phash(image, hash_size=16))
```

### 3. Redis (ìƒíƒœ ê´€ë¦¬ & ìºì‹±)

```
Redis ë°ì´í„° êµ¬ì¡°:

# 1. Job ì§„í–‰ë¥ 
Key: job:{job_id}:progress
Value: {
  "progress": 50,
  "message": "AI ë¶„ì„ ì¤‘...",
  "updated_at": "2025-10-30T10:30:45"
}
TTL: 3600ì´ˆ (1ì‹œê°„)

# 2. Job ê²°ê³¼
Key: job:{job_id}:result
Value: {
  "waste_type": "í”Œë¼ìŠ¤í‹± - PET",
  "confidence": 0.95,
  "feedback": "ê¹¨ë—ì´ ì„¸ì²™ í›„ ë¼ë²¨ ì œê±°",
  "nearby_bins": [...]
}
TTL: 86400ì´ˆ (24ì‹œê°„)

# 3. ì´ë¯¸ì§€ í•´ì‹œ ìºì‹± (ì¤‘ë³µ ë°©ì§€)
Key: cache:hash:{hash}
Value: {ì™„ì „í•œ ë¶„ì„ ê²°ê³¼}
TTL: 604800ì´ˆ (7ì¼)

ìºì‹œ íˆíŠ¸ìœ¨ ëª©í‘œ: 70% ì´ìƒ
â†’ AI API ë¹„ìš© 70% ì ˆê°!
```

---

## ğŸ”„ ë°ì´í„° íë¦„

### ì „ì²´ ì‹œí€€ìŠ¤

```mermaid
sequenceDiagram
    actor User as ğŸ‘¤ ì‚¬ìš©ì
    participant FE as Frontend
    participant BE as Backend
    participant S3 as S3 Bucket
    participant Redis as Redis
    participant CW as Celery Worker
    participant AI as AI API
    
    User->>FE: [1] ì“°ë ˆê¸° ì‚¬ì§„ ì´¬ì˜
    FE->>BE: [2] POST /api/v1/waste/analyze
    BE->>BE: [3] Job ID ìƒì„±
    BE->>FE: S3 Presigned URL ë°˜í™˜ (0.1ì´ˆ)
    
    FE->>S3: [4] ì´ë¯¸ì§€ ì§ì ‘ ì—…ë¡œë“œ (2ì´ˆ)
    FE->>BE: [5] POST /waste/upload-complete/{job_id}
    BE->>Redis: [6] Celery Task íì— ì¶”ê°€
    
    Note over FE,BE: [7] Frontend í´ë§ ì‹œì‘ (0.5ì´ˆë§ˆë‹¤)
    
    activate CW
    Redis->>CW: [8] Task ìˆ˜ì‹ 
    CW->>Redis: 10% - ì´ë¯¸ì§€ ë‹¤ìš´ë¡œë“œ
    FE->>BE: GET /status
    BE->>Redis: ì§„í–‰ë¥  ì¡°íšŒ
    Redis-->>BE: 10%
    BE-->>FE: progress: 10%
    
    CW->>Redis: 20% - í•´ì‹œ ê³„ì‚° & ìºì‹œ í™•ì¸
    CW->>Redis: 30% - ì „ì²˜ë¦¬
    
    FE->>BE: GET /status
    BE->>Redis: ì§„í–‰ë¥  ì¡°íšŒ
    Redis-->>BE: 30%
    BE-->>FE: progress: 30%
    
    CW->>AI: 50% - AI ë¹„ì „ ë¶„ì„
    AI-->>CW: ë¶„ì„ ê²°ê³¼
    CW->>Redis: ì§„í–‰ë¥  ì—…ë°ì´íŠ¸
    
    CW->>AI: 70% - LLM í”¼ë“œë°±
    AI-->>CW: í”¼ë“œë°± ìƒì„±
    
    FE->>BE: GET /status
    BE->>Redis: ì§„í–‰ë¥  ì¡°íšŒ
    Redis-->>BE: 70%
    BE-->>FE: progress: 70%
    
    CW->>AI: 90% - ìœ„ì¹˜ ê²€ìƒ‰
    AI-->>CW: ìˆ˜ê±°í•¨ ëª©ë¡
    CW->>Redis: 100% - ê²°ê³¼ ì €ì¥ & ìºì‹±
    deactivate CW
    
    FE->>BE: [9] GET /status
    BE->>Redis: ì§„í–‰ë¥  ì¡°íšŒ
    Redis-->>BE: 100% + result
    BE-->>FE: progress: 100%, result: {...}
    
    FE->>User: [10] ê²°ê³¼ í‘œì‹œ
```

---

## âš¡ ìµœì í™” ì „ëµ

### 1. ì´ë¯¸ì§€ í•´ì‹œ ê¸°ë°˜ ìºì‹± (í•µì‹¬!)

```python
def calculate_image_hash(image_path: str) -> str:
    """
    Perceptual Hash (pHash):
    - ë™ì¼í•œ ì´ë¯¸ì§€ â†’ ë™ì¼í•œ í•´ì‹œ
    - ì•½ê°„ì˜ ë³€í˜• (íšŒì „, í¬ê¸°) â†’ ë™ì¼í•œ í•´ì‹œ
    - ì™„ì „íˆ ë‹¤ë¥¸ ì´ë¯¸ì§€ â†’ ë‹¤ë¥¸ í•´ì‹œ
    """
    image = Image.open(image_path)
    img_hash = imagehash.phash(image, hash_size=16)
    return str(img_hash)

# ì‚¬ìš© ì˜ˆì‹œ
hash1 = calculate_hash("ì½œë¼ìº”_ì •ë©´.jpg")    # "a1b2c3d4e5f6"
hash2 = calculate_hash("ì½œë¼ìº”_ì¸¡ë©´.jpg")    # "a1b2c3d4e5f6" (ê±°ì˜ ë™ì¼)
hash3 = calculate_hash("ì‚¬ì´ë‹¤ìº”.jpg")       # "z9y8x7w6v5u4" (ë‹¤ë¦„)

# ìºì‹œ íˆíŠ¸ ì˜ˆìƒ
cached = redis.get(f"cache:hash:{hash1}")
if cached:
    # AI í˜¸ì¶œ ìŠ¤í‚µ! ë¹„ìš© ì ˆê°!
    return json.loads(cached)
```

**íš¨ê³¼:**
- âœ… AI API ë¹„ìš© 70% ì ˆê°
- âœ… ì‘ë‹µ ì†ë„ 10ë°° í–¥ìƒ (0.5ì´ˆ)
- âœ… ì„œë²„ ë¶€í•˜ ê°ì†Œ

### 2. Celery Worker Auto Scaling

```yaml
# ECS Task Definition
AutoScalingTarget:
  MinCapacity: 2
  MaxCapacity: 20
  
ScalingPolicy:
  TargetTrackingScaling:
    TargetValue: 70  # CPU 70% ìœ ì§€
    ScaleInCooldown: 60
    ScaleOutCooldown: 30

# ì‹œë‚˜ë¦¬ì˜¤
ì‹œê°„ëŒ€ë³„ ìë™ ì¡°ì ˆ:
- ì˜¤ì „ 6ì‹œ: Worker 2ê°œ
- ì˜¤í›„ 12ì‹œ (í”¼í¬): Worker 15ê°œ
- ì˜¤í›„ 9ì‹œ: Worker 8ê°œ
- ìì •: Worker 2ê°œ
```

### 3. CloudFront CDN í™œìš©

```yaml
# Lambda@Edge ì´ë¯¸ì§€ ìµœì í™”
Event: viewer-request
Function:
  - ì´ë¯¸ì§€ ë¦¬ì‚¬ì´ì§• (ìµœëŒ€ 1024x1024)
  - WebP ë³€í™˜ (ìš©ëŸ‰ 30% ê°ì†Œ)
  - ì••ì¶• (Brotli > Gzip)

íš¨ê³¼:
- ì—…ë¡œë“œ ì‹œê°„ 50% ë‹¨ì¶•
- ëŒ€ì—­í­ ë¹„ìš© ì ˆê°
- ëª¨ë°”ì¼ ë„¤íŠ¸ì›Œí¬ ì¹œí™”ì 
```

---

## ğŸ“ˆ í™•ì¥ì„± ê³ ë ¤ì‚¬í•­

### ìˆ˜í‰ í™•ì¥ (Horizontal Scaling)

```
í˜„ì¬ êµ¬ì¡°ëŠ” ì™„ì „í•œ Stateless ì„¤ê³„:
- FastAPI: ë¬´ì œí•œ í™•ì¥ ê°€ëŠ¥
- Celery Worker: ë¬´ì œí•œ í™•ì¥ ê°€ëŠ¥
- Redis: Cluster Modeë¡œ í™•ì¥
- PostgreSQL: Read Replica ì¶”ê°€

1,000ëª… ë™ì‹œ ì ‘ì† ëŒ€ì‘:
- FastAPI: 10+ instances
- Celery Worker: 50+ instances
- Redis: 6-node cluster
- RDS: Multi-AZ + Read Replica 3ê°œ
```

### ë¹„ìš© ìµœì í™”

```
í˜„ì¬ ì„¤ì • (ì›” 1ë§Œ ìš”ì²­):
- ECS Fargate: $50
- ElastiCache Redis: $15
- RDS PostgreSQL: $30
- S3 + CloudFront: $10
- AI API (ìºì‹± 70%): $15
í•©ê³„: ~$120/ì›”

í™•ì¥ ì‹œ (ì›” 10ë§Œ ìš”ì²­):
- ECS Fargate (Auto Scaling): $200
- ElastiCache Redis: $50
- RDS PostgreSQL: $100
- S3 + CloudFront: $30
- AI API (ìºì‹± 70%): $150
í•©ê³„: ~$530/ì›”
```

---

## ğŸ“š ê´€ë ¨ ë¬¸ì„œ

- [Polling vs WebSocket ë¹„êµ](polling-vs-websocket.md)
- [ë°°í¬ ê°€ì´ë“œ](../deployment/full-guide.md)
- [Docker ì‚¬ìš©ë²•](../deployment/docker.md)

---

**ì‘ì„±ì¼**: 2025-10-30  
**ë²„ì „**: 1.0  
**ìƒíƒœ**: âœ… ìŠ¹ì¸ë¨

