# ğŸ—ï¸ ì´ë¯¸ì§€ ì²˜ë¦¬ ì•„í‚¤í…ì²˜

> **4-Tier ê¸°ë°˜ AI ì“°ë ˆê¸° ë¶„ë¥˜ íŒŒì´í”„ë¼ì¸**  
> **RabbitMQ (Task Queue) + Redis (Cache & State)**  
> **ë‚ ì§œ**: 2025-10-31  
> **ìƒíƒœ**: âœ… í”„ë¡œë•ì…˜ ì¤€ë¹„

## ğŸ“‹ ëª©ì°¨

1. [ì‹œìŠ¤í…œ ê°œìš”](#ì‹œìŠ¤í…œ-ê°œìš”)
2. [4-Tier ì•„í‚¤í…ì²˜](#4-tier-ì•„í‚¤í…ì²˜)
3. [Celery ì´ì¤‘ ì—°ê²°](#celery-ì´ì¤‘-ì—°ê²°)
4. [ë°ì´í„° íë¦„](#ë°ì´í„°-íë¦„)
5. [Redis ìºì‹± ì „ëµ](#redis-ìºì‹±-ì „ëµ)
6. [ìµœì í™”](#ìµœì í™”)

---

## ğŸ¯ ì‹œìŠ¤í…œ ê°œìš”

### ì„œë¹„ìŠ¤ ëª©í‘œ

```
ì‚¬ìš©ìê°€ ì“°ë ˆê¸° ì‚¬ì§„ì„ ì°ìœ¼ë©´:
1. GPT-4o Visionì´ ì¬ì§ˆ/í˜•íƒœ/í˜¼í•© ì—¬ë¶€ ë¶„ì„
2. LLMì´ "ì–´ë–»ê²Œ, ì™œ ê·¸ë ‡ê²Œ ë²„ë ¤ì•¼ í•˜ëŠ”ì§€" ì„¤ëª…
3. ìœ„ì¹˜ ê¸°ë°˜ìœ¼ë¡œ ê°€ì¥ ê°€ê¹Œìš´ ì¬í™œìš© ìˆ˜ê±°í•¨ ì¶”ì²œ

í•µì‹¬:
âœ… ì‘ë‹µ ì‹œê°„: 3-5ì´ˆ (ìºì‹œ íˆíŠ¸ ì‹œ 1ì´ˆ)
âœ… AI ë¹„ìš© ì ˆê°: 70% (Image Hash Cache)
âœ… ë™ì‹œ ì²˜ë¦¬: 100-500ëª…
```

---

## ğŸ—ï¸ 4-Tier ì•„í‚¤í…ì²˜

### Image Processing in 4-Tier

```mermaid
graph TB
    subgraph Client["Client Layer"]
        User["ì‚¬ìš©ì
Mobile App"]
    end
    
    subgraph AWS["AWS Services"]
        Route53["Route53"]
        ALB["ALB
L7 Routing"]
        S3["S3
Pre-signed URL
Image Storage"]
    end
    
    subgraph Tier1["Tier 1: Control Plane"]
        CP["Master
Orchestration"]
    end
    
    subgraph Tier2["Tier 2: Data Plane"]
        API["waste-service x2
FastAPI Sync API
 
1. Job ID ìƒì„±
2. S3 URL ë°œê¸‰
3. Task ë°œí–‰"]
        
        Worker["AI Workers x3
Celery Async
 
1. ì´ë¯¸ì§€ ë‹¤ìš´ë¡œë“œ
2. Hash ê³„ì‚°
3. GPT-4o Vision
4. ê²°ê³¼ ì €ì¥"]
    end
    
    subgraph Tier3["Tier 3: Message Queue"]
        RMQ["RabbitMQ HA x3
 
q.ai Queue
 
Task ì „ë‹¬
ì¼íšŒì„± ë©”ì‹œì§€"]
    end
    
    subgraph Tier4["Tier 4: Persistence"]
        Redis["Redis
 
DB 1: Image Hash Cache
DB 2: Job Progress
 
ë°˜ë³µ ì¡°íšŒ ê°€ëŠ¥"]
        
        DB["PostgreSQL
 
Final Results
User History"]
    end
    
    subgraph External["External"]
        OpenAI["OpenAI
GPT-4o Vision"]
    end
    
    User --> Route53
    Route53 --> ALB
    ALB --> API
    
    User --> S3
    
    API -->|"1. publish task"| RMQ
    RMQ -->|"2. consume task"| Worker
    Worker -->|"3. ACK (ì‚­ì œ)"| RMQ
    
    Worker -->|"4. update progress
0.5ì´ˆë§ˆë‹¤ ë®ì–´ì“°ê¸°"| Redis
    Worker -->|"5. check cache
ë°˜ë³µ ì¡°íšŒ"| Redis
    
    API -->|"6. get progress
0.5ì´ˆë§ˆë‹¤ ì¡°íšŒ"| Redis
    
    Worker --> S3
    Worker --> OpenAI
    Worker --> DB
    
    style Client fill:#0d47a1,color:#fff,stroke:#01579b,stroke-width:3px
    style AWS fill:#e65100,color:#fff,stroke:#bf360c,stroke-width:3px
    style Tier1 fill:#1565c0,color:#fff,stroke:#0d47a1,stroke-width:4px
    style Tier2 fill:#2e7d32,color:#fff,stroke:#1b5e20,stroke-width:4px
    style Tier3 fill:#f57c00,color:#fff,stroke:#e65100,stroke-width:4px
    style Tier4 fill:#c2185b,color:#fff,stroke:#880e4f,stroke-width:4px
    style API fill:#81c784,color:#000,stroke:#66bb6a,stroke-width:2px
    style Worker fill:#a5d6a7,color:#000,stroke:#81c784,stroke-width:2px
    style RMQ fill:#ffb74d,color:#000,stroke:#ffa726,stroke-width:2px
    style Redis fill:#f48fb1,color:#000,stroke:#ec407a,stroke-width:2px
    style DB fill:#f8bbd0,color:#000,stroke:#f48fb1,stroke-width:2px
```

---

## ğŸ”„ Celery ì´ì¤‘ ì—°ê²° êµ¬ì¡°

### RabbitMQ + Redis ë™ì‹œ ì‚¬ìš©

```python
from celery import Celery

# CeleryëŠ” ë‘ ê°œì˜ ë…ë¦½ì ì¸ ì—°ê²° ìœ ì§€
app = Celery('waste_processor',
    # Tier 3: Message Queue (Task ì „ë‹¬)
    broker='amqp://admin:password@rabbitmq.messaging:5672//',
    
    # Tier 4: Storage (ê²°ê³¼ ë° ìƒíƒœ ì €ì¥)
    result_backend='redis://redis.default:6379/0'
)

# ì—­í•  ë¶„ë¦¬:
# RabbitMQ (broker): Taskë¥¼ Producer â†’ Consumer ì „ë‹¬
# Redis (result_backend): Task ê²°ê³¼ ë° ì§„í–‰ë¥  ì €ì¥
```

### Tier 3: RabbitMQ (Message Queue)

```
ì—­í• : Task ì „ë‹¬ (ì¼íšŒì„± ë©”ì‹œì§€)

íŠ¹ì„±:
âœ… Producerê°€ Task ë°œí–‰
âœ… Queueì— ë©”ì‹œì§€ ì €ì¥
âœ… Consumerê°€ consume
âœ… ACK í›„ ë©”ì‹œì§€ ì‚­ì œ
âœ… í•œ ë²ˆë§Œ ì „ë‹¬ (Exactly Once)

ì‚¬ìš©:
â””â”€ Task ì „ë‹¬ìš©
   - API â†’ Workerë¡œ ì‘ì—… ìš”ì²­
   - Priority, Routing ì§€ì›
   - Delivery Guarantee
```

### Tier 4: Redis (Persistence - Cache & State)

```
ì—­í• : ìƒíƒœ ì €ì¥ (ë°˜ë³µ ì¡°íšŒ ê°€ëŠ¥)

íŠ¹ì„±:
âœ… Key-Value Store
âœ… Random Access (íŠ¹ì • key ì§ì ‘ ì¡°íšŒ)
âœ… Overwrite ê°€ëŠ¥ (ìµœì‹  ê°’ìœ¼ë¡œ ì—…ë°ì´íŠ¸)
âœ… ì—¬ëŸ¬ ë²ˆ ì½ì–´ë„ ë°ì´í„° ìœ ì§€
âœ… TTL ìë™ ë§Œë£Œ

ì‚¬ìš©:
â”œâ”€ DB 0: Celery Result Backend
â”œâ”€ DB 1: Image Hash Cache â­â­â­â­â­
â”œâ”€ DB 2: Job Progress Tracking â­â­â­â­
â””â”€ DB 3: Session Store
```

---

## ğŸ“Š ë°ì´í„° íë¦„

### ì „ì²´ ì‹œí€€ìŠ¤ (4-Tier)

```mermaid
sequenceDiagram
    actor User
    participant App as Mobile App
    participant API as Tier 2 Sync<br/>waste-service
    participant MQ as Tier 3 MQ<br/>RabbitMQ q.ai
    participant Worker as Tier 2 Async<br/>AI Worker
    participant Redis as Tier 4 Cache<br/>Redis DB1,DB2
    participant DB as Tier 4 DB<br/>PostgreSQL
    participant S3
    participant AI as OpenAI API
    
    User->>App: [1] ì“°ë ˆê¸° ì‚¬ì§„ ì´¬ì˜
    
    App->>API: [2] POST /api/v1/waste/analyze
    API->>API: Job ID ìƒì„±
    API->>App: S3 Pre-signed URL
    
    App->>S3: [3] ì´ë¯¸ì§€ ì§ì ‘ ì—…ë¡œë“œ
    
    App->>API: [4] POST /upload-complete/{job_id}
    
    Note over API: Tier 2 Data Plane<br/>ë¹„ì¦ˆë‹ˆìŠ¤ ë¡œì§
    API->>MQ: [5] Publish q.ai<br/>ai.analyze
    Note over MQ: Tier 3 Middleware<br/>ë©”ì‹œì§€ ì „ë‹¬ (ì¼íšŒì„±)
    
    MQ->>Worker: [6] Consume task
    Worker->>MQ: ACK (ë©”ì‹œì§€ ì‚­ì œ)
    
    activate Worker
    Note over Worker: Tier 2 Data Plane<br/>ë¹„ë™ê¸° ì²˜ë¦¬
    
    Worker->>Redis: [7] 10% ì´ë¯¸ì§€ ë‹¤ìš´ë¡œë“œ ì¤‘
    Note over Redis: Tier 4 Persistence<br/>DB 2: Progress
    
    Worker->>S3: ì´ë¯¸ì§€ ë‹¤ìš´ë¡œë“œ
    Worker->>Worker: Hash ê³„ì‚° (pHash)
    
    Worker->>Redis: [8] ìºì‹œ í™•ì¸<br/>cache:hash:{phash}
    Note over Redis: Tier 4 Persistence<br/>DB 1: Image Cache
    
    alt Cache Hit 70%
        Redis-->>Worker: ìºì‹œëœ ê²°ê³¼
        Worker->>Redis: 100% ì™„ë£Œ
        Worker-->>App: ì¦‰ì‹œ ì‘ë‹µ 1ì´ˆ
    else Cache Miss 30%
        Worker->>Redis: 30% ì „ì²˜ë¦¬ ì¤‘
        Worker->>Redis: 50% AI ë¶„ì„ ì¤‘
        
        Worker->>AI: GPT-4o Vision API
        AI-->>Worker: ë¶„ë¥˜ ê²°ê³¼
        
        Worker->>Redis: 70% í”¼ë“œë°± ìƒì„± ì¤‘
        Worker->>Redis: 90% ìœ„ì¹˜ ê²€ìƒ‰ ì¤‘
        
        Worker->>DB: ìµœì¢… ê²°ê³¼ ì €ì¥
        Note over DB: Tier 4 Persistence<br/>ì˜êµ¬ ì €ì¥
        
        Worker->>Redis: ìºì‹± 7ì¼<br/>cache:hash:{phash}
        Worker->>Redis: 100% ì™„ë£Œ
    end
    deactivate Worker
    
    loop Polling 0.5ì´ˆë§ˆë‹¤
        App->>API: [9] GET /status/{job_id}
        API->>Redis: ì§„í–‰ë¥  ì¡°íšŒ (ë°˜ë³µ)
        Note over Redis: Tier 4<br/>ê°™ì€ Key ë°˜ë³µ ì¡°íšŒ ê°€ëŠ¥!
        Redis-->>App: progress: 80%
    end
    
    App->>API: [10] GET /result/{job_id}
    API->>Redis: ìµœì¢… ê²°ê³¼ ì¡°íšŒ
    Redis-->>App: result
    
    App->>User: ê²°ê³¼ í‘œì‹œ
```

---

## ğŸ° RabbitMQ ì—­í•  (Tier 3)

### Task ì „ë‹¬ë§Œ!

```python
# Producer (waste-service, Tier 2)
from celery import current_app

@app.post("/upload-complete/{job_id}")
async def upload_complete(job_id: str):
    # RabbitMQì— Task ë°œí–‰
    current_app.send_task(
        'tasks.analyze_image',
        args=[job_id],
        queue='q.ai',
        routing_key='ai.analyze',
        priority=10
    )
    # â†’ RabbitMQ q.aiì— ë©”ì‹œì§€ ì¶”ê°€
    # â†’ Workerê°€ consumeí•  ë•Œê¹Œì§€ ëŒ€ê¸°
    
    return {"status": "queued"}

# Consumer (AI Worker, Tier 2)
@celery_app.task(bind=True, queue='q.ai')
def analyze_image(self, job_id):
    # RabbitMQì—ì„œ ë©”ì‹œì§€ ë°›ìŒ
    # (ì—¬ê¸°ì„œ ë©”ì‹œì§€ëŠ” íì—ì„œ ì œê±°ë¨)
    
    # ì‹¤ì œ ì²˜ë¦¬...
    
    # ì™„ë£Œ í›„ RabbitMQì— ACK
    # â†’ ë©”ì‹œì§€ ì™„ì „ ì‚­ì œ
    return result

# RabbitMQ ì—­í• :
# âœ… Producer â†’ Consumer ë©”ì‹œì§€ ì „ë‹¬
# âœ… í•œ ë²ˆ ì „ë‹¬í•˜ë©´ ë
# âŒ ì§„í–‰ë¥  ì €ì¥ ëª» í•¨ (ë©”ì‹œì§€ ì‚­ì œë˜ë‹ˆê¹Œ)
```

---

## ğŸ’¾ Redis ì—­í•  (Tier 4)

### 1. Image Hash Cache (DB 1) â­â­â­â­â­

**ê°€ì¥ ì¤‘ìš”! AI ë¹„ìš© 70% ì ˆê°**

```python
import imagehash
from PIL import Image
import redis

redis_cache = redis.Redis(host='redis.default', port=6379, db=1)

@celery_app.task
def analyze_image(job_id):
    # 1. ì´ë¯¸ì§€ ë‹¤ìš´ë¡œë“œ
    image_path = download_from_s3(f"{job_id}.jpg")
    
    # 2. Perceptual Hash ê³„ì‚°
    img = Image.open(image_path)
    phash = str(imagehash.phash(img, hash_size=16))
    
    # 3. ìºì‹œ í™•ì¸ (Redis DB 1)
    cache_key = f"cache:image:hash:{phash}"
    cached = redis_cache.get(cache_key)
    
    if cached:
        # ìºì‹œ íˆíŠ¸! AI API í˜¸ì¶œ ìŠ¤í‚µ!
        print("âœ… ìºì‹œ íˆíŠ¸! AI ë¹„ìš© ì ˆê°!")
        return json.loads(cached)
    
    # 4. ìºì‹œ ë¯¸ìŠ¤ â†’ AI ë¶„ì„
    result = await analyze_with_gpt4o_vision(image_path)
    
    # 5. ê²°ê³¼ ìºì‹± (7ì¼)
    redis_cache.setex(
        cache_key,
        86400 * 7,  # 7ì¼
        json.dumps(result)
    )
    
    return result

# íš¨ê³¼:
# - ê°™ì€ ì“°ë ˆê¸° ì‚¬ì§„ (ì½œë¼ìº”, ìš°ìœ íŒ© ë“±)
# - 10,000 ìš”ì²­ ì¤‘ 7,000 ìºì‹œ íˆíŠ¸
# - AI API í˜¸ì¶œ: 3,000íšŒë§Œ (70% ì ˆê°!)
# - ë¹„ìš© ì ˆê°: $100/ì›” ì´ìƒ
```

### 2. Job Progress Tracking (DB 2) â­â­â­â­

**0.5ì´ˆë§ˆë‹¤ ë°˜ë³µ ì¡°íšŒ**

```python
redis_progress = redis.Redis(host='redis.default', port=6379, db=2)

# Worker (ì§„í–‰ë¥  ì—…ë°ì´íŠ¸)
def analyze_image(job_id):
    # 10% - ë‹¤ìš´ë¡œë“œ
    update_progress(job_id, 10, "ì´ë¯¸ì§€ ë‹¤ìš´ë¡œë“œ ì¤‘...")
    download_image()
    
    # 30% - í•´ì‹œ ê³„ì‚°
    update_progress(job_id, 30, "ìºì‹œ í™•ì¸ ì¤‘...")
    calculate_hash()
    
    # 50% - AI ë¶„ì„
    update_progress(job_id, 50, "AI ë¶„ì„ ì¤‘...")
    analyze_with_ai()
    
    # 100% - ì™„ë£Œ
    update_progress(job_id, 100, "ì™„ë£Œ!")

def update_progress(job_id, progress, message):
    # Redisì— ì§„í–‰ë¥  ì €ì¥ (Overwrite)
    redis_progress.setex(
        f"job:{job_id}:progress",
        3600,  # 1ì‹œê°„ TTL
        json.dumps({
            "progress": progress,
            "message": message,
            "updated_at": datetime.now().isoformat()
        })
    )

# API (ì§„í–‰ë¥  ì¡°íšŒ)
@app.get("/status/{job_id}")
async def get_status(job_id: str):
    # Redisì—ì„œ ì¡°íšŒ (0.5ì´ˆë§ˆë‹¤ ë°˜ë³µ!)
    progress = await redis_progress.get(f"job:{job_id}:progress")
    
    # âœ… ê°™ì€ Keyë¥¼ ë¬´í•œ ë°˜ë³µ ì¡°íšŒ ê°€ëŠ¥
    # âœ… ë©”ì‹œì§€ ì‚­ì œ ì•ˆ ë¨
    # âœ… ì—¬ëŸ¬ API ì„œë²„ì—ì„œ ë™ì‹œ ì¡°íšŒ
    
    return json.loads(progress)

# RabbitMQë¡œëŠ” ë¶ˆê°€ëŠ¥í•œ ì´ìœ :
# âŒ consumeí•˜ë©´ ë©”ì‹œì§€ ì‚­ì œë¨
# âŒ 0.5ì´ˆë§ˆë‹¤ ìƒˆ ë©”ì‹œì§€ ë°œí–‰? (ë¹„íš¨ìœ¨)
# âŒ Random access ë¶ˆê°€
```

### 3. Celery Result Backend (DB 0) â­â­â­

**Celery í‘œì¤€**

```python
# Celery ì„¤ì •
result_backend = 'redis://redis.default:6379/0'

# Worker (ìë™ ì €ì¥)
@app.task
def analyze_image(job_id):
    return {"waste_type": "PET", "confidence": 0.95}
    # Celeryê°€ ìë™ìœ¼ë¡œ Redis DB 0ì— ì €ì¥
    # celery-task-meta-{task_id} = {...}

# API (ê²°ê³¼ ì¡°íšŒ)
task = analyze_image.apply_async(args=[job_id])
result = task.get(timeout=10)  # Redisì—ì„œ ì¡°íšŒ
# task.state  â†’ 'SUCCESS'
# task.result â†’ {"waste_type": "PET", ...}
```

### 4. Session Store (DB 3) â­â­

```python
redis_session = redis.Redis(host='redis.default', port=6379, db=3)

# Refresh Token ì €ì¥
redis_session.setex(
    f"session:{user_id}:refresh",
    86400 * 30,  # 30ì¼
    refresh_token
)

# OAuth State (CSRF ë°©ì§€)
redis_session.setex(
    f"oauth:state:{state}",
    600,  # 10ë¶„
    json.dumps(user_data)
)
```

---

## ğŸ”„ ì™„ì „í•œ ë°ì´í„° íë¦„

### ì½”ë“œ ì˜ˆì‹œ

```python
# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
# Tier 2: FastAPI (waste-service)
# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

@app.post("/api/v1/waste/analyze")
async def create_analysis():
    job_id = str(uuid.uuid4())
    
    # S3 Pre-signed URL
    upload_url = s3.generate_presigned_url(
        'put_object',
        Params={'Bucket': 'images', 'Key': f'{job_id}.jpg'},
        ExpiresIn=300
    )
    
    # Redis DB 2: ì´ˆê¸° ì§„í–‰ë¥ 
    await redis_progress.setex(
        f"job:{job_id}:progress",
        3600,
        json.dumps({"progress": 0, "status": "pending"})
    )
    
    return {"job_id": job_id, "upload_url": upload_url}

@app.post("/upload-complete/{job_id}")
async def upload_complete(job_id: str):
    # RabbitMQ (Tier 3)ì— Task ë°œí–‰
    celery_app.send_task(
        'tasks.analyze_image',
        args=[job_id],
        queue='q.ai',
        routing_key='ai.analyze'
    )
    # â†’ RabbitMQ q.aiì— ë©”ì‹œì§€ ì¶”ê°€
    
    return {"status": "processing"}

@app.get("/status/{job_id}")
async def get_status(job_id: str):
    # Redis DB 2ì—ì„œ ì§„í–‰ë¥  ì¡°íšŒ (0.5ì´ˆë§ˆë‹¤ ë°˜ë³µ)
    progress = await redis_progress.get(f"job:{job_id}:progress")
    return json.loads(progress)

# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
# Tier 2: Celery Worker (AI Worker)
# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

celery_app = Celery(
    broker='amqp://admin:password@rabbitmq.messaging:5672//',  # Tier 3
    result_backend='redis://redis.default:6379/0'  # Tier 4
)

@celery_app.task(bind=True, queue='q.ai')
def analyze_image(self, job_id):
    # [RabbitMQì—ì„œ ë©”ì‹œì§€ ë°›ìŒ â†’ ACK â†’ ë©”ì‹œì§€ ì‚­ì œ]
    
    # 10% - ë‹¤ìš´ë¡œë“œ
    update_progress(job_id, 10, "ë‹¤ìš´ë¡œë“œ ì¤‘...")
    image_path = download_from_s3(f"{job_id}.jpg")
    
    # 20% - Hash ê³„ì‚°
    update_progress(job_id, 20, "ìºì‹œ í™•ì¸ ì¤‘...")
    img = Image.open(image_path)
    phash = str(imagehash.phash(img, hash_size=16))
    
    # ìºì‹œ í™•ì¸ (Redis DB 1)
    cache_key = f"cache:image:hash:{phash}"
    cached = redis_cache.get(cache_key)
    
    if cached:
        update_progress(job_id, 100, "ìºì‹œ íˆíŠ¸!")
        return json.loads(cached)
    
    # 50% - AI ë¶„ì„
    update_progress(job_id, 50, "AI ë¶„ì„ ì¤‘...")
    result = gpt4o_vision_api(image_path)
    
    # 70% - í”¼ë“œë°± ìƒì„±
    update_progress(job_id, 70, "í”¼ë“œë°± ìƒì„± ì¤‘...")
    feedback = generate_feedback(result)
    
    # 90% - DB ì €ì¥
    update_progress(job_id, 90, "ì €ì¥ ì¤‘...")
    save_to_db(job_id, result)
    
    # ê²°ê³¼ ìºì‹± (Redis DB 1, 7ì¼)
    final_result = {"waste_type": result, "feedback": feedback}
    redis_cache.setex(cache_key, 86400 * 7, json.dumps(final_result))
    
    # 100% - ì™„ë£Œ
    update_progress(job_id, 100, "ì™„ë£Œ!")
    
    return final_result  # â†’ Redis DB 0 (result_backend)

def update_progress(job_id, progress, message):
    # Redis DB 2ì— ì§„í–‰ë¥  ì €ì¥ (Overwrite)
    redis_progress.setex(
        f"job:{job_id}:progress",
        3600,
        json.dumps({"progress": progress, "message": message})
    )
```

---

## ğŸ’¡ Redis ìºì‹± ì „ëµ (Tier 4)

### Image Hash Cache (í•µì‹¬!)

```python
# Perceptual Hash (pHash)
def calculate_image_hash(image_path):
    """
    ë™ì¼/ìœ ì‚¬ ì´ë¯¸ì§€ ê°ì§€:
    - ì •í™•íˆ ê°™ì€ ì‚¬ì§„ â†’ ê°™ì€ í•´ì‹œ
    - ì•½ê°„ íšŒì „/í¬ê¸° ë³€ê²½ â†’ ê°™ì€ í•´ì‹œ
    - ì™„ì „íˆ ë‹¤ë¥¸ ì‚¬ì§„ â†’ ë‹¤ë¥¸ í•´ì‹œ
    """
    img = Image.open(image_path)
    return str(imagehash.phash(img, hash_size=16))

# ì˜ˆì‹œ:
hash1 = phash("ì½œë¼ìº”_ì •ë©´.jpg")     # "a1b2c3d4e5f6g7h8"
hash2 = phash("ì½œë¼ìº”_ì¸¡ë©´.jpg")     # "a1b2c3d4e5f6g7h8" (ê±°ì˜ ë™ì¼!)
hash3 = phash("ì‚¬ì´ë‹¤ìº”.jpg")        # "z9y8x7w6v5u4t3s2" (ë‹¤ë¦„)

# ìºì‹œ ì „ëµ:
cache_key = f"cache:image:hash:{hash1}"
cached_result = redis.get(cache_key)

if cached_result:
    # ìºì‹œ íˆíŠ¸!
    # - AI API í˜¸ì¶œ ìŠ¤í‚µ
    # - ì‘ë‹µ ì‹œê°„: 1ì´ˆ
    # - ë¹„ìš©: $0
    return json.loads(cached_result)

# ìºì‹œ ë¯¸ìŠ¤
result = call_ai_api()  # 3-5ì´ˆ, ë¹„ìš© $0.01
redis.setex(cache_key, 86400 * 7, json.dumps(result))

# íš¨ê³¼:
# ì›” 10,000 ìš”ì²­ Ã— 70% ìºì‹œ íˆíŠ¸ = 7,000íšŒ ì ˆê°
# ë¹„ìš© ì ˆê°: 7,000 Ã— $0.01 = $70/ì›”
```

---

## ğŸ“Š Redis DBë³„ ë°ì´í„° êµ¬ì¡°

```python
# Redis 6ê°œ DB í™œìš©

# DB 0: Celery Result Backend (Celery ìë™ ê´€ë¦¬)
celery-task-meta-{task_id} = {
    "status": "SUCCESS",
    "result": {...},
    "traceback": null,
    "children": []
}
TTL: task_result_expires (default 24h)

# DB 1: Image Hash Cache â­â­â­â­â­
cache:image:hash:{phash} = {
    "waste_type": "PET í”Œë¼ìŠ¤í‹±",
    "confidence": 0.95,
    "feedback": "ê¹¨ë—ì´ ì„¸ì²™ í›„ ë¼ë²¨ ì œê±°...",
    "analyzed_at": "2025-10-31T10:30:00"
}
TTL: 604800ì´ˆ (7ì¼)
ì˜ˆìƒ í¬ê¸°: 1KB Ã— 10,000 = 10MB
ìºì‹œ íˆíŠ¸ìœ¨: 70%+

# DB 2: Job Progress Tracking â­â­â­â­
job:{job_id}:progress = {
    "progress": 50,
    "message": "AI ë¶„ì„ ì¤‘...",
    "stage": "ai_vision",
    "updated_at": "2025-10-31T10:30:45"
}
TTL: 3600ì´ˆ (1ì‹œê°„)
ì—…ë°ì´íŠ¸ ë¹ˆë„: 10-15íšŒ/job
ì¡°íšŒ ë¹ˆë„: 20-30íšŒ/job (0.5ì´ˆë§ˆë‹¤)

# DB 3: Session Store â­â­
session:{user_id}:refresh_token = "eyJhbGc..."
TTL: 2592000ì´ˆ (30ì¼)

oauth:state:{state} = {"user_id": 123, "provider": "kakao"}
TTL: 600ì´ˆ (10ë¶„)

# DB 4: Rate Limiting â­
ratelimit:ip:{ip}:{endpoint} = 15  # ìš”ì²­ íšŸìˆ˜
TTL: 60ì´ˆ (1ë¶„)
```

---

## ğŸ¯ ìµœì í™” íš¨ê³¼

### Image Hash Cache íš¨ê³¼

```
ì‹œë‚˜ë¦¬ì˜¤: ì›” 10,000 ì´ë¯¸ì§€ ë¶„ì„ ìš”ì²­

ìºì‹œ ì—†ì´:
â”œâ”€ AI API í˜¸ì¶œ: 10,000íšŒ
â”œâ”€ í‰ê·  ë¹„ìš©: $0.01/ìš”ì²­
â”œâ”€ ì´ ë¹„ìš©: $100/ì›”
â””â”€ í‰ê·  ì‘ë‹µ: 5ì´ˆ

Image Hash Cache (70% íˆíŠ¸):
â”œâ”€ AI API í˜¸ì¶œ: 3,000íšŒ (70% ì ˆê°!)
â”œâ”€ ìºì‹œ íˆíŠ¸: 7,000íšŒ
â”œâ”€ AI ë¹„ìš©: $30/ì›”
â”œâ”€ Redis ë¹„ìš©: ~$5/ì›”
â”œâ”€ ì´ ë¹„ìš©: $35/ì›”
â”œâ”€ ì ˆê°: $65/ì›” (65%!)
â””â”€ í‰ê·  ì‘ë‹µ: 2.2ì´ˆ (ìºì‹œ 1ì´ˆ + AI 5ì´ˆ)

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
Image Hash Cache = ê°€ì¥ ì¤‘ìš”í•œ ìµœì í™”!
```

---

## ğŸ¯ ê²°ë¡ 

### RabbitMQ vs Redis ì—­í• 

```
Tier 3: RabbitMQ (Message Queue)
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
âœ… Task ì „ë‹¬ (ì¼íšŒì„±)
âœ… Producer â†’ Consumer
âœ… Consume í›„ ë©”ì‹œì§€ ì‚­ì œ
âœ… Priority, Routing
âœ… Delivery Guarantee

ì‚¬ìš©:
â””â”€ Task Queue (ë¹„ë™ê¸° ì‘ì—… ìš”ì²­)

âŒ Progress Tracking ë¶ˆê°€
   - ë©”ì‹œì§€ ì‚­ì œë¨
   - ë°˜ë³µ ì¡°íšŒ ë¶ˆê°€
   - Random access ë¶ˆê°€

Tier 4: Redis (Persistence - Cache & State)
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
âœ… ìƒíƒœ ì €ì¥ (ë°˜ë³µ ì¡°íšŒ ê°€ëŠ¥)
âœ… Key-Value (Random access)
âœ… Overwrite ê°€ëŠ¥
âœ… ì—¬ëŸ¬ ë²ˆ ì½ê¸° ê°€ëŠ¥
âœ… TTL ìë™ ê´€ë¦¬

ì‚¬ìš©:
â”œâ”€ DB 0: Celery Result Backend
â”œâ”€ DB 1: Image Hash Cache â­â­â­â­â­ (70% ì ˆê°!)
â”œâ”€ DB 2: Job Progress Tracking â­â­â­â­
â””â”€ DB 3: Session Store

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

Celery ì—°ê²°:
âœ… broker = RabbitMQ (Task ì „ë‹¬)
âœ… result_backend = Redis (ê²°ê³¼ ë° ìƒíƒœ ì €ì¥)

ë‘ ê°œ ëª¨ë‘ í•„ìš”!
```

---

**image-processing-architecture.mdê°€ ì˜¬ë°”ë¥¸ êµ¬ì¡°ë¡œ ì¬ì‘ì„±ë˜ì—ˆìŠµë‹ˆë‹¤!** âœ…

**ê°€ì¥ ì¤‘ìš”í•œ ê²ƒ: Redis DB 1 (Image Hash Cache) - AI ë¹„ìš© 70% ì ˆê°!** ğŸ’°