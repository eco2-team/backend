# Celery ë™ì‘ ë°©ì‹ ë° ì•„í‚¤í…ì²˜

ì‘ì„± ì¼ì‹œ: 2025-11-06
ì‹œìŠ¤í…œ: Growbin Backend
ì°¸ê³ : Celery Official Docs, Robin Storage

---

## ğŸ“š ëª©ì°¨

1. [Celeryë€?](#1-celeryë€)
2. [Celery ì•„í‚¤í…ì²˜](#2-celery-ì•„í‚¤í…ì²˜)
3. [ë©”ì‹œì§€ íë¦„](#3-ë©”ì‹œì§€-íë¦„)
4. [Task ìƒëª…ì£¼ê¸°](#4-task-ìƒëª…ì£¼ê¸°)
5. [Result Backend](#5-result-backend)
6. [Celery Beat (ìŠ¤ì¼€ì¤„ëŸ¬)](#6-celery-beat-ìŠ¤ì¼€ì¤„ëŸ¬)
7. [ë™ì‹œì„± ëª¨ë¸](#7-ë™ì‹œì„±-ëª¨ë¸)
8. [ì—ëŸ¬ ì²˜ë¦¬ ë° ì¬ì‹œë„](#8-ì—ëŸ¬-ì²˜ë¦¬-ë°-ì¬ì‹œë„)
9. [ëª¨ë‹ˆí„°ë§](#9-ëª¨ë‹ˆí„°ë§)
10. [Best Practices](#10-best-practices)

---

## 1. Celeryë€?

### 1.1 ì •ì˜

```
Celery = ë¶„ì‚° Task Queue ì‹œìŠ¤í…œ
  - ë¹„ë™ê¸° ì‘ì—… ì²˜ë¦¬
  - ìŠ¤ì¼€ì¤„ë§ (ì£¼ê¸°ì  ì‘ì—…)
  - Python ê¸°ë°˜ (ë‹¤ë¥¸ ì–¸ì–´ë„ ì§€ì›)
```

### 1.2 ì£¼ìš” êµ¬ì„± ìš”ì†Œ

```mermaid
graph TB
    subgraph Components["Celery êµ¬ì„± ìš”ì†Œ"]
        Producer["1. Producer<br/>(ì‘ì—… ìƒì„±ì)<br/>API, ì›¹ ì„œë²„ ë“±"]
        Broker["2. Broker<br/>(ë©”ì‹œì§€ í)<br/>RabbitMQ, Redis, Amazon SQS"]
        Worker["3. Worker<br/>(ì‘ì—… ì‹¤í–‰ì)<br/>Celery Worker Process"]
        Backend["4. Result Backend<br/>(ê²°ê³¼ ì €ì¥ì†Œ)<br/>PostgreSQL, Redis, MongoDB"]
        Beat["5. Beat<br/>(ìŠ¤ì¼€ì¤„ëŸ¬)<br/>Celery Beat Process"]
    end
    
    style Producer fill:#cce5ff,stroke:#007bff,stroke-width:3px,color:#000
    style Broker fill:#F39C12,stroke:#C87F0A,stroke-width:3px,color:#000
    style Worker fill:#7B68EE,stroke:#4B3C8C,stroke-width:3px,color:#fff
    style Backend fill:#3498DB,stroke:#2874A6,stroke-width:3px,color:#fff
    style Beat fill:#51CF66,stroke:#2F9E44,stroke-width:3px,color:#fff
```

---

## 2. Celery ì•„í‚¤í…ì²˜

### 2.1 ì „ì²´ êµ¬ì¡°

```mermaid
graph TB
    subgraph Producer["1ï¸âƒ£ Producer (Task ë°œí–‰)"]
        API["FastAPI / Django<br/>task.delay(args)<br/>task.apply_async(...)"]
    end
    
    subgraph Broker["2ï¸âƒ£ Message Broker (ë©”ì‹œì§€ í)"]
        RMQ["RabbitMQ<br/>â€¢ Exchange (topic, direct, fanout)<br/>â€¢ Queue (durable, persistent)<br/>â€¢ Routing Key"]
        Redis["ë˜ëŠ” Redis<br/>â€¢ List ê¸°ë°˜ í"]
    end
    
    subgraph Worker["3ï¸âƒ£ Celery Worker (Task ì‹¤í–‰)"]
        Process["Worker Process<br/>â”œâ”€ Consumer (ë©”ì‹œì§€ ìˆ˜ì‹ )<br/>â”œâ”€ Scheduler (ETA, Countdown)<br/>â””â”€ Pool (ë™ì‹œ ì‹¤í–‰)"]
        Pool["Pool íƒ€ì…:<br/>â€¢ Prefork (multiprocessing)<br/>â€¢ Eventlet (green threads)<br/>â€¢ Gevent (coroutines)<br/>â€¢ Solo (ë‹¨ì¼ ìŠ¤ë ˆë“œ)"]
    end
    
    subgraph Backend["4ï¸âƒ£ Result Backend (ê²°ê³¼ ì €ì¥)"]
        DB["PostgreSQL / Redis / MongoDB<br/>task_id â†’ result"]
    end
    
    subgraph Beat["5ï¸âƒ£ Celery Beat (ìŠ¤ì¼€ì¤„ëŸ¬)"]
        Schedule["Beat Process<br/>â€¢ Crontab<br/>â€¢ Interval<br/>â†’ Broker â†’ Worker"]
    end
    
    API -->|Publish<br/>Serialization: JSON/Pickle| RMQ
    RMQ -->|Consume| Process
    Process -->|Store Result| DB
    Schedule -.->|ì£¼ê¸°ì  Task ë°œí–‰| RMQ
    
    style API fill:#cce5ff,stroke:#007bff,stroke-width:3px,color:#000
    style RMQ fill:#F39C12,stroke:#C87F0A,stroke-width:3px,color:#000
    style Redis fill:#E74C3C,stroke:#C0392B,stroke-width:2px,color:#fff
    style Process fill:#7B68EE,stroke:#4B3C8C,stroke-width:3px,color:#fff
    style Pool fill:#9370DB,stroke:#5A478A,stroke-width:2px,color:#fff
    style DB fill:#3498DB,stroke:#2874A6,stroke-width:3px,color:#fff
    style Schedule fill:#51CF66,stroke:#2F9E44,stroke-width:3px,color:#fff
```

---

## 3. ë©”ì‹œì§€ íë¦„

### 3.1 Task ë°œí–‰ (Producer)

```python
# app/api/waste.py
from celery_app import app

@router.post("/analyze")
async def analyze_waste(image: UploadFile):
    # 1. Task ë°œí–‰ (ë¹„ë™ê¸°)
    result = image_upload_task.delay(image.filename)
    
    # ë˜ëŠ” ì˜µì…˜ ì§€ì •
    result = image_upload_task.apply_async(
        args=[image.filename],
        countdown=10,        # 10ì´ˆ í›„ ì‹¤í–‰
        expires=3600,        # 1ì‹œê°„ í›„ ë§Œë£Œ
        retry=True,          # ì¬ì‹œë„ í—ˆìš©
        retry_policy={
            'max_retries': 3,
            'interval_start': 0,
            'interval_step': 0.2,
            'interval_max': 0.2,
        }
    )
    
    return {"task_id": result.id, "status": "pending"}
```

**ë°œí–‰ ì‹œ ìƒì„±ë˜ëŠ” ë©”ì‹œì§€**:
```json
{
  "id": "550e8400-e29b-41d4-a716-446655440000",
  "task": "workers.storage.tasks.image_upload_task",
  "args": ["image.jpg"],
  "kwargs": {},
  "retries": 0,
  "eta": null,
  "expires": "2025-01-01T01:00:00",
  "utc": true,
  "callbacks": null,
  "errbacks": null,
  "chord": null,
  "group": null
}
```

### 3.2 ë©”ì‹œì§€ ë¼ìš°íŒ… (Broker)

```python
# config/celery_config.py
from kombu import Queue, Exchange

app.conf.task_queues = [
    # Default Queue
    Queue('celery', Exchange('celery'), routing_key='celery'),
    
    # ë„ë©”ì¸ë³„ Queue
    Queue('user_input', Exchange('growbin'), routing_key='user.input'),
    Queue('vision_analysis', Exchange('growbin'), routing_key='vision.analyze'),
    Queue('response_generation', Exchange('growbin'), routing_key='response.generate'),
    
    # ìš°ì„ ìˆœìœ„ Queue
    Queue('high_priority', Exchange('growbin'), routing_key='priority.high',
          queue_arguments={'x-max-priority': 10}),
]

# Task ë¼ìš°íŒ… ê·œì¹™
app.conf.task_routes = {
    'workers.storage.tasks.image_upload_task': {
        'queue': 'user_input',
        'routing_key': 'user.input',
    },
    'workers.ai.tasks.gpt5_analysis_task': {
        'queue': 'vision_analysis',
        'routing_key': 'vision.analyze',
    },
}
```

**RabbitMQ Exchange íƒ€ì…**:
```
1. Direct Exchange
   - Routing Key ì •í™•íˆ ì¼ì¹˜
   - ë‹¨ìˆœ ë¼ìš°íŒ…

2. Topic Exchange (ê¶Œì¥)
   - Routing Key íŒ¨í„´ ë§¤ì¹­
   - user.* â†’ ëª¨ë“  user ê´€ë ¨ Queue

3. Fanout Exchange
   - ëª¨ë“  Queueì— ë¸Œë¡œë“œìºìŠ¤íŠ¸
   - ë¡œê¹…, ì•Œë¦¼ ë“±

4. Headers Exchange
   - ë©”ì‹œì§€ í—¤ë” ê¸°ë°˜ ë¼ìš°íŒ…
```

### 3.3 ë©”ì‹œì§€ ì†Œë¹„ (Worker)

```python
# Worker ì‹œì‘
# $ celery -A celery_app worker -Q user_input,vision_analysis -c 4

# Worker ë‚´ë¶€ ë™ì‘
class Worker:
    def start(self):
        # 1. Broker ì—°ê²°
        self.connect_to_broker()
        
        # 2. Queue êµ¬ë…
        self.consumer.consume(queues=['user_input', 'vision_analysis'])
        
        # 3. ë©”ì‹œì§€ ìˆ˜ì‹  ëŒ€ê¸° (ë¬´í•œ ë£¨í”„)
        while True:
            # 4. ë©”ì‹œì§€ ìˆ˜ì‹ 
            message = self.consumer.receive()
            
            # 5. Prefetch í™•ì¸ (ë™ì‹œ ì‹¤í–‰ ì œí•œ)
            if self.pool.is_full():
                self.consumer.reject(message, requeue=True)
                continue
            
            # 6. Poolì— Task í• ë‹¹
            self.pool.apply_async(
                self.execute_task,
                args=(message,)
            )
    
    def execute_task(self, message):
        try:
            # 1. Deserialization
            task_id = message['id']
            task_name = message['task']
            args = message['args']
            kwargs = message['kwargs']
            
            # 2. Task í•¨ìˆ˜ ì‹¤í–‰
            task_func = self.get_task(task_name)
            result = task_func(*args, **kwargs)
            
            # 3. Result Backendì— ì €ì¥
            self.backend.store_result(task_id, result, 'SUCCESS')
            
            # 4. ACK to Broker
            self.consumer.ack(message)
        
        except Exception as e:
            # ì—ëŸ¬ ì²˜ë¦¬
            self.backend.store_result(task_id, str(e), 'FAILURE')
            
            # Retry ë˜ëŠ” Reject
            if message['retries'] < task_func.max_retries:
                self.consumer.reject(message, requeue=True)
            else:
                self.consumer.ack(message)  # DLQë¡œ ì´ë™
```

---

## 4. Task ìƒëª…ì£¼ê¸°

### 4.1 Task ìƒíƒœ ì „ì´

```mermaid
stateDiagram-v2
    [*] --> PENDING: Task ìƒì„±
    PENDING --> RECEIVED: Worker ìˆ˜ì‹ 
    PENDING --> IGNORED: expires ë“±
    RECEIVED --> STARTED: ì‹¤í–‰ ì‹œì‘
    STARTED --> SUCCESS: ì„±ê³µ
    STARTED --> FAILURE: ì‹¤íŒ¨
    STARTED --> REVOKED: ì·¨ì†Œë¨
    FAILURE --> RETRY: ì¬ì‹œë„
    RETRY --> STARTED: ì¬ì‹¤í–‰
    SUCCESS --> [*]
    FAILURE --> [*]
    REVOKED --> [*]
    IGNORED --> [*]
    
    note right of PENDING
        ì´ˆê¸° ìƒíƒœ
        Task ë°œí–‰ë¨
    end note
    
    note right of SUCCESS
        ìµœì¢… ì„±ê³µ ìƒíƒœ
        Result Backend ì €ì¥
    end note
    
    note right of FAILURE
        ìµœì¢… ì‹¤íŒ¨ ìƒíƒœ
        ì¬ì‹œë„ ê°€ëŠ¥
    end note
```

### 4.2 Task ì •ì˜

```python
# workers/storage/tasks.py
from celery import Task
from celery_app import app

# ê¸°ë³¸ Task
@app.task
def simple_task(x, y):
    return x + y

# ì»¤ìŠ¤í…€ Base Task
class CallbackTask(Task):
    """Task ìƒëª…ì£¼ê¸° Hook"""
    
    def on_success(self, retval, task_id, args, kwargs):
        """Task ì„±ê³µ ì‹œ"""
        logger.info(f"Task {task_id} succeeded with result: {retval}")
    
    def on_failure(self, exc, task_id, args, kwargs, einfo):
        """Task ì‹¤íŒ¨ ì‹œ"""
        logger.error(f"Task {task_id} failed: {exc}")
    
    def on_retry(self, exc, task_id, args, kwargs, einfo):
        """Task ì¬ì‹œë„ ì‹œ"""
        logger.warning(f"Task {task_id} retrying: {exc}")

# ì»¤ìŠ¤í…€ Base Task ì‚¬ìš©
@app.task(base=CallbackTask, bind=True)
def image_upload_task(self, image_path):
    try:
        s3_path = upload_to_s3(image_path)
        return {"s3_path": s3_path}
    except S3Error as e:
        # ì¬ì‹œë„
        raise self.retry(exc=e, countdown=60, max_retries=3)
```

### 4.3 Task ì˜µì…˜

```python
@app.task(
    # ì´ë¦„ (ë¼ìš°íŒ… í‚¤ë¡œ ì‚¬ìš©)
    name='workers.storage.image_upload',
    
    # ì¬ì‹œë„ ì„¤ì •
    autoretry_for=(S3Error,),      # ìë™ ì¬ì‹œë„í•  ì˜ˆì™¸
    retry_kwargs={'max_retries': 5, 'countdown': 60},
    
    # íƒ€ì„ì•„ì›ƒ
    time_limit=300,        # Hard timeout (5ë¶„)
    soft_time_limit=270,   # Soft timeout (4.5ë¶„)
    
    # Result Backend
    ignore_result=False,   # ê²°ê³¼ ì €ì¥ ì—¬ë¶€
    
    # ACK ì„¤ì •
    acks_late=True,        # Task ì™„ë£Œ í›„ ACK (ê¶Œì¥)
    reject_on_worker_lost=True,  # Worker ì£½ìœ¼ë©´ ì¬íì‰
    
    # ìš°ì„ ìˆœìœ„
    priority=5,            # 0-9 (ë†’ì„ìˆ˜ë¡ ìš°ì„ )
    
    # Rate Limit
    rate_limit='100/m',    # ë¶„ë‹¹ 100ê°œ
)
def image_upload_task(image_path):
    pass
```

---

## 5. Result Backend

### 5.1 ê²°ê³¼ ì €ì¥ ë° ì¡°íšŒ

```python
# ê²°ê³¼ ì €ì¥ (Workerì—ì„œ ìë™)
result = image_upload_task.delay("image.jpg")

# ê²°ê³¼ ì¡°íšŒ (Producer/APIì—ì„œ)
from celery.result import AsyncResult

# 1. Task IDë¡œ ì¡°íšŒ
task_result = AsyncResult(task_id, app=app)

# 2. ìƒíƒœ í™•ì¸
print(task_result.state)  # PENDING, SUCCESS, FAILURE, ...

# 3. ê²°ê³¼ ê°€ì ¸ì˜¤ê¸° (blocking)
try:
    result = task_result.get(timeout=10)  # 10ì´ˆ ëŒ€ê¸°
    print(result)
except TimeoutError:
    print("Task not completed yet")

# 4. ê²°ê³¼ ê°€ì ¸ì˜¤ê¸° (non-blocking)
if task_result.ready():
    result = task_result.result
else:
    print("Task still running")

# 5. ì„±ê³µ ì—¬ë¶€
if task_result.successful():
    print("Task succeeded")
elif task_result.failed():
    print("Task failed")

# 6. ì—ëŸ¬ ì •ë³´
if task_result.failed():
    print(task_result.traceback)
```

### 5.2 Result Backend êµ¬ì¡° (PostgreSQL)

```sql
-- Celery Result Table
CREATE TABLE celery_taskmeta (
    id SERIAL PRIMARY KEY,
    task_id VARCHAR(255) UNIQUE NOT NULL,
    status VARCHAR(50) NOT NULL,
    result BYTEA,          -- Pickle/JSON serialized
    date_done TIMESTAMP,
    traceback TEXT,
    name VARCHAR(255),
    args BYTEA,
    kwargs BYTEA,
    worker VARCHAR(255),
    retries INTEGER,
    queue VARCHAR(255)
);

CREATE INDEX idx_celery_taskmeta_task_id ON celery_taskmeta(task_id);
CREATE INDEX idx_celery_taskmeta_status ON celery_taskmeta(status);
```

---

## 6. Celery Beat (ìŠ¤ì¼€ì¤„ëŸ¬)

### 6.1 ì£¼ê¸°ì  ì‘ì—… ì •ì˜

```python
# config/celery_config.py
from celery.schedules import crontab

app.conf.beat_schedule = {
    # 5ë¶„ë§ˆë‹¤ ì‹¤í–‰
    'sync-wal-every-5-minutes': {
        'task': 'workers.storage.tasks.sync_wal_to_postgres',
        'schedule': 300.0,  # ì´ˆ ë‹¨ìœ„
    },
    
    # ë§¤ì¼ ìì • ì‹¤í–‰
    'backup-daily-at-midnight': {
        'task': 'workers.maintenance.tasks.backup_database',
        'schedule': crontab(hour=0, minute=0),
    },
    
    # ì›”ìš”ì¼ ì˜¤ì „ 9ì‹œ ì‹¤í–‰
    'weekly-report-monday-9am': {
        'task': 'workers.analytics.tasks.generate_weekly_report',
        'schedule': crontab(hour=9, minute=0, day_of_week=1),
    },
    
    # ë§¤ì‹œê°„ 10ë¶„ì— ì‹¤í–‰
    'cleanup-expired-tasks': {
        'task': 'workers.maintenance.tasks.cleanup_expired_tasks',
        'schedule': crontab(minute=10),
    },
    
    # ë™ì  ìŠ¤ì¼€ì¤„ (interval)
    'dynamic-interval': {
        'task': 'workers.monitoring.tasks.health_check',
        'schedule': timedelta(minutes=5),
    },
}
```

### 6.2 Celery Beat ì‹œì‘

```bash
# Beat ì‹œì‘ (ë³„ë„ í”„ë¡œì„¸ìŠ¤)
celery -A celery_app beat --loglevel=info

# Workerì™€ Beatë¥¼ í•¨ê»˜ ì‹œì‘ (ê°œë°œ í™˜ê²½)
celery -A celery_app worker --beat --loglevel=info

# Beat ìŠ¤ì¼€ì¤„ DB (SQLAlchemy)
celery -A celery_app beat --scheduler django_celery_beat.schedulers:DatabaseScheduler
```

---

## 7. ë™ì‹œì„± ëª¨ë¸

### 7.1 Pool íƒ€ì… ë¹„êµ

```python
# 1. Prefork (ê¸°ë³¸ê°’, multiprocessing)
# - CPU ì§‘ì•½ì  ì‘ì—…ì— ì í•©
# - í”„ë¡œì„¸ìŠ¤ ê²©ë¦¬ (ì•ˆì •ì )
celery -A celery_app worker --pool=prefork --concurrency=4

# 2. Eventlet (green threads)
# - I/O ì§‘ì•½ì  ì‘ì—…ì— ì í•©
# - ìˆ˜ì²œ ê°œ ë™ì‹œ ì‹¤í–‰ ê°€ëŠ¥
celery -A celery_app worker --pool=eventlet --concurrency=1000

# 3. Gevent (coroutines)
# - Eventletê³¼ ìœ ì‚¬
# - ë„¤íŠ¸ì›Œí¬ I/O ìµœì í™”
celery -A celery_app worker --pool=gevent --concurrency=1000

# 4. Solo (ë‹¨ì¼ ìŠ¤ë ˆë“œ)
# - ë””ë²„ê¹…ìš©
# - í”„ë¡œë•ì…˜ ë¹„ê¶Œì¥
celery -A celery_app worker --pool=solo
```

**Pool ì„ íƒ ê°€ì´ë“œ**:
```
CPU ì§‘ì•½ì  (ì´ë¯¸ì§€ ì²˜ë¦¬, ì•”í˜¸í™”):
  â†’ Prefork (multiprocessing)

I/O ì§‘ì•½ì  (API í˜¸ì¶œ, DB ì¿¼ë¦¬):
  â†’ Eventlet ë˜ëŠ” Gevent

í˜¼í•© ì›Œí¬ë¡œë“œ:
  â†’ Prefork (ì ë‹¹í•œ concurrency)
```

### 7.2 Prefetch ì„¤ì •

```python
# config/celery_config.py

# Prefetch Multiplier
# - Workerê°€ í•œ ë²ˆì— ê°€ì ¸ì˜¬ ë©”ì‹œì§€ ìˆ˜
# - prefetch = concurrency * prefetch_multiplier
app.conf.worker_prefetch_multiplier = 4  # ê¸°ë³¸ê°’

# ì˜ˆ: concurrency=4, multiplier=4 â†’ prefetch=16
# WorkerëŠ” í•œ ë²ˆì— 16ê°œ ë©”ì‹œì§€ë¥¼ ê°€ì ¸ì˜´

# I/O ì§‘ì•½ì  ì‘ì—… (Eventlet)
app.conf.worker_prefetch_multiplier = 1  # 1ê°œì”© ê°€ì ¸ì˜´

# CPU ì§‘ì•½ì  ì‘ì—… (Prefork)
app.conf.worker_prefetch_multiplier = 4  # 4ê°œì”© ê°€ì ¸ì˜´
```

---

## 8. ì—ëŸ¬ ì²˜ë¦¬ ë° ì¬ì‹œë„

### 8.1 ìë™ ì¬ì‹œë„

```python
@app.task(
    autoretry_for=(S3Error, NetworkError),  # ìë™ ì¬ì‹œë„í•  ì˜ˆì™¸
    retry_kwargs={
        'max_retries': 5,           # ìµœëŒ€ 5ë²ˆ ì¬ì‹œë„
        'countdown': 60,            # 60ì´ˆ í›„ ì¬ì‹œë„
    },
    retry_backoff=True,             # Exponential Backoff
    retry_backoff_max=600,          # ìµœëŒ€ 10ë¶„
    retry_jitter=True,              # Jitter ì¶”ê°€ (ì¶©ëŒ ë°©ì§€)
)
def upload_to_s3(file_path):
    # S3Error ë°œìƒ ì‹œ ìë™ ì¬ì‹œë„
    s3_client.upload_file(file_path, bucket, key)
```

### 8.2 ìˆ˜ë™ ì¬ì‹œë„

```python
@app.task(bind=True)
def manual_retry_task(self, x):
    try:
        return risky_operation(x)
    except CustomError as e:
        # ì¡°ê±´ë¶€ ì¬ì‹œë„
        if self.request.retries < self.max_retries:
            raise self.retry(
                exc=e,
                countdown=60 * (2 ** self.request.retries),  # Exponential
                max_retries=5
            )
        else:
            # ìµœëŒ€ ì¬ì‹œë„ ì´ˆê³¼ â†’ DLQë¡œ
            logger.error(f"Task failed after {self.max_retries} retries")
            raise
```

### 8.3 Dead Letter Queue (DLQ)

```python
# config/celery_config.py
from kombu import Queue

app.conf.task_queues = [
    Queue('user_input', routing_key='user.input'),
    
    # Dead Letter Queue
    Queue('user_input.dlq', routing_key='user.input.dlq',
          queue_arguments={
              'x-message-ttl': 86400000,  # 24ì‹œê°„ ë³´ê´€
          }),
]

# Task ì‹¤íŒ¨ ì‹œ DLQë¡œ ì´ë™
@app.task(bind=True)
def task_with_dlq(self, data):
    try:
        return process_data(data)
    except Exception as e:
        # ì¬ì‹œë„ ì´ˆê³¼ ì‹œ DLQë¡œ
        if self.request.retries >= self.max_retries:
            dlq_task.apply_async(
                args=[self.request.id, data, str(e)],
                queue='user_input.dlq'
            )
        raise
```

---

## 9. ëª¨ë‹ˆí„°ë§

### 9.1 Flower (ì›¹ UI)

```bash
# Flower ì‹œì‘
celery -A celery_app flower --port=5555

# ì›¹ ë¸Œë¼ìš°ì €: http://localhost:5555
```

**Flower ê¸°ëŠ¥**:
- Task ëª©ë¡ ë° ìƒíƒœ
- Worker ìƒíƒœ ë° í†µê³„
- Task ì‹¤í–‰ ì‹œê°„ ê·¸ë˜í”„
- Task ì¬ì‹œë„, ì·¨ì†Œ

### 9.2 Celery Events

```python
# Worker Events í™œì„±í™”
celery -A celery_app worker --events

# Event Listener
from celery import Celery
from celery.events import EventReceiver

app = Celery()

def monitor_events():
    with app.connection() as conn:
        recv = EventReceiver(conn, handlers={
            'task-sent': on_task_sent,
            'task-received': on_task_received,
            'task-started': on_task_started,
            'task-succeeded': on_task_succeeded,
            'task-failed': on_task_failed,
        })
        recv.capture(limit=None, timeout=None, wakeup=True)

def on_task_sent(event):
    print(f"Task sent: {event['uuid']}")

def on_task_succeeded(event):
    print(f"Task succeeded: {event['uuid']}, result: {event['result']}")
```

### 9.3 Prometheus ë©”íŠ¸ë¦­

```python
# celery_prometheus_exporter ì‚¬ìš©
# pip install celery-prometheus-exporter

# ë©”íŠ¸ë¦­ ì—”ë“œí¬ì¸íŠ¸: http://localhost:8888/metrics
celery-prometheus-exporter

# Prometheus ë©”íŠ¸ë¦­ ì˜ˆì‹œ
celery_tasks_total{state="SUCCESS",name="image_upload_task"} 1234
celery_tasks_total{state="FAILURE",name="image_upload_task"} 56
celery_task_duration_seconds_sum{name="image_upload_task"} 123.45
celery_worker_up{hostname="worker1"} 1
```

---

## 10. Best Practices

### 10.1 Task ì„¤ê³„

```python
# âœ… Good: Idempotent (ë©±ë“±ì„±)
@app.task
def idempotent_task(user_id):
    # ì—¬ëŸ¬ ë²ˆ ì‹¤í–‰í•´ë„ ê°™ì€ ê²°ê³¼
    user = User.objects.get(id=user_id)
    if not user.email_verified:
        user.email_verified = True
        user.save()

# âŒ Bad: Non-Idempotent
@app.task
def non_idempotent_task(user_id):
    # ì—¬ëŸ¬ ë²ˆ ì‹¤í–‰ ì‹œ ì¤‘ë³µ
    user = User.objects.get(id=user_id)
    user.credits += 100  # ì¬ì‹œë„ ì‹œ ì¤‘ë³µ ì ë¦½!
    user.save()

# âœ… Good: ì‘ì€ Task
@app.task
def small_task(item_id):
    # ë‹¨ì¼ ì±…ì„
    item = Item.objects.get(id=item_id)
    return process_item(item)

# âŒ Bad: í° Task
@app.task
def huge_task(user_id):
    # ë„ˆë¬´ ë§ì€ ì‘ì—…
    user = User.objects.get(id=user_id)
    process_user(user)
    send_email(user)
    update_analytics(user)
    generate_report(user)
    # ...
```

### 10.2 ì—ëŸ¬ ì²˜ë¦¬

```python
# âœ… Good: ëª…ì‹œì  ì˜ˆì™¸ ì²˜ë¦¬
@app.task(bind=True)
def good_error_handling(self, data):
    try:
        return process_data(data)
    except ValidationError as e:
        # ì¬ì‹œë„ ë¶ˆí•„ìš”í•œ ì—ëŸ¬ â†’ ì¦‰ì‹œ ì‹¤íŒ¨
        logger.error(f"Validation error: {e}")
        raise
    except NetworkError as e:
        # ì¼ì‹œì  ì—ëŸ¬ â†’ ì¬ì‹œë„
        raise self.retry(exc=e, countdown=60)

# âŒ Bad: ëª¨ë“  ì˜ˆì™¸ catch
@app.task
def bad_error_handling(data):
    try:
        return process_data(data)
    except Exception:
        # ëª¨ë“  ì˜ˆì™¸ ë¬´ì‹œ â†’ ë””ë²„ê¹… ë¶ˆê°€
        pass
```

### 10.3 Result Backend

```python
# âœ… Good: ignore_result=True (ê²°ê³¼ ë¶ˆí•„ìš” ì‹œ)
@app.task(ignore_result=True)
def fire_and_forget_task(data):
    # ê²°ê³¼ í•„ìš” ì—†ìŒ (ë¡œê¹…, ì•Œë¦¼ ë“±)
    send_notification(data)

# âŒ Bad: ëª¨ë“  Taskì— Result ì €ì¥
@app.task(ignore_result=False)
def unnecessary_result_task(data):
    # Result Backend ë¶€í•˜ ì¦ê°€
    send_notification(data)
```

### 10.4 Rate Limiting

```python
# âœ… Good: Rate Limit ì„¤ì •
@app.task(rate_limit='100/m')  # ë¶„ë‹¹ 100ê°œ
def api_call_task(endpoint):
    # ì™¸ë¶€ API í˜¸ì¶œ
    return requests.get(endpoint).json()

# ì‚¬ìš©ìë³„ Rate Limit
@app.task(bind=True)
def user_rate_limited_task(self, user_id, data):
    # Redisë¡œ Rate Limit ì²´í¬
    key = f"rate_limit:user:{user_id}"
    if redis.get(key):
        raise self.retry(countdown=60)
    
    redis.setex(key, 60, 1)
    return process_data(data)
```

### 10.5 Task Chains & Groups

```python
from celery import chain, group, chord

# Chain (ìˆœì°¨ ì‹¤í–‰)
result = chain(
    task1.s(arg1),
    task2.s(),
    task3.s()
).apply_async()

# Group (ë³‘ë ¬ ì‹¤í–‰)
result = group(
    task1.s(1),
    task1.s(2),
    task1.s(3)
).apply_async()

# Chord (ë³‘ë ¬ â†’ ì§‘ê³„)
result = chord(
    group(task1.s(i) for i in range(10))
)(collect_results.s())
```

---

## ğŸ“š ì°¸ê³  ìë£Œ

- [Celery Official Documentation](https://docs.celeryproject.org/)
- [RabbitMQ vs Redis as Celery Broker](https://docs.celeryproject.org/en/stable/getting-started/backends-and-brokers/index.html)
- [Celery Best Practices](https://docs.celeryproject.org/en/stable/userguide/tasks.html#tips-and-best-practices)
- [Flower Monitoring](https://flower.readthedocs.io/)

---

**ë‹¤ìŒ**: [Worker Layer ì•„í‚¤í…ì²˜](./WORKER_LAYER_ARCHITECTURE.md)

