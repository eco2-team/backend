# ğŸ—ï¸ IaC êµ¬ì„± (Terraform + Ansible)

> **ëª©ì **: 4-Node í´ëŸ¬ìŠ¤í„° ìë™ ë°°í¬  
> **ë„êµ¬**: Terraform (AWS ì¸í”„ë¼) + Ansible (Kubernetes ì„¤ì •)  
> **ë‚ ì§œ**: 2025-10-31  
> **ìƒíƒœ**: âœ… í”„ë¡œë•ì…˜ ì™„ë£Œ (75ê°œ ì»¤ë°‹)

**ìë™ ë°°í¬**: `./scripts/auto-rebuild.sh` (40-50ë¶„)

## ğŸ“‹ ëª©ì°¨

1. [IaC ì „ëµ](#iac-ì „ëµ)
2. [Terraform êµ¬ì„±](#terraform-êµ¬ì„±)
3. [Ansible êµ¬ì„±](#ansible-êµ¬ì„±)
4. [ë°°í¬ í”„ë¡œì„¸ìŠ¤](#ë°°í¬-í”„ë¡œì„¸ìŠ¤)
5. [êµ¬í˜„ ê³„íš](#êµ¬í˜„-ê³„íš)

---

## ğŸ¯ IaC ì „ëµ

### Terraform vs Ansible ì—­í•  ë¶„ë¦¬

```mermaid
graph TB
    subgraph Terraform["Terraform (ì¸í”„ë¼ í”„ë¡œë¹„ì €ë‹)"]
        TF1[AWS ë¦¬ì†ŒìŠ¤<br/>EC2, VPC, SG<br/>EBS, Elastic IP]
        TF2[ìƒíƒœ ê´€ë¦¬<br/>terraform.tfstate<br/>S3 Backend]
    end
    
    subgraph Ansible["Ansible (ì„¤ì • ê´€ë¦¬)"]
        AN1[OS ì„¤ì •<br/>íŒ¨í‚¤ì§€ ì„¤ì¹˜<br/>ì»¤ë„ ì„¤ì •]
        AN2[K8s ì„¤ì¹˜<br/>kubeadm init/join<br/>CNI í”ŒëŸ¬ê·¸ì¸]
        AN3[Add-ons<br/>ArgoCD, RabbitMQ<br/>Monitoring]
    end
    
    TF1 --> TF2
    TF2 -->|ì¸ìŠ¤í„´ìŠ¤ ì •ë³´| AN1
    AN1 --> AN2
    AN2 --> AN3
    
    style TF1 fill:#e6d5ff,stroke:#8844ff,stroke-width:3px,color:#000
    style TF2 fill:#e6d5ff,stroke:#8844ff,stroke-width:2px,color:#000
    style AN1 fill:#ffd1d1,stroke:#dc3545,stroke-width:3px,color:#000
    style AN2 fill:#ffd1d1,stroke:#dc3545,stroke-width:3px,color:#000
    style AN3 fill:#ffd1d1,stroke:#dc3545,stroke-width:3px,color:#000
```

### ì±…ì„ ë¶„ë¦¬

| ë„êµ¬ | ì—­í•  | ê´€ë¦¬ ëŒ€ìƒ | ìƒíƒœ ê´€ë¦¬ |
|------|------|----------|----------|
| **Terraform** | ì¸í”„ë¼ í”„ë¡œë¹„ì €ë‹ | EC2, VPC, SG, EBS, EIP | tfstate (S3) |
| **Ansible** | ì„¤ì • ê´€ë¦¬ | OS ì„¤ì •, K8s ì„¤ì¹˜, Add-ons | Idempotent Playbook |

---

## ğŸ”§ Terraform êµ¬ì„±

### í”„ë¡œì íŠ¸ êµ¬ì¡°

```
terraform/
â”œâ”€â”€ main.tf                    # ë©”ì¸ ì„¤ì •
â”œâ”€â”€ variables.tf               # ë³€ìˆ˜ ì •ì˜
â”œâ”€â”€ outputs.tf                 # ì¶œë ¥ (IP ì£¼ì†Œ ë“±)
â”œâ”€â”€ terraform.tfvars          # ë³€ìˆ˜ ê°’
â”œâ”€â”€ backend.tf                # State Backend (S3)
â”‚
â”œâ”€â”€ modules/
â”‚   â”œâ”€â”€ vpc/                  # VPC, ì„œë¸Œë„·
â”‚   â”‚   â”œâ”€â”€ main.tf
â”‚   â”‚   â”œâ”€â”€ variables.tf
â”‚   â”‚   â””â”€â”€ outputs.tf
â”‚   â”‚
â”‚   â”œâ”€â”€ security-groups/      # ë³´ì•ˆ ê·¸ë£¹
â”‚   â”‚   â”œâ”€â”€ main.tf
â”‚   â”‚   â””â”€â”€ variables.tf
â”‚   â”‚
â”‚   â”œâ”€â”€ ec2/                  # EC2 ì¸ìŠ¤í„´ìŠ¤
â”‚   â”‚   â”œâ”€â”€ main.tf
â”‚   â”‚   â”œâ”€â”€ variables.tf
â”‚   â”‚   â””â”€â”€ outputs.tf
â”‚   â”‚
â”‚   â””â”€â”€ ebs/                  # EBS ë³¼ë¥¨
â”‚       â”œâ”€â”€ main.tf
â”‚       â””â”€â”€ variables.tf
â”‚
â””â”€â”€ environments/
    â”œâ”€â”€ dev/
    â”‚   â””â”€â”€ terraform.tfvars
    â””â”€â”€ prod/
        â””â”€â”€ terraform.tfvars
```

### main.tf ê°œìš”

```hcl
# terraform/main.tf
terraform {
  required_version = ">= 1.0"
  
  required_providers {
    aws = {
      source  = "hashicorp/aws"
      version = "~> 5.0"
    }
  }
  
  backend "s3" {
    bucket = "sesacthon-terraform-state"
    key    = "k8s-cluster/terraform.tfstate"
    region = "ap-northeast-2"
  }
}

provider "aws" {
  region = var.aws_region
  
  default_tags {
    tags = {
      Project     = "SeSACTHON"
      ManagedBy   = "Terraform"
      Environment = var.environment
    }
  }
}

# VPC
module "vpc" {
  source = "./modules/vpc"
  
  vpc_cidr = var.vpc_cidr
  environment = var.environment
}

# Security Groups
module "security_groups" {
  source = "./modules/security-groups"
  
  vpc_id = module.vpc.vpc_id
  allowed_ssh_cidr = var.allowed_ssh_cidr
}

# EC2 Instances
module "master" {
  source = "./modules/ec2"
  
  instance_type = "t3.medium"
  instance_name = "k8s-master"
  subnet_id = module.vpc.public_subnet_ids[0]
  security_group_ids = [
    module.security_groups.master_sg_id
  ]
  
  user_data = templatefile("${path.module}/user-data/master.sh", {
    hostname = "k8s-master"
  })
}

module "worker_1" {
  source = "./modules/ec2"
  
  instance_type = "t3.medium"
  instance_name = "k8s-worker-1"
  subnet_id = module.vpc.public_subnet_ids[1]
  security_group_ids = [
    module.security_groups.worker_sg_id
  ]
  
  tags = {
    Workload = "cpu"
  }
}

module "worker_2" {
  source = "./modules/ec2"
  
  instance_type = "t3.small"
  instance_name = "k8s-worker-2"
  subnet_id = module.vpc.public_subnet_ids[2]
  security_group_ids = [
    module.security_groups.worker_sg_id
  ]
  
  tags = {
    Workload = "network"
  }
}
```

### ê´€ë¦¬ ëŒ€ìƒ ë¦¬ì†ŒìŠ¤

```
Terraformìœ¼ë¡œ ìƒì„±:
â”œâ”€ VPC & Subnets (3ê°œ AZ)
â”œâ”€ Internet Gateway
â”œâ”€ Route Tables
â”œâ”€ Security Groups (Master, Worker)
â”œâ”€ EC2 Instances (Master Ã—1, Worker Ã—2)
â”œâ”€ EBS Volumes (ê° 30GB)
â”œâ”€ Elastic IPs (Masterìš©)
â””â”€ Key Pair (SSH ì ‘ê·¼)

ì´ ë¦¬ì†ŒìŠ¤: ì•½ 20ê°œ
ì‹¤í–‰ ì‹œê°„: 5ë¶„
```

---

## ğŸ¤– Ansible êµ¬ì„±

### í”„ë¡œì íŠ¸ êµ¬ì¡°

```
ansible/
â”œâ”€â”€ ansible.cfg               # Ansible ì„¤ì •
â”œâ”€â”€ inventory/
â”‚   â”œâ”€â”€ hosts.ini            # ì¸ë²¤í† ë¦¬ (Terraform ì¶œë ¥)
â”‚   â””â”€â”€ group_vars/
â”‚       â”œâ”€â”€ all.yml          # ê³µí†µ ë³€ìˆ˜
â”‚       â”œâ”€â”€ masters.yml      # Master ë³€ìˆ˜
â”‚       â””â”€â”€ workers.yml      # Worker ë³€ìˆ˜
â”‚
â”œâ”€â”€ playbooks/
â”‚   â”œâ”€â”€ 00-prerequisites.yml # OS ì„¤ì •, Docker ì„¤ì¹˜
â”‚   â”œâ”€â”€ 01-k8s-install.yml   # kubeadm, kubelet, kubectl
â”‚   â”œâ”€â”€ 02-master-init.yml   # kubeadm init
â”‚   â”œâ”€â”€ 03-worker-join.yml   # kubeadm join
â”‚   â”œâ”€â”€ 04-cni-install.yml   # Flannel CNI
â”‚   â”œâ”€â”€ 05-addons.yml        # Ingress, Cert-manager
â”‚   â”œâ”€â”€ 06-argocd.yml        # ArgoCD ì„¤ì¹˜
â”‚   â”œâ”€â”€ 07-rabbitmq.yml      # RabbitMQ ì„¤ì¹˜
â”‚   â””â”€â”€ 08-monitoring.yml    # Prometheus + Grafana
â”‚
â”œâ”€â”€ roles/
â”‚   â”œâ”€â”€ common/              # ê³µí†µ ì„¤ì •
â”‚   â”œâ”€â”€ docker/              # Docker ì„¤ì¹˜
â”‚   â”œâ”€â”€ kubernetes/          # K8s ì„¤ì¹˜
â”‚   â”œâ”€â”€ argocd/              # ArgoCD
â”‚   â””â”€â”€ rabbitmq/            # RabbitMQ
â”‚
â””â”€â”€ site.yml                 # ë§ˆìŠ¤í„° í”Œë ˆì´ë¶
```

### inventory/hosts.ini

```ini
# ansible/inventory/hosts.ini
[all:vars]
ansible_user=ubuntu
ansible_ssh_private_key_file=~/.ssh/sesacthon.pem
ansible_python_interpreter=/usr/bin/python3

[masters]
k8s-master ansible_host=<MASTER_PUBLIC_IP> private_ip=<MASTER_PRIVATE_IP>

[workers]
k8s-worker-1 ansible_host=<WORKER1_PUBLIC_IP> private_ip=<WORKER1_PRIVATE_IP> workload=cpu instance_type=t3.medium
k8s-worker-2 ansible_host=<WORKER2_PUBLIC_IP> private_ip=<WORKER2_PRIVATE_IP> workload=network instance_type=t3.small

[k8s_cluster:children]
masters
workers
```

### site.yml (ë§ˆìŠ¤í„° í”Œë ˆì´ë¶)

```yaml
# ansible/site.yml
---
- name: Kubernetes Cluster Setup
  hosts: all
  become: yes
  gather_facts: yes
  
  roles:
    - common
    - docker
    - kubernetes

- name: Initialize Master
  hosts: masters
  become: yes
  
  tasks:
    - name: kubeadm init
      command: >
        kubeadm init
        --pod-network-cidr=10.244.0.0/16
        --apiserver-advertise-address={{ private_ip }}
        --node-name={{ inventory_hostname }}
      register: kubeadm_init
      when: not kubeadm_init_done | default(false)
    
    - name: Save join command
      copy:
        content: "{{ kubeadm_init.stdout_lines | select('search', 'kubeadm join') | list }}"
        dest: /tmp/kubeadm_join_command.sh
    
    - name: Setup kubeconfig
      shell: |
        mkdir -p $HOME/.kube
        cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
        chown $(id -u):$(id -g) $HOME/.kube/config

- name: Install CNI
  hosts: masters
  become: yes
  
  tasks:
    - name: Apply Flannel
      command: kubectl apply -f https://github.com/flannel-io/flannel/releases/latest/download/kube-flannel.yml

- name: Join Workers
  hosts: workers
  become: yes
  
  tasks:
    - name: Copy join command
      copy:
        src: /tmp/kubeadm_join_command.sh
        dest: /tmp/join.sh
        mode: '0755'
    
    - name: Join cluster
      command: /tmp/join.sh

- name: Label Workers
  hosts: masters
  
  tasks:
    - name: Label worker-1
      command: kubectl label nodes k8s-worker-1 workload=cpu instance-type=t3.medium
    
    - name: Label worker-2
      command: kubectl label nodes k8s-worker-2 workload=network instance-type=t3.small

- name: Install ArgoCD
  hosts: masters
  
  tasks:
    - name: Create ArgoCD namespace
      command: kubectl create namespace argocd --dry-run=client -o yaml | kubectl apply -f -
    
    - name: Install ArgoCD
      command: kubectl apply -n argocd -f https://raw.githubusercontent.com/argoproj/argo-cd/stable/manifests/install.yaml

- name: Install RabbitMQ
  hosts: masters
  
  tasks:
    - name: Add Bitnami Helm repo
      command: helm repo add bitnami https://charts.bitnami.com/bitnami
    
    - name: Install RabbitMQ
      command: >
        helm install rabbitmq bitnami/rabbitmq
        --namespace messaging
        --create-namespace
        --set auth.username=admin
        --set auth.password=changeme
        --set persistence.enabled=true
        --set persistence.size=10Gi
```

---

## ğŸ”„ ë°°í¬ í”„ë¡œì„¸ìŠ¤

### ì „ì²´ íë¦„

```mermaid
flowchart TD
    A[ë¡œì»¬ PC] --> B[Terraform Plan]
    B --> C{ë¦¬ë·°}
    C -->|ìŠ¹ì¸| D[Terraform Apply]
    C -->|ê±°ë¶€| A
    
    D --> E[EC2 ì¸ìŠ¤í„´ìŠ¤ ìƒì„±<br/>5ë¶„]
    
    E --> F[Terraform Output<br/>IP ì£¼ì†Œ ì¶”ì¶œ]
    F --> G[Ansible Inventory<br/>ìë™ ìƒì„±]
    
    G --> H[Ansible Playbook<br/>00-prerequisites]
    H --> I[Ansible Playbook<br/>01-k8s-install]
    I --> J[Ansible Playbook<br/>02-master-init]
    J --> K[Ansible Playbook<br/>03-worker-join]
    K --> L[Ansible Playbook<br/>04-06 Add-ons]
    
    L --> M[í´ëŸ¬ìŠ¤í„° ì¤€ë¹„ ì™„ë£Œ<br/>30ë¶„]
    M --> N[ArgoCD Applications<br/>kubectl apply]
    N --> O[ì„œë¹„ìŠ¤ ë°°í¬ ì™„ë£Œ]
    
    style D fill:#e6d5ff,stroke:#8844ff,stroke-width:3px,color:#000
    style E fill:#e6d5ff,stroke:#8844ff,stroke-width:2px,color:#000
    style H fill:#ffd1d1,stroke:#dc3545,stroke-width:3px,color:#000
    style I fill:#ffd1d1,stroke:#dc3545,stroke-width:3px,color:#000
    style J fill:#ffd1d1,stroke:#dc3545,stroke-width:3px,color:#000
    style K fill:#ffd1d1,stroke:#dc3545,stroke-width:3px,color:#000
    style L fill:#ffd1d1,stroke:#dc3545,stroke-width:3px,color:#000
    style M fill:#d1f2eb,stroke:#28a745,stroke-width:4px,color:#000
    style O fill:#d1f2eb,stroke:#28a745,stroke-width:4px,color:#000
```

---

## ğŸ“¦ Terraform ìƒì„¸ ì„¤ê³„

### ìƒì„±í•  ë¦¬ì†ŒìŠ¤

```hcl
# terraform/main.tf

# 1. VPC
resource "aws_vpc" "k8s_vpc" {
  cidr_block = "10.0.0.0/16"
  enable_dns_hostnames = true
  enable_dns_support = true
  
  tags = {
    Name = "k8s-vpc"
    "kubernetes.io/cluster/sesacthon" = "shared"
  }
}

# 2. ì„œë¸Œë„· (3ê°œ)
resource "aws_subnet" "public" {
  count = 3
  
  vpc_id = aws_vpc.k8s_vpc.id
  cidr_block = "10.0.${count.index + 1}.0/24"
  availability_zone = data.aws_availability_zones.available.names[count.index]
  map_public_ip_on_launch = true
  
  tags = {
    Name = "k8s-public-subnet-${count.index + 1}"
  }
}

# 3. ë³´ì•ˆ ê·¸ë£¹ - Master
resource "aws_security_group" "master" {
  name = "k8s-master-sg"
  vpc_id = aws_vpc.k8s_vpc.id
  
  # SSH
  ingress {
    from_port = 22
    to_port = 22
    protocol = "tcp"
    cidr_blocks = [var.allowed_ssh_cidr]
    description = "SSH from admin"
  }
  
  # Kubernetes API
  ingress {
    from_port = 6443
    to_port = 6443
    protocol = "tcp"
    cidr_blocks = ["0.0.0.0/0"]
    description = "Kubernetes API"
  }
  
  # HTTP/HTTPS
  ingress {
    from_port = 80
    to_port = 80
    protocol = "tcp"
    cidr_blocks = ["0.0.0.0/0"]
  }
  
  ingress {
    from_port = 443
    to_port = 443
    protocol = "tcp"
    cidr_blocks = ["0.0.0.0/0"]
  }
  
  # etcd
  ingress {
    from_port = 2379
    to_port = 2380
    protocol = "tcp"
    self = true
    description = "etcd"
  }
  
  # Kubelet
  ingress {
    from_port = 10250
    to_port = 10252
    protocol = "tcp"
    security_groups = [aws_security_group.worker.id]
  }
  
  # Egress all
  egress {
    from_port = 0
    to_port = 0
    protocol = "-1"
    cidr_blocks = ["0.0.0.0/0"]
  }
}

# 4. ë³´ì•ˆ ê·¸ë£¹ - Worker
resource "aws_security_group" "worker" {
  name = "k8s-worker-sg"
  vpc_id = aws_vpc.k8s_vpc.id
  
  # SSH
  ingress {
    from_port = 22
    to_port = 22
    protocol = "tcp"
    cidr_blocks = [var.allowed_ssh_cidr]
  }
  
  # Kubelet
  ingress {
    from_port = 10250
    to_port = 10250
    protocol = "tcp"
    security_groups = [aws_security_group.master.id]
  }
  
  # NodePort
  ingress {
    from_port = 30000
    to_port = 32767
    protocol = "tcp"
    security_groups = [aws_security_group.master.id]
  }
  
  # Worker ê°„ í†µì‹ 
  ingress {
    from_port = 0
    to_port = 0
    protocol = "-1"
    self = true
  }
  
  egress {
    from_port = 0
    to_port = 0
    protocol = "-1"
    cidr_blocks = ["0.0.0.0/0"]
  }
}

# 5. EC2 Instances
resource "aws_instance" "master" {
  ami = data.aws_ami.ubuntu.id
  instance_type = "t3.medium"
  subnet_id = aws_subnet.public[0].id
  vpc_security_group_ids = [aws_security_group.master.id]
  key_name = aws_key_pair.k8s.key_name
  
  root_block_device {
    volume_size = 30
    volume_type = "gp3"
    encrypted = true
  }
  
  user_data = templatefile("${path.module}/user-data/common.sh", {
    hostname = "k8s-master"
  })
  
  tags = {
    Name = "k8s-master"
    Role = "master"
  }
}

resource "aws_instance" "worker" {
  count = 2
  
  ami = data.aws_ami.ubuntu.id
  instance_type = count.index == 0 ? "t3.medium" : "t3.small"
  subnet_id = aws_subnet.public[count.index + 1].id
  vpc_security_group_ids = [aws_security_group.worker.id]
  key_name = aws_key_pair.k8s.key_name
  
  root_block_device {
    volume_size = 30
    volume_type = "gp3"
    encrypted = true
  }
  
  user_data = templatefile("${path.module}/user-data/common.sh", {
    hostname = "k8s-worker-${count.index + 1}"
  })
  
  tags = {
    Name = "k8s-worker-${count.index + 1}"
    Role = "worker"
    Workload = count.index == 0 ? "cpu" : "network"
  }
}

# 6. Elastic IP (Master)
resource "aws_eip" "master" {
  instance = aws_instance.master.id
  domain = "vpc"
  
  tags = {
    Name = "k8s-master-eip"
  }
}
```

### variables.tf

```hcl
# terraform/variables.tf
variable "aws_region" {
  description = "AWS ë¦¬ì „"
  type = string
  default = "ap-northeast-2"
}

variable "environment" {
  description = "í™˜ê²½ (dev, prod)"
  type = string
  default = "prod"
}

variable "vpc_cidr" {
  description = "VPC CIDR ë¸”ë¡"
  type = string
  default = "10.0.0.0/16"
}

variable "allowed_ssh_cidr" {
  description = "SSH ì ‘ê·¼ í—ˆìš© CIDR"
  type = string
  default = "0.0.0.0/0"  # í”„ë¡œë•ì…˜ì—ì„œëŠ” íŠ¹ì • IPë¡œ ì œí•œ
}

variable "cluster_name" {
  description = "K8s í´ëŸ¬ìŠ¤í„° ì´ë¦„"
  type = string
  default = "sesacthon"
}
```

### outputs.tf

```hcl
# terraform/outputs.tf
output "master_public_ip" {
  description = "Master ë…¸ë“œ Public IP"
  value = aws_eip.master.public_ip
}

output "master_private_ip" {
  description = "Master ë…¸ë“œ Private IP"
  value = aws_instance.master.private_ip
}

output "worker_public_ips" {
  description = "Worker ë…¸ë“œ Public IPs"
  value = aws_instance.worker[*].public_ip
}

output "worker_private_ips" {
  description = "Worker ë…¸ë“œ Private IPs"
  value = aws_instance.worker[*].private_ip
}

output "ansible_inventory" {
  description = "Ansible Inventory ìë™ ìƒì„±"
  value = templatefile("${path.module}/templates/hosts.tpl", {
    master_public_ip = aws_eip.master.public_ip
    master_private_ip = aws_instance.master.private_ip
    worker_1_public_ip = aws_instance.worker[0].public_ip
    worker_1_private_ip = aws_instance.worker[0].private_ip
    worker_2_public_ip = aws_instance.worker[1].public_ip
    worker_2_private_ip = aws_instance.worker[1].private_ip
  })
}
```

---

## ğŸ¤– Ansible ìƒì„¸ ì„¤ê³„

### Playbook ì˜ˆì‹œ

```yaml
# ansible/playbooks/02-master-init.yml
---
- name: Initialize Kubernetes Master
  hosts: masters
  become: yes
  
  vars:
    pod_network_cidr: "10.244.0.0/16"
  
  tasks:
    - name: Check if cluster is initialized
      stat:
        path: /etc/kubernetes/admin.conf
      register: kubeadm_init_stat
    
    - name: kubeadm init
      command: >
        kubeadm init
        --pod-network-cidr={{ pod_network_cidr }}
        --apiserver-advertise-address={{ private_ip }}
        --node-name={{ inventory_hostname }}
      register: kubeadm_init_output
      when: not kubeadm_init_stat.stat.exists
    
    - name: Create .kube directory
      file:
        path: /home/ubuntu/.kube
        state: directory
        owner: ubuntu
        group: ubuntu
        mode: '0755'
    
    - name: Copy kubeconfig
      copy:
        src: /etc/kubernetes/admin.conf
        dest: /home/ubuntu/.kube/config
        owner: ubuntu
        group: ubuntu
        mode: '0644'
        remote_src: yes
    
    - name: Extract join command
      shell: |
        kubeadm token create --print-join-command
      register: join_command
      when: not kubeadm_init_stat.stat.exists
    
    - name: Save join command
      local_action:
        module: copy
        content: "{{ join_command.stdout }}"
        dest: "/tmp/kubeadm_join_command.sh"
      when: join_command is defined

# ansible/playbooks/06-argocd.yml
---
- name: Install ArgoCD
  hosts: masters
  become: yes
  become_user: ubuntu
  
  tasks:
    - name: Create ArgoCD namespace
      kubernetes.core.k8s:
        name: argocd
        api_version: v1
        kind: Namespace
        state: present
    
    - name: Install ArgoCD
      kubernetes.core.k8s:
        state: present
        src: https://raw.githubusercontent.com/argoproj/argo-cd/stable/manifests/install.yaml
    
    - name: Wait for ArgoCD pods
      kubernetes.core.k8s_info:
        kind: Pod
        namespace: argocd
        label_selectors:
          - app.kubernetes.io/name=argocd-server
      register: argocd_pods
      until: argocd_pods.resources[0].status.phase == "Running"
      retries: 30
      delay: 10

# ansible/playbooks/07-rabbitmq.yml
---
- name: Install RabbitMQ
  hosts: masters
  become: yes
  become_user: ubuntu
  
  tasks:
    - name: Add Bitnami Helm repo
      kubernetes.core.helm_repository:
        name: bitnami
        repo_url: https://charts.bitnami.com/bitnami
    
    - name: Create messaging namespace
      kubernetes.core.k8s:
        name: messaging
        api_version: v1
        kind: Namespace
        state: present
    
    - name: Install RabbitMQ via Helm
      kubernetes.core.helm:
        name: rabbitmq
        chart_ref: bitnami/rabbitmq
        release_namespace: messaging
        values:
          auth:
            username: admin
            password: "{{ rabbitmq_password }}"
          persistence:
            enabled: true
            size: 10Gi
          resources:
            requests:
              cpu: 200m
              memory: 256Mi
            limits:
              cpu: 500m
              memory: 512Mi
          nodeSelector:
            kubernetes.io/hostname: k8s-master
```

---

## ğŸš€ ì‹¤í–‰ ëª…ë ¹ì–´

### Terraform

```bash
# 1. ì´ˆê¸°í™”
cd terraform
terraform init

# 2. ê³„íš í™•ì¸
terraform plan -out=tfplan

# 3. ì ìš©
terraform apply tfplan

# 4. ì¶œë ¥ í™•ì¸
terraform output

# 5. Ansible Inventory ìë™ ìƒì„±
terraform output -raw ansible_inventory > ../ansible/inventory/hosts.ini
```

### Ansible

```bash
# 1. Ping í…ŒìŠ¤íŠ¸
cd ansible
ansible all -i inventory/hosts.ini -m ping

# 2. ì „ì²´ í”Œë ˆì´ë¶ ì‹¤í–‰
ansible-playbook -i inventory/hosts.ini site.yml

# ë˜ëŠ” ë‹¨ê³„ë³„ ì‹¤í–‰
ansible-playbook -i inventory/hosts.ini playbooks/00-prerequisites.yml
ansible-playbook -i inventory/hosts.ini playbooks/01-k8s-install.yml
ansible-playbook -i inventory/hosts.ini playbooks/02-master-init.yml
ansible-playbook -i inventory/hosts.ini playbooks/03-worker-join.yml
ansible-playbook -i inventory/hosts.ini playbooks/04-cni-install.yml
ansible-playbook -i inventory/hosts.ini playbooks/05-addons.yml
ansible-playbook -i inventory/hosts.ini playbooks/06-argocd.yml
ansible-playbook -i inventory/hosts.ini playbooks/07-rabbitmq.yml
ansible-playbook -i inventory/hosts.ini playbooks/08-monitoring.yml

# 3. í´ëŸ¬ìŠ¤í„° í™•ì¸
ssh ubuntu@$(terraform output -raw master_public_ip)
kubectl get nodes
```

---

## ğŸ’¡ IaCì˜ ì´ì 

### 1. ì¬í˜„ ê°€ëŠ¥ì„±

```
í•œ ë²ˆ ì‘ì„±í•˜ë©´:
âœ… ë™ì¼í•œ í™˜ê²½ì„ ì–¸ì œë“  ì¬ìƒì„±
âœ… dev, staging, prod í™˜ê²½ ì¼ê´€ì„±
âœ… ì¬í•´ ë³µêµ¬ ì‹œ ë¹ ë¥¸ ë³µêµ¬ (1ì‹œê°„)
```

### 2. ë²„ì „ ê´€ë¦¬

```
terraform/ í´ë” ì „ì²´ë¥¼ Gitì—:
âœ… ì¸í”„ë¼ ë³€ê²½ ì´ë ¥ ì¶”ì 
âœ… íŠ¹ì • ì‹œì ìœ¼ë¡œ ë¡¤ë°±
âœ… ì½”ë“œ ë¦¬ë·° (ì¸í”„ë¼ë„!)
```

### 3. í˜‘ì—…

```
íŒ€ì›ë“¤ì´:
âœ… Terraform ì½”ë“œë§Œ ë³´ë©´ ì¸í”„ë¼ ì´í•´
âœ… PRë¡œ ì¸í”„ë¼ ë³€ê²½ ì œì•ˆ
âœ… ìë™í™”ëœ í…ŒìŠ¤íŠ¸ (terraform plan)
```

### 4. ë¹„ìš© ê´€ë¦¬

```
terraform destroy:
âœ… ê°œë°œ ì™„ë£Œ í›„ ì¸í”„ë¼ ì‚­ì œ (ë¹„ìš© ì ˆê°)
âœ… í•„ìš”í•  ë•Œë§Œ terraform apply
âœ… ë¹„ìš© ì˜ˆì¸¡ (terraform plan)
```

---

## âš ï¸ ì£¼ì˜ì‚¬í•­

### Terraform State ê´€ë¦¬

```bash
# S3 Backend ì‚¬ìš© (í•„ìˆ˜!)
# terraform/backend.tf
terraform {
  backend "s3" {
    bucket = "sesacthon-terraform-state"
    key = "k8s-cluster/terraform.tfstate"
    region = "ap-northeast-2"
    
    # State Lock (DynamoDB)
    dynamodb_table = "terraform-lock"
    encrypt = true
  }
}

# StateëŠ” ì ˆëŒ€ Gitì— ì»¤ë°‹ ê¸ˆì§€!
# .gitignoreì— ì¶”ê°€:
terraform.tfstate
terraform.tfstate.backup
.terraform/
```

### Sensitive ì •ë³´

```bash
# ë¯¼ê° ì •ë³´ëŠ” ë³€ìˆ˜ë¡œ
# terraform.tfvars (Git ë¬´ì‹œ)
rabbitmq_password = "super-secret-password"
db_password = "another-secret"

# ë˜ëŠ” í™˜ê²½ë³€ìˆ˜
export TF_VAR_rabbitmq_password="secret"

# Ansible Vault ì‚¬ìš©
ansible-vault encrypt group_vars/all.yml
ansible-playbook site.yml --ask-vault-pass
```

---

## ğŸ“‹ êµ¬í˜„ ì²´í¬ë¦¬ìŠ¤íŠ¸

### Terraform

- [ ] VPC ëª¨ë“ˆ ì‘ì„±
- [ ] ë³´ì•ˆ ê·¸ë£¹ ëª¨ë“ˆ
- [ ] EC2 ëª¨ë“ˆ
- [ ] S3 Backend ì„¤ì •
- [ ] variables.tf ì •ì˜
- [ ] outputs.tf ì •ì˜
- [ ] user-data ìŠ¤í¬ë¦½íŠ¸

### Ansible

- [ ] inventory í…œí”Œë¦¿
- [ ] common role (OS ì„¤ì •)
- [ ] docker role (Docker ì„¤ì¹˜)
- [ ] kubernetes role (kubeadm ì„¤ì¹˜)
- [ ] master-init playbook
- [ ] worker-join playbook
- [ ] argocd playbook
- [ ] rabbitmq playbook
- [ ] monitoring playbook

---

## ğŸ¯ ìµœì¢… êµ¬ì¡°

```
backend/
â”œâ”€â”€ terraform/               # ì¸í”„ë¼ ì½”ë“œ
â”‚   â”œâ”€â”€ main.tf
â”‚   â”œâ”€â”€ variables.tf
â”‚   â”œâ”€â”€ outputs.tf
â”‚   â”œâ”€â”€ modules/
â”‚   â””â”€â”€ environments/
â”‚
â”œâ”€â”€ ansible/                 # ì„¤ì • ì½”ë“œ
â”‚   â”œâ”€â”€ ansible.cfg
â”‚   â”œâ”€â”€ inventory/
â”‚   â”œâ”€â”€ playbooks/
â”‚   â”œâ”€â”€ roles/
â”‚   â””â”€â”€ site.yml
â”‚
â”œâ”€â”€ docs/
â”‚   â””â”€â”€ architecture/
â”‚       â””â”€â”€ iac-terraform-ansible.md  # ì´ ë¬¸ì„œ
â”‚
â””â”€â”€ scripts/
    â”œâ”€â”€ provision.sh         # ì „ì²´ ìë™í™” ìŠ¤í¬ë¦½íŠ¸
    â””â”€â”€ destroy.sh           # ì¸í”„ë¼ ì‚­ì œ
```

---

## ğŸš€ One-command ë°°í¬

```bash
# scripts/provision.sh
#!/bin/bash
set -e

echo "ğŸš€ K8s í´ëŸ¬ìŠ¤í„° í”„ë¡œë¹„ì €ë‹ ì‹œì‘..."

# 1. Terraform
cd terraform
terraform init
terraform apply -auto-approve
terraform output -raw ansible_inventory > ../ansible/inventory/hosts.ini

# 2. Ansible (2ë¶„ ëŒ€ê¸° í›„)
sleep 120  # EC2 ì™„ì „íˆ ë¶€íŒ… ëŒ€ê¸°
cd ../ansible
ansible-playbook -i inventory/hosts.ini site.yml

# 3. ArgoCD Applications
sleep 60
ssh ubuntu@$(cd ../terraform && terraform output -raw master_public_ip) \
  "kubectl apply -f /tmp/argocd-applications.yaml"

echo "âœ… í´ëŸ¬ìŠ¤í„° í”„ë¡œë¹„ì €ë‹ ì™„ë£Œ!"
echo "Master IP: $(cd terraform && terraform output -raw master_public_ip)"
echo "ArgoCD: https://argocd.yourdomain.com"
```

---

## ğŸ“š ì°¸ê³  ìë£Œ

- [Terraform AWS Provider](https://registry.terraform.io/providers/hashicorp/aws/latest/docs)
- [Ansible Kubernetes Module](https://docs.ansible.com/ansible/latest/collections/kubernetes/core/index.html)
- [Terraform Best Practices](https://www.terraform-best-practices.com/)

---

**ì‘ì„±ì¼**: 2025-10-30  
**ìƒíƒœ**: ğŸ”„ ê²€í†  ëŒ€ê¸°  
**ì˜ˆìƒ êµ¬ì¶• ì‹œê°„**: Terraform (5ë¶„) + Ansible (30ë¶„) = 35ë¶„

