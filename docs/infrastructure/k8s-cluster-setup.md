# ğŸš€ K8s í´ëŸ¬ìŠ¤í„° êµ¬ì¶• ê°€ì´ë“œ (4-Node)

> **êµ¬ì„±**: 1 Master + 3 Workers (App + Async + Storage)  
> **ë„êµ¬**: Kubernetes (kubeadm) + Calico VXLAN  
> **ë‚ ì§œ**: 2025-10-31  
> **ìƒíƒœ**: âœ… í”„ë¡œë•ì…˜ ì¤€ë¹„ ì™„ë£Œ

âš ï¸ **ì´ ë¬¸ì„œëŠ” ìˆ˜ë™ ì„¤ì¹˜ ê°€ì´ë“œì…ë‹ˆë‹¤.**  
**ìë™ ë°°í¬ëŠ” [Ansible](iac-terraform-ansible.md)ì„ ì‚¬ìš©í•˜ì„¸ìš”!**

## ğŸ“‹ ëª©ì°¨

1. [í´ëŸ¬ìŠ¤í„° ì‚¬ì–‘](#í´ëŸ¬ìŠ¤í„°-ì‚¬ì–‘)
2. [ì¸í”„ë¼ êµ¬ì„±](#ì¸í”„ë¼-êµ¬ì„±)
3. [ì„¤ì¹˜ ê°€ì´ë“œ](#ì„¤ì¹˜-ê°€ì´ë“œ)
4. [ì„œë¹„ìŠ¤ ë°°í¬](#ì„œë¹„ìŠ¤-ë°°í¬)
5. [ìš´ì˜ ê°€ì´ë“œ](#ìš´ì˜-ê°€ì´ë“œ)

---

## ğŸ—ï¸ í´ëŸ¬ìŠ¤í„° ì‚¬ì–‘

### 4-Node êµ¬ì„± (Instagram + Robin íŒ¨í„´)

```mermaid
graph TB
    subgraph K8s["Kubernetes Cluster (4-Node)"]
        Master[Master<br/>t3.large<br/>2 vCPU, 8GB, 80GB<br/>$60/ì›”<br/><br/>Control Plane:<br/>- kube-apiserver<br/>- etcd<br/>- scheduler<br/>- controller<br/><br/>Monitoring:<br/>- Prometheus<br/>- Grafana]
        
        Worker1[Worker-1<br/>t3.medium<br/>2 vCPU, 4GB, 40GB<br/>$30/ì›”<br/><br/>Application:<br/>- auth-service<br/>- users-service<br/>- locations-service<br/>(FastAPI / Sync)]
        
        Worker2[Worker-2<br/>t3.medium<br/>2 vCPU, 4GB, 40GB<br/>$30/ì›”<br/><br/>Async Workers:<br/>- celery-ai-worker<br/>- celery-batch-worker<br/>(GPT-4o Vision)]
        
        Storage[Storage<br/>t3.large<br/>2 vCPU, 8GB, 100GB<br/>$60/ì›”<br/><br/>Stateful:<br/>- RabbitMQ HA<br/>- PostgreSQL<br/>- Redis]
    end
    
    Master -.->|manage| Worker1
    Master -.->|manage| Worker2
    Master -.->|manage| Storage
    Worker1 -->|publish| Storage
    Worker2 -->|consume| Storage
    
    style Master fill:#cce5ff,stroke:#0d47a1,stroke-width:3px
    style Worker1 fill:#d1f2eb,stroke:#33691e,stroke-width:2px
    style Worker2 fill:#ffe0b3,stroke:#f57f17,stroke-width:2px
    style Storage fill:#ffd1d1,stroke:#880e4f,stroke-width:3px
    
    subgraph Services["ì„œë¹„ìŠ¤ ë¶„ì‚°"]
        Heavy[Heavy Workload<br/>waste, recycling]
        Light[Light Workload<br/>auth, users, locations]
    end
    
    Users --> Master
    Master -.->|orchestrate| Worker1
    Master -.->|orchestrate| Worker2
    
    Worker1 --> Heavy
    Worker2 --> Light
    
    style Master fill:#ffd1d1,stroke:#dc3545,stroke-width:4px,color:#000
    style Worker1 fill:#ffe0b3,stroke:#fd7e14,stroke-width:3px,color:#000
    style Worker2 fill:#d1f2eb,stroke:#28a745,stroke-width:3px,color:#000
```

### ë¹„ìš© ê³„ì‚°

```
=== ì»´í“¨íŒ… ===
Master (t3.medium): $30/ì›”
Worker 1 (t3.medium): $30/ì›”
Worker 2 (t3.small): $15/ì›”
ì†Œê³„: $75/ì›”

=== ìŠ¤í† ë¦¬ì§€ ===
EBS gp3 (30GB Ã— 3): $2/ì›” Ã— 3 = $6/ì›”

=== ë„¤íŠ¸ì›Œí¬ ===
ë°ì´í„° ì „ì†¡ (100GB): $9/ì›”

=== ë ˆì§€ìŠ¤íŠ¸ë¦¬ ===
ECR (ë˜ëŠ” Docker Hub $0): $1/ì›”

â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
ì´: $91/ì›”
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

ìµœì í™” ì˜µì…˜ (ì„ íƒ):
â”œâ”€ Worker 2ë¥¼ Spotìœ¼ë¡œ: $15 â†’ $4.5 (ì ˆê° $10.5)
â”œâ”€ Docker Hub ì‚¬ìš©: $1 â†’ $0 (ì ˆê° $1)
â””â”€ ìµœì í™” í›„: $79.5/ì›”

vs Docker Compose: $60/ì›”
ì°¨ì´: $19.5-31/ì›” (1.3-1.5ë°°)

â†’ K8s ìƒíƒœê³„ë¥¼ ì›” $20 ì¶”ê°€ë¡œ!
```

---

## ğŸ–¥ï¸ ì¸í”„ë¼ êµ¬ì„±

### AWS EC2 ì¸ìŠ¤í„´ìŠ¤ ìŠ¤í™

```yaml
# terraform ë˜ëŠ” AWS Console ì„¤ì •

Master Node:
  instance_type: t3.medium
  ami: ubuntu-22.04
  vpc_security_group:
    - SSH (22): ë³¸ì¸ IPë§Œ
    - K8s API (6443): Worker IPs
    - HTTP/HTTPS (80, 443): 0.0.0.0/0
    - Kubernetes (10250, 10251, 10252): Worker IPs
  ebs:
    size: 30GB
    type: gp3
  tags:
    Name: sesacthon-k8s-master
    Role: master

Worker Node 1:
  instance_type: t3.medium
  ami: ubuntu-22.04
  vpc_security_group:
    - SSH (22): ë³¸ì¸ IP
    - Kubelet (10250): Master IP
    - NodePort (30000-32767): Master IP
  ebs:
    size: 30GB
    type: gp3
  tags:
    Name: sesacthon-k8s-worker-1
    Role: worker
    Workload: heavy

Worker Node 2:
  instance_type: t3.small
  ami: ubuntu-22.04
  vpc_security_group:
    - SSH (22): ë³¸ì¸ IP
    - Kubelet (10250): Master IP
    - NodePort (30000-32767): Master IP
  ebs:
    size: 20GB
    type: gp3
  tags:
    Name: sesacthon-k8s-worker-2
    Role: worker
    Workload: light
```

### ë„¤íŠ¸ì›Œí¬ ì„¤ì •

#### VPC êµ¬ì„±

```
VPC: 10.0.0.0/16
â”œâ”€ Public Subnet A: 10.0.1.0/24 (ap-northeast-2a)
â”œâ”€ Public Subnet B: 10.0.2.0/24 (ap-northeast-2b)
â””â”€ Public Subnet C: 10.0.3.0/24 (ap-northeast-2c)

Internet Gateway: igw-xxxxx
Route Table: 0.0.0.0/0 â†’ IGW
```

#### ë³´ì•ˆ ê·¸ë£¹ (Security Groups)

```mermaid
graph TB
    subgraph Internet["ğŸŒ ì¸í„°ë„·"]
        Users["ì‚¬ìš©ì
Any IP"]
        Admin["ê´€ë¦¬ì
ë³¸ì¸ IP"]
    end
    
    subgraph VPC["VPC 10.0.0.0/16"]
        subgraph MasterSG["Master Security Group"]
            Master["Master Node
10.0.1.x"]
        end
        
        subgraph WorkerSG["Worker Security Group"]
            Worker1["Worker-1
10.0.2.x"]
            Worker2["Worker-2
10.0.2.x"]
            Storage["Storage
10.0.2.x"]
        end
    end
    
    Admin -->|"SSH (22)"| Master
    Admin -->|"SSH (22)"| Worker1
    Admin -->|"SSH (22)"| Worker2
    Admin -->|"SSH (22)"| Storage
    
    Users -->|"HTTPS (443) ALB"| Master
    Users -->|"HTTP (80) ALB"| Master
    
    Master <-->|"6443 (API Server)"| Worker1
    Master <-->|"6443"| Worker2
    Master <-->|"6443"| Storage
    
    Master <-->|"10250 (Kubelet)"| Worker1
    Master <-->|"10250"| Worker2
    Master <-->|"10250"| Storage
    
    Master <-->|"2379-2380 (etcd)"| Master
    
    Worker1 <-->|"All Traffic (Self)"| Worker1
    Worker1 <-->|"4789 UDP (VXLAN)"| Worker2
    Worker1 <-->|"4789 UDP"| Storage
    Worker1 <-->|"4789 UDP"| Master
    
    Worker2 <-->|"All Traffic (Self)"| Worker2
    Worker2 <-->|"4789 UDP"| Storage
    Worker2 <-->|"4789 UDP"| Master
    
    Storage <-->|"All Traffic (Self)"| Storage
    Storage <-->|"4789 UDP"| Master
    
    style Internet fill:#cce5ff,stroke:#0d47a1,stroke-width:2px
    style VPC fill:#d1f2eb,stroke:#33691e,stroke-width:3px
    style MasterSG fill:#ffe0b3,stroke:#f57f17,stroke-width:2px
    style WorkerSG fill:#ffd1d1,stroke:#880e4f,stroke-width:2px
    style Master fill:#ffe0b3,stroke:#e65100,stroke-width:2px
    style Worker1 fill:#d1f2eb,stroke:#2e7d32,stroke-width:2px
    style Worker2 fill:#d1f2eb,stroke:#2e7d32,stroke-width:2px
    style Storage fill:#ffd1d1,stroke:#c2185b,stroke-width:2px
```

#### Master Security Group ìƒì„¸

| ë°©í–¥ | í”„ë¡œí† ì½œ | í¬íŠ¸ | ì†ŒìŠ¤ | ëª©ì  |
|------|---------|------|------|------|
| Inbound | TCP | 22 | ë³¸ì¸ IP | SSH ì ‘ì† |
| Inbound | TCP | 6443 | 0.0.0.0/0 | Kubernetes API Server |
| Inbound | TCP | 2379-2380 | Self | etcd server client API |
| Inbound | TCP | 10250 | Worker SG | Kubelet API |
| Inbound | TCP | 10251 | Self | kube-scheduler |
| Inbound | TCP | 10252 | Self | kube-controller-manager |
| Inbound | TCP | 80 | ALB SG | HTTP (Ingress) |
| Inbound | TCP | 443 | ALB SG | HTTPS (Ingress) |
| Inbound | UDP | 4789 | Master SG, Worker SG | Calico VXLAN |
| Outbound | All | All | 0.0.0.0/0 | ëª¨ë“  ì•„ì›ƒë°”ìš´ë“œ |

#### Worker Security Group ìƒì„¸

| ë°©í–¥ | í”„ë¡œí† ì½œ | í¬íŠ¸ | ì†ŒìŠ¤ | ëª©ì  |
|------|---------|------|------|------|
| Inbound | TCP | 22 | ë³¸ì¸ IP | SSH ì ‘ì† |
| Inbound | TCP | 10250 | Master SG | Kubelet API |
| Inbound | TCP | 30000-32767 | Master SG | NodePort Services |
| Inbound | All | All | Worker SG (Self) | Pod ê°„ í†µì‹  |
| Inbound | UDP | 4789 | Master SG, Worker SG | Calico VXLAN |
| Outbound | All | All | 0.0.0.0/0 | ëª¨ë“  ì•„ì›ƒë°”ìš´ë“œ |

---

## âš¡ ì„¤ì¹˜ ê°€ì´ë“œ

### Phase 1: EC2 ì¸ìŠ¤í„´ìŠ¤ ì¤€ë¹„ (30ë¶„)

```bash
# ===== 3ê°œ ì¸ìŠ¤í„´ìŠ¤ ëª¨ë‘ ì‹¤í–‰ =====

# 1. ì‹œìŠ¤í…œ ì—…ë°ì´íŠ¸
sudo apt update && sudo apt upgrade -y

# 2. í•„ìˆ˜ íŒ¨í‚¤ì§€ ì„¤ì¹˜
sudo apt install -y curl wget git

# 3. í˜¸ìŠ¤íŠ¸ëª… ì„¤ì •
# Master
sudo hostnamectl set-hostname k8s-master

# Worker 1
sudo hostnamectl set-hostname k8s-worker-1

# Worker 2
sudo hostnamectl set-hostname k8s-worker-2

# 4. /etc/hosts ì—…ë°ì´íŠ¸ (ëª¨ë“  ë…¸ë“œ)
cat <<EOF | sudo tee -a /etc/hosts
<MASTER_IP>   k8s-master
<WORKER1_IP>  k8s-worker-1
<WORKER2_IP>  k8s-worker-2
EOF

# 5. ë°©í™”ë²½ ë¹„í™œì„±í™” (ì„ íƒì , ë³´ì•ˆê·¸ë£¹ ì‚¬ìš© ì‹œ)
sudo ufw disable
```

### Phase 2: Kubernetes Master ì„¤ì¹˜ (1ì‹œê°„)

```bash
# ===== Master Nodeì—ì„œ ì‹¤í–‰ =====

# 1. Docker ì„¤ì¹˜
sudo apt-get update
sudo apt-get install -y apt-transport-https ca-certificates curl
curl -fsSL https://get.docker.com -o get-docker.sh
sudo sh get-docker.sh
sudo usermod -aG docker $USER

# 2. kubeadm, kubelet, kubectl ì„¤ì¹˜
sudo mkdir -p /etc/apt/keyrings
curl -fsSL https://pkgs.k8s.io/core:/stable:/v1.28/deb/Release.key | \
  sudo gpg --dearmor -o /etc/apt/keyrings/kubernetes-apt-keyring.gpg

echo 'deb [signed-by=/etc/apt/keyrings/kubernetes-apt-keyring.gpg] https://pkgs.k8s.io/core:/stable:/v1.28/deb/ /' | \
  sudo tee /etc/apt/sources.list.d/kubernetes.list

sudo apt-get update
sudo apt-get install -y kubelet kubeadm kubectl
sudo apt-mark hold kubelet kubeadm kubectl

# 3. ìŠ¤ì™‘ ë¹„í™œì„±í™” (í•„ìˆ˜)
sudo swapoff -a
sudo sed -i '/ swap / s/^/#/' /etc/fstab

# 4. ì»¤ë„ ëª¨ë“ˆ ë¡œë“œ
sudo modprobe overlay
sudo modprobe br_netfilter

cat <<EOF | sudo tee /etc/modules-load.d/k8s.conf
overlay
br_netfilter
EOF

# 5. sysctl ì„¤ì •
cat <<EOF | sudo tee /etc/sysctl.d/k8s.conf
net.bridge.bridge-nf-call-iptables  = 1
net.bridge.bridge-nf-call-ip6tables = 1
net.ipv4.ip_forward                 = 1
EOF

sudo sysctl --system

# 6. kubeadm init (í´ëŸ¬ìŠ¤í„° ì´ˆê¸°í™”)
sudo kubeadm init \
  --pod-network-cidr=192.168.0.0/16 \
  --apiserver-advertise-address=<MASTER_PRIVATE_IP> \
  --node-name=k8s-master

# pod-network-cidr: Calico ê¸°ë³¸ê°’ (192.168.0.0/16)

# ì¶œë ¥ëœ kubeadm join ëª…ë ¹ì–´ ì €ì¥!
# kubeadm join <MASTER_IP>:6443 --token <TOKEN> \
#   --discovery-token-ca-cert-hash sha256:<HASH>

# 7. kubeconfig ì„¤ì •
mkdir -p $HOME/.kube
sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
sudo chown $(id -u):$(id -g) $HOME/.kube/config

# 8. CNI í”ŒëŸ¬ê·¸ì¸ ì„¤ì¹˜ (Calico - ì•ˆì •ì )
kubectl apply -f \
  https://raw.githubusercontent.com/projectcalico/calico/v3.26.4/manifests/calico.yaml

# Calico ì„ íƒ ì´ìœ :
# - Flannelë³´ë‹¤ ì•ˆì •ì 
# - í”„ë¡œë•ì…˜ ê²€ì¦ë¨
# - ë„¤íŠ¸ì›Œí¬ ì •ì±… ì§€ì›
# - ëŒ€ê·œëª¨ í´ëŸ¬ìŠ¤í„° ìµœì í™”

# 9. Master Taint ì œê±° (Masterì—ë„ Pod ë°°ì¹˜í•˜ë ¤ë©´, ì„ íƒ)
# kubectl taint nodes k8s-master node-role.kubernetes.io/control-plane:NoSchedule-

# 10. ì„¤ì¹˜ í™•ì¸
kubectl get nodes
# NAME         STATUS   ROLES           AGE   VERSION
# k8s-master   Ready    control-plane   2m    v1.28.4

# âœ… Master ì„¤ì¹˜ ì™„ë£Œ!
```

### Phase 3: Kubernetes Worker ì¡°ì¸ (10ë¶„ Ã— 2)

```bash
# ===== Worker Node 1, 2ì—ì„œ ê°ê° ì‹¤í–‰ =====

# 1. Docker ì„¤ì¹˜ (Masterì™€ ë™ì¼)
curl -fsSL https://get.docker.com -o get-docker.sh
sudo sh get-docker.sh

# 2. kubeadm, kubelet, kubectl ì„¤ì¹˜ (Masterì™€ ë™ì¼)
sudo mkdir -p /etc/apt/keyrings
curl -fsSL https://pkgs.k8s.io/core:/stable:/v1.28/deb/Release.key | \
  sudo gpg --dearmor -o /etc/apt/keyrings/kubernetes-apt-keyring.gpg

echo 'deb [signed-by=/etc/apt/keyrings/kubernetes-apt-keyring.gpg] https://pkgs.k8s.io/core:/stable:/v1.28/deb/ /' | \
  sudo tee /etc/apt/sources.list.d/kubernetes.list

sudo apt-get update
sudo apt-get install -y kubelet kubeadm kubectl
sudo apt-mark hold kubelet kubeadm kubectl

# 3. ìŠ¤ì™‘ ë¹„í™œì„±í™”
sudo swapoff -a
sudo sed -i '/ swap / s/^/#/' /etc/fstab

# 4. ì»¤ë„ ëª¨ë“ˆ & sysctl (Masterì™€ ë™ì¼)
sudo modprobe overlay
sudo modprobe br_netfilter

cat <<EOF | sudo tee /etc/sysctl.d/k8s.conf
net.bridge.bridge-nf-call-iptables  = 1
net.bridge.bridge-nf-call-ip6tables = 1
net.ipv4.ip_forward                 = 1
EOF

sudo sysctl --system

# 5. Masterì—ì„œ ë³µì‚¬í•œ kubeadm join ëª…ë ¹ì–´ ì‹¤í–‰
sudo kubeadm join <MASTER_IP>:6443 \
  --token <TOKEN> \
  --discovery-token-ca-cert-hash sha256:<HASH>

# Worker 1 ë ˆì´ë¸” ì¶”ê°€
# (Masterì—ì„œ ì‹¤í–‰)
kubectl label nodes k8s-worker-1 workload=cpu
kubectl label nodes k8s-worker-1 instance-type=t3.medium

# Worker 2 ë ˆì´ë¸” ì¶”ê°€
kubectl label nodes k8s-worker-2 workload=network
kubectl label nodes k8s-worker-2 instance-type=t3.small

# ===== Masterì—ì„œ í™•ì¸ =====
kubectl get nodes
# NAME           STATUS   ROLES           AGE   VERSION
# k8s-master     Ready    control-plane   10m   v1.28.4
# k8s-worker-1   Ready    <none>          5m    v1.28.4
# k8s-worker-2   Ready    <none>          3m    v1.28.4

# Label í™•ì¸
kubectl get nodes --show-labels

# âœ… í´ëŸ¬ìŠ¤í„° êµ¬ì¶• ì™„ë£Œ! (ì´ 1.5ì‹œê°„)
```

### Phase 4: í•„ìˆ˜ Add-ons ì„¤ì¹˜ (1ì‹œê°„)

```bash
# ===== 1. AWS Load Balancer Controller (15ë¶„) =====

# Helm ì„¤ì¹˜
curl https://raw.githubusercontent.com/helm/helm/main/scripts/get-helm-3 | bash

# CRDs ì„¤ì¹˜
kubectl apply -k "github.com/aws/eks-charts/stable/aws-load-balancer-controller/crds?ref=master"

# Helm repo ì¶”ê°€
helm repo add eks https://aws.github.io/eks-charts
helm repo update

# ALB Controller ì„¤ì¹˜
helm install aws-load-balancer-controller eks/aws-load-balancer-controller \
  -n kube-system \
  --set clusterName=prod-sesacthon \
  --set region=ap-northeast-2 \
  --set vpcId=vpc-xxxxx

# ALB Controller Pod ëŒ€ê¸°
kubectl wait --for=condition=ready pod \
  -l app.kubernetes.io/name=aws-load-balancer-controller \
  -n kube-system \
  --timeout=300s

# ===== 2. Cert-manager (Certificate ê´€ë¦¬, 10ë¶„) =====
kubectl apply -f https://github.com/cert-manager/cert-manager/releases/download/v1.13.0/cert-manager.yaml

# ===== 3. ACM Certificate (AWS Consoleì—ì„œ ë¯¸ë¦¬ ìƒì„±) =====
# AWS Console â†’ Certificate Manager
# 1. Request certificate
# 2. Domain: *.growbin.app
# 3. Validation: DNS (Route53)
# 4. ARN ë©”ëª¨: arn:aws:acm:ap-northeast-2:xxxxx:certificate/xxxxx
#
# cert-managerëŠ” Certificate ë¦¬ì†ŒìŠ¤ ê´€ë¦¬ìš©ìœ¼ë¡œë§Œ ì‚¬ìš©

# ===== 3. Metrics Server (HPAìš©, 5ë¶„) =====
kubectl apply -f https://github.com/kubernetes-sigs/metrics-server/releases/latest/download/components.yaml

# ===== 4. ArgoCD (GitOps, 15ë¶„) =====
kubectl create namespace argocd
kubectl apply -n argocd -f \
  https://raw.githubusercontent.com/argoproj/argo-cd/stable/manifests/install.yaml

# ArgoCD Ingress ì„¤ì •
cat <<EOF | kubectl apply -f -
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: argocd-server-ingress
  namespace: argocd
  annotations:
    kubernetes.io/ingress.class: alb
    alb.ingress.kubernetes.io/scheme: internet-facing
    alb.ingress.kubernetes.io/target-type: ip
    alb.ingress.kubernetes.io/certificate-arn: arn:aws:acm:ap-northeast-2:xxxxx:certificate/xxxxx
    alb.ingress.kubernetes.io/backend-protocol: HTTPS
spec:
  rules:
  - host: growbin.app
    http:
      paths:
      - path: /argocd
        pathType: Prefix
        backend:
          service:
            name: argocd-server
            port:
              number: 443
EOF

# ArgoCD ì´ˆê¸° ë¹„ë°€ë²ˆí˜¸
kubectl -n argocd get secret argocd-initial-admin-secret \
  -o jsonpath="{.data.password}" | base64 -d && echo

# ===== 5. Prometheus + Grafana (20ë¶„, ì„ íƒ) =====
helm repo add prometheus-community https://prometheus-community.github.io/helm-charts
helm repo update

helm install prometheus prometheus-community/kube-prometheus-stack \
  --namespace monitoring \
  --create-namespace \
  --set prometheus.prometheusSpec.retention=7d \
  --set grafana.adminPassword=admin123

# Grafana Ingress
cat <<EOF | kubectl apply -f -
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: grafana-ingress
  namespace: monitoring
  annotations:
    kubernetes.io/ingress.class: alb
    alb.ingress.kubernetes.io/scheme: internet-facing
    alb.ingress.kubernetes.io/target-type: ip
    alb.ingress.kubernetes.io/certificate-arn: arn:aws:acm:ap-northeast-2:xxxxx:certificate/xxxxx
    alb.ingress.kubernetes.io/group.name: growbin-alb
spec:
  rules:
  - host: growbin.app
    http:
      paths:
      - path: /grafana
        pathType: Prefix
        backend:
          service:
            name: prometheus-grafana
            port:
              number: 80
EOF

# âœ… Add-ons ì„¤ì¹˜ ì™„ë£Œ!
```

---

## ğŸ“¦ ì„œë¹„ìŠ¤ ë°°í¬

### Helm Charts êµ¬ì¡°

```
charts/
â”œâ”€â”€ auth/
â”‚   â”œâ”€â”€ Chart.yaml
â”‚   â”œâ”€â”€ values.yaml
â”‚   â”œâ”€â”€ values-dev.yaml
â”‚   â”œâ”€â”€ values-prod.yaml
â”‚   â””â”€â”€ templates/
â”‚       â”œâ”€â”€ deployment.yaml
â”‚       â”œâ”€â”€ service.yaml
â”‚       â”œâ”€â”€ hpa.yaml              # Auto Scaling
â”‚       â”œâ”€â”€ configmap.yaml
â”‚       â””â”€â”€ secret.yaml
â”‚
â”œâ”€â”€ users/
â”œâ”€â”€ waste/
â”‚   â”œâ”€â”€ Chart.yaml
â”‚   â”œâ”€â”€ values.yaml
â”‚   â””â”€â”€ templates/
â”‚       â”œâ”€â”€ deployment.yaml       # API Server
â”‚       â”œâ”€â”€ worker-deployment.yaml  # Celery Worker
â”‚       â”œâ”€â”€ service.yaml
â”‚       â”œâ”€â”€ hpa.yaml
â”‚       â””â”€â”€ pdb.yaml              # Pod Disruption Budget
â”‚
â”œâ”€â”€ recycling/
â””â”€â”€ locations/
```

### ì˜ˆì‹œ: Waste Service Helm Chart

```yaml
# charts/waste/Chart.yaml
apiVersion: v2
name: waste-service
description: AI ê¸°ë°˜ ì“°ë ˆê¸° ë¶„ë¥˜ ì„œë¹„ìŠ¤
version: 1.0.0
appVersion: 1.0.0

# charts/waste/values.yaml
replicaCount: 2  # Worker 1ì— ë°°í¬

image:
  repository: 123456789.dkr.ecr.ap-northeast-2.amazonaws.com/waste-service
  tag: latest
  pullPolicy: Always

service:
  type: ClusterIP
  port: 80
  targetPort: 8000

ingress:
  enabled: true
  annotations:
    kubernetes.io/ingress.class: alb
    alb.ingress.kubernetes.io/scheme: internet-facing
    alb.ingress.kubernetes.io/target-type: ip
    alb.ingress.kubernetes.io/certificate-arn: arn:aws:acm:ap-northeast-2:xxxxx:certificate/xxxxx
    alb.ingress.kubernetes.io/group.name: growbin-alb
  hosts:
    - host: growbin.app
      paths:
        - path: /api/v1/waste
          pathType: Prefix

# Node Affinity (Worker 1ì—ë§Œ ë°°í¬)
nodeSelector:
  workload: heavy

resources:
  requests:
    cpu: 200m
    memory: 256Mi
  limits:
    cpu: 1000m
    memory: 1Gi

autoscaling:
  enabled: true
  minReplicas: 2
  maxReplicas: 5
  targetCPUUtilizationPercentage: 70

env:
  - name: DATABASE_URL
    valueFrom:
      secretKeyRef:
        name: waste-secrets
        key: database-url
  - name: REDIS_URL
    value: "redis://redis-service:6379/1"
  - name: AI_VISION_API_URL
    valueFrom:
      configMapKeyRef:
        name: waste-config
        key: ai-vision-url

# Celery Worker
worker:
  enabled: true
  replicaCount: 3
  resources:
    requests:
      cpu: 500m
      memory: 512Mi
    limits:
      cpu: 2000m
      memory: 2Gi
```

### Deployment í…œí”Œë¦¿

```yaml
# charts/waste/templates/deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: {{ include "waste-service.fullname" . }}
  labels:
    {{- include "waste-service.labels" . | nindent 4 }}
spec:
  replicas: {{ .Values.replicaCount }}
  selector:
    matchLabels:
      {{- include "waste-service.selectorLabels" . | nindent 6 }}
  strategy:
    type: RollingUpdate
    rollingUpdate:
      maxSurge: 1
      maxUnavailable: 0  # ë¬´ì¤‘ë‹¨ ë°°í¬
  template:
    metadata:
      labels:
        {{- include "waste-service.selectorLabels" . | nindent 8 }}
    spec:
      {{- with .Values.nodeSelector }}
      nodeSelector:
        {{- toYaml . | nindent 8 }}
      {{- end }}
      containers:
      - name: {{ .Chart.Name }}
        image: "{{ .Values.image.repository }}:{{ .Values.image.tag }}"
        imagePullPolicy: {{ .Values.image.pullPolicy }}
        ports:
        - name: http
          containerPort: 8000
          protocol: TCP
        env:
        {{- toYaml .Values.env | nindent 8 }}
        livenessProbe:
          httpGet:
            path: /health
            port: http
          initialDelaySeconds: 30
          periodSeconds: 10
        readinessProbe:
          httpGet:
            path: /health
            port: http
          initialDelaySeconds: 10
          periodSeconds: 5
        resources:
          {{- toYaml .Values.resources | nindent 12 }}
```

### ArgoCD Application

```yaml
# argocd/applications/waste.yaml
apiVersion: argoproj.io/v1alpha1
kind: Application
metadata:
  name: waste-service
  namespace: argocd
spec:
  project: default
  
  source:
    repoURL: https://github.com/your-org/sesacthon-backend
    targetRevision: main
    path: charts/waste
    helm:
      valueFiles:
        - values-prod.yaml
  
  destination:
    server: https://kubernetes.default.svc
    namespace: waste
  
  syncPolicy:
    automated:
      prune: true
      selfHeal: true
      allowEmpty: false
    syncOptions:
      - CreateNamespace=true
    retry:
      limit: 5
      backoff:
        duration: 5s
        factor: 2
        maxDuration: 3m

---
apiVersion: argoproj.io/v1alpha1
kind: Application
metadata:
  name: auth-service
  namespace: argocd
spec:
  project: default
  source:
    repoURL: https://github.com/your-org/sesacthon-backend
    targetRevision: main
    path: charts/auth
  destination:
    server: https://kubernetes.default.svc
    namespace: auth
  syncPolicy:
    automated:
      prune: true
      selfHeal: true

# ë‹¤ë¥¸ ì„œë¹„ìŠ¤ë“¤ë„ ë™ì¼ íŒ¨í„´
```

---

## ğŸ”§ ì„œë¹„ìŠ¤ ë°°í¬ ì˜ˆì‹œ

### 1. Helmìœ¼ë¡œ ì§ì ‘ ë°°í¬

```bash
# Waste Service ë°°í¬
helm install waste charts/waste \
  --namespace waste \
  --create-namespace \
  --values charts/waste/values-prod.yaml

# ë°°í¬ í™•ì¸
kubectl get pods -n waste
kubectl get svc -n waste
kubectl get ingress -n waste

# ë¡œê·¸ í™•ì¸
kubectl logs -n waste -l app=waste-service -f
```

### 2. ArgoCDë¡œ GitOps ë°°í¬ (ê¶Œì¥)

```bash
# ArgoCD Application ë“±ë¡
kubectl apply -f argocd/applications/

# ArgoCD UIì—ì„œ í™•ì¸
# https://argocd.yourdomain.com

# CLIë¡œ ë™ê¸°í™”
argocd app sync waste-service

# ìë™ ë™ê¸°í™” í™œì„±í™” (ê¶Œì¥)
argocd app set waste-service --sync-policy automated
```

---

## ğŸ¯ Pod ë°°ì¹˜ ì „ëµ

### Node Affinity ì„¤ì •

```yaml
# Heavy Workload â†’ Worker 1 (t3.medium)
nodeSelector:
  workload: heavy

# Light Workload â†’ Worker 2 (t3.small)
nodeSelector:
  workload: light

# ë˜ëŠ” ë” ì •êµí•œ Affinity
affinity:
  nodeAffinity:
    preferredDuringSchedulingIgnoredDuringExecution:
    - weight: 100
      preference:
        matchExpressions:
        - key: workload
          operator: In
          values:
          - heavy
```

### ì˜ˆìƒ Pod ë°°ì¹˜

```
Master Node (k8s-master):
â”œâ”€ Control Plane (k3s ìì²´)
â”œâ”€ ArgoCD (3 pods)
â”œâ”€ Ingress Controller (1 pod)
â”œâ”€ Cert-manager (1 pod)
â””â”€ Metrics Server (1 pod)

Worker 1 (k8s-worker-1, t3.medium):
â”œâ”€ waste-service (2 pods)
â”œâ”€ waste-worker (3 pods)  â† Celery
â”œâ”€ recycling-service (2 pods)
â””â”€ Prometheus (ì„ íƒ)

Worker 2 (k8s-worker-2, t3.small):
â”œâ”€ auth-service (2 pods)
â”œâ”€ users-service (1 pod)
â””â”€ locations-service (1 pod)
```

---

## ğŸ” ìš´ì˜ ê°€ì´ë“œ

### ê¸°ë³¸ ëª…ë ¹ì–´

```bash
# ë…¸ë“œ ìƒíƒœ í™•ì¸
kubectl get nodes -o wide

# ëª¨ë“  Pod í™•ì¸
kubectl get pods -A

# ì„œë¹„ìŠ¤ë³„ Pod
kubectl get pods -n waste
kubectl get pods -n auth

# ë¡œê·¸ í™•ì¸
kubectl logs -n waste -l app=waste-service -f

# Pod ë‚´ë¶€ ì ‘ì†
kubectl exec -it -n waste <pod-name> -- bash

# ë¦¬ì†ŒìŠ¤ ì‚¬ìš©ëŸ‰
kubectl top nodes
kubectl top pods -A

# Ingress í™•ì¸
kubectl get ingress -A
```

### ë°°í¬ ì—…ë°ì´íŠ¸

```bash
# ë°©ë²• 1: Helm Upgrade
helm upgrade waste charts/waste \
  --namespace waste \
  --values charts/waste/values-prod.yaml

# ë°©ë²• 2: ArgoCD Sync (GitOps)
# Gitì— Helm valuesë§Œ ì—…ë°ì´íŠ¸í•˜ë©´ ìë™ ë°°í¬!
git add charts/waste/values-prod.yaml
git commit -m "chore: Update waste image to v1.2.3"
git push

# ArgoCDê°€ 3ë¶„ ì´ë‚´ ìë™ ê°ì§€ & ë°°í¬

# ë°©ë²• 3: ì´ë¯¸ì§€ë§Œ ë³€ê²½
kubectl set image deployment/waste-service \
  waste-service=new-image:v1.2.3 \
  -n waste

# Rolling Update ì§„í–‰ ìƒí™©
kubectl rollout status deployment/waste-service -n waste
```

### ë¡¤ë°±

```bash
# Helm ë¡¤ë°±
helm rollback waste 1 -n waste

# Kubectl ë¡¤ë°±
kubectl rollout undo deployment/waste-service -n waste

# íŠ¹ì • ë¦¬ë¹„ì „ìœ¼ë¡œ
kubectl rollout undo deployment/waste-service \
  --to-revision=2 -n waste

# ArgoCD ë¡¤ë°±
argocd app rollback waste-service
```

### ìŠ¤ì¼€ì¼ë§

```bash
# ìˆ˜ë™ ìŠ¤ì¼€ì¼
kubectl scale deployment waste-service \
  --replicas=5 -n waste

# HPA í™•ì¸
kubectl get hpa -n waste

# HPA ìˆ˜ì •
kubectl edit hpa waste-service -n waste
```

---

## ğŸ“Š ë¦¬ì†ŒìŠ¤ í• ë‹¹ ê³„íš

### Master Node (t3.medium)

```
ì´ ë¦¬ì†ŒìŠ¤:
â”œâ”€ CPU: 2 cores
â””â”€ Memory: 4GB

ì‚¬ìš©:
â”œâ”€ k3s Control Plane: 0.5 CPU, 1GB
â”œâ”€ ArgoCD: 0.2 CPU, 0.5GB
â”œâ”€ Ingress: 0.1 CPU, 0.3GB
â”œâ”€ Cert-manager: 0.1 CPU, 0.2GB
â”œâ”€ Metrics Server: 0.05 CPU, 0.1GB
â””â”€ ì—¬ìœ : 1.05 CPU, 1.9GB

â†’ ì¶©ë¶„í•¨! âœ…
```

### Worker 1 (t3.medium, Heavy)

```
ì´ ë¦¬ì†ŒìŠ¤:
â”œâ”€ CPU: 2 cores
â””â”€ Memory: 4GB

ê³„íš:
â”œâ”€ waste-service Ã— 2: 0.4 CPU, 0.5GB
â”œâ”€ waste-worker Ã— 3: 1.5 CPU, 1.5GB
â”œâ”€ recycling-service Ã— 2: 0.4 CPU, 0.5GB
â””â”€ ì—¬ìœ : -0.3 CPU, 1.5GB

â†’ CPU ë¶€ì¡± ì˜ˆìƒ!

í•´ê²°:
1. waste-workerë¥¼ 2ê°œë¡œ ì¤„ì„
2. HPAë¡œ Worker 1ë„ ìŠ¤ì¼€ì¼ ì•„ì›ƒ
3. Spot Instance ì¶”ê°€ (t3.medium, $9/ì›”)
```

### Worker 2 (t3.small, Light)

```
ì´ ë¦¬ì†ŒìŠ¤:
â”œâ”€ CPU: 2 cores
â””â”€ Memory: 2GB

ê³„íš:
â”œâ”€ auth-service Ã— 2: 0.4 CPU, 0.5GB
â”œâ”€ users-service Ã— 1: 0.2 CPU, 0.3GB
â”œâ”€ locations-service Ã— 1: 0.2 CPU, 0.3GB
â””â”€ ì—¬ìœ : 1.2 CPU, 0.9GB

â†’ ì¶©ë¶„í•¨! âœ…
```

---

## âš ï¸ non-HA êµ¬ì„± ì£¼ì˜ì‚¬í•­

### Single Point of Failure

```
Master Node ì¥ì•  ì‹œ:
âŒ ì „ì²´ í´ëŸ¬ìŠ¤í„° ë‹¤ìš´
âŒ ìƒˆ Pod ìŠ¤ì¼€ì¤„ë§ ë¶ˆê°€
âŒ API Server ì ‘ê·¼ ë¶ˆê°€

í•˜ì§€ë§Œ:
âœ… ê¸°ì¡´ PodëŠ” ê³„ì† ì‹¤í–‰ë¨
âœ… ì„œë¹„ìŠ¤ ìì²´ëŠ” ì‘ë™ (ì¼ì‹œì )
âœ… Master ì¬ì‹œì‘í•˜ë©´ ë³µêµ¬

ëŒ€ì‘:
1. Master ì •ê¸° ìŠ¤ëƒ…ìƒ· (ì¼ 1íšŒ)
2. ë°±ì—… ìŠ¤í¬ë¦½íŠ¸ ì¤€ë¹„
3. ë¹ ë¥¸ ë³µêµ¬ ì ˆì°¨ ë¬¸ì„œí™”
```

### ë°±ì—… ì „ëµ

```bash
# 1. etcd ë°±ì—… (Kubernetes)
# etcdëŠ” /var/lib/etcdì— ì €ì¥ë¨

# ë°±ì—… ìŠ¤í¬ë¦½íŠ¸
cat <<'EOF' > /usr/local/bin/etcd-backup.sh
#!/bin/bash
DATE=$(date +%Y%m%d_%H%M%S)
BACKUP_DIR="/backup/etcd"

mkdir -p $BACKUP_DIR

# etcdctlë¡œ ìŠ¤ëƒ…ìƒ· ìƒì„±
ETCDCTL_API=3 etcdctl snapshot save $BACKUP_DIR/snapshot-$DATE.db \
  --endpoints=https://127.0.0.1:2379 \
  --cacert=/etc/kubernetes/pki/etcd/ca.crt \
  --cert=/etc/kubernetes/pki/etcd/server.crt \
  --key=/etc/kubernetes/pki/etcd/server.key

# 7ì¼ ì´ìƒ ëœ ë°±ì—… ì‚­ì œ
find $BACKUP_DIR -name "snapshot-*.db" -mtime +7 -delete

echo "Backup completed: snapshot-$DATE.db"
EOF

chmod +x /usr/local/bin/etcd-backup.sh

# Cron ë“±ë¡ (ë§¤ì¼ 02:00)
echo "0 2 * * * /usr/local/bin/etcd-backup.sh" | sudo crontab -

# etcd ë³µêµ¬ ë°©ë²• (ì¥ì•  ì‹œ)
# sudo kubeadm reset
# ETCDCTL_API=3 etcdctl snapshot restore /backup/etcd/snapshot-YYYYMMDD.db
# sudo kubeadm init --ignore-preflight-errors=DirAvailable--var-lib-etcd

# 2. PV ë°ì´í„° ë°±ì—…
# PostgreSQL, Redis ë³¼ë¥¨
kubectl exec -n default postgres-0 -- \
  pg_dump -U postgres sesacthon > backup-$(date +%Y%m%d).sql
```

---

## ğŸš€ CI/CD íŒŒì´í”„ë¼ì¸

### GitHub Actions

```yaml
# .github/workflows/k8s-deploy.yml
name: K8s Deploy

on:
  push:
    branches: [main]
    paths:
      - 'services/**'
      - 'charts/**'

jobs:
  deploy:
    runs-on: ubuntu-latest
    
    steps:
      - uses: actions/checkout@v4
      
      - name: AWS ìê²©ì¦ëª… ì„¤ì •
        uses: aws-actions/configure-aws-credentials@v4
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: ap-northeast-2
      
      - name: ECR ë¡œê·¸ì¸
        run: |
          aws ecr get-login-password | docker login \
            --username AWS \
            --password-stdin $ECR_REGISTRY
      
      - name: Docker ë¹Œë“œ & í‘¸ì‹œ
        run: |
          docker build -t waste-service:${{ github.sha }} ./services/waste
          docker tag waste-service:${{ github.sha }} $ECR_REGISTRY/waste:${{ github.sha }}
          docker push $ECR_REGISTRY/waste:${{ github.sha }}
      
      - name: Helm values ì—…ë°ì´íŠ¸
        run: |
          yq e ".image.tag = \"${{ github.sha }}\"" \
            -i charts/waste/values-prod.yaml
          
          git config user.name "GitHub Actions"
          git config user.email "actions@github.com"
          git add charts/waste/values-prod.yaml
          git commit -m "chore: Update waste to ${{ github.sha }}"
          git push
      
      # ArgoCDê°€ ìë™ìœ¼ë¡œ ê°ì§€í•˜ê³  ë°°í¬!
```

---

## ğŸ“ˆ ëª¨ë‹ˆí„°ë§

### Prometheus + Grafana ëŒ€ì‹œë³´ë“œ

```bash
# Grafana ì ‘ì†
# https://grafana.yourdomain.com
# ID: admin, PW: admin123

ì£¼ìš” ë©”íŠ¸ë¦­:
â”œâ”€ CPU/Memory ì‚¬ìš©ë¥ 
â”œâ”€ Pod ìƒíƒœ
â”œâ”€ Network I/O
â”œâ”€ API ìš”ì²­ ìˆ˜
â””â”€ ì—ëŸ¬ìœ¨
```

### ArgoCD ëŒ€ì‹œë³´ë“œ

```bash
# ArgoCD ì ‘ì†
# https://argocd.yourdomain.com

í™•ì¸ ì‚¬í•­:
â”œâ”€ ì„œë¹„ìŠ¤ë³„ Sync ìƒíƒœ
â”œâ”€ Health ìƒíƒœ
â”œâ”€ ìµœê·¼ ë°°í¬ ì´ë ¥
â””â”€ Git Diff
```

---

## ğŸ¯ ìµœì¢… ìŠ¤í™ ìš”ì•½

### í´ëŸ¬ìŠ¤í„° êµ¬ì„±

```
Master: t3.medium Ã— 1 = $30/ì›”
Worker 1: t3.medium Ã— 1 = $30/ì›”
Worker 2: t3.small Ã— 1 = $15/ì›”
EBS: $6/ì›”
ECR: $1/ì›”
ë°ì´í„°: $9/ì›”
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
ì´: $91/ì›”
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

ì¶”ê°€ ìµœì í™” (ì„ íƒ):
+ Spot Instance (Worker 1): -$21/ì›”
â†’ ìµœì¢…: $70/ì›”
```

### ì„œë¹„ìŠ¤ ë°°í¬ ê³„íš

```
Namespace: auth
â”œâ”€ auth-service: 2 replicas
â””â”€ Node: worker-2

Namespace: users  
â”œâ”€ users-service: 1 replica
â””â”€ Node: worker-2

Namespace: waste
â”œâ”€ waste-service: 2 replicas
â”œâ”€ waste-worker: 3 replicas
â””â”€ Node: worker-1

Namespace: recycling
â”œâ”€ recycling-service: 2 replicas
â””â”€ Node: worker-1

Namespace: locations
â”œâ”€ locations-service: 1 replica
â””â”€ Node: worker-2
```

---

## âœ… ì²´í¬ë¦¬ìŠ¤íŠ¸

### êµ¬ì¶• ë‹¨ê³„

- [ ] AWS EC2 ì¸ìŠ¤í„´ìŠ¤ 3ëŒ€ ìƒì„±
- [ ] ë³´ì•ˆ ê·¸ë£¹ ì„¤ì •
- [ ] kubeadm Master ì´ˆê¸°í™”
- [ ] Workers ì¡°ì¸ (3ê°œ)
- [ ] AWS Load Balancer Controller ì„¤ì¹˜
- [ ] Cert-manager ì„¤ì¹˜
- [ ] ArgoCD ì„¤ì¹˜
- [ ] Prometheus + Grafana (ì„ íƒ)
- [ ] Helm Charts ì‘ì„± (5ê°œ ì„œë¹„ìŠ¤)
- [ ] ArgoCD Applications ë“±ë¡
- [ ] ë„ë©”ì¸ ì—°ê²° & SSL ì„¤ì •
- [ ] ë°±ì—… ìŠ¤í¬ë¦½íŠ¸ ì„¤ì •

### ê²€ì¦ ë‹¨ê³„

- [ ] ëª¨ë“  ë…¸ë“œ Ready ìƒíƒœ
- [ ] ëª¨ë“  Pod Running ìƒíƒœ
- [ ] Ingress ì •ìƒ ë™ì‘
- [ ] SSL ì¸ì¦ì„œ ë°œê¸‰
- [ ] ArgoCD Sync ì •ìƒ
- [ ] HPA ë™ì‘ í™•ì¸
- [ ] ë¶€í•˜ í…ŒìŠ¤íŠ¸
- [ ] ë¡¤ë°± í…ŒìŠ¤íŠ¸

---

## ğŸ“š ë‹¤ìŒ ë‹¨ê³„

1. [GitOps íŒŒì´í”„ë¼ì¸ ì„¤ì •](gitops-multi-service.md)
2. [ì„œë¹„ìŠ¤ë³„ Helm Chart ì‘ì„±](../deployment/helm-charts.md)
3. [ëª¨ë‹ˆí„°ë§ ì„¤ì •](../deployment/monitoring.md)

---

**ì‘ì„±ì¼**: 2025-10-30  
**êµ¬ì„±**: 1 Master + 2 Worker + non-HA  
**ë¹„ìš©**: $70-91/ì›”  
**ìƒíƒœ**: âœ… ìµœì¢… í™•ì •

