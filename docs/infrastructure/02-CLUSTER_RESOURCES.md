# ğŸ–¥ï¸ í´ëŸ¬ìŠ¤í„° ë¦¬ì†ŒìŠ¤ í˜„í™©

> **7-Node Kubernetes í´ëŸ¬ìŠ¤í„° ìƒì„¸ ëª…ì„¸**  
> **ìµœì¢… ì—…ë°ì´íŠ¸**: 2025-11-05  
> **ì†ŒìŠ¤**: Terraform + Ansible

---

## ğŸ“‹ ëª©ì°¨

1. [í´ëŸ¬ìŠ¤í„° ê°œìš”](#í´ëŸ¬ìŠ¤í„°-ê°œìš”)
2. [ë…¸ë“œ ìƒì„¸ ëª…ì„¸](#ë…¸ë“œ-ìƒì„¸-ëª…ì„¸)
3. [ë„¤íŠ¸ì›Œí¬ êµ¬ì„±](#ë„¤íŠ¸ì›Œí¬-êµ¬ì„±)
4. [ë¦¬ì†ŒìŠ¤ ë¶„ì„](#ë¦¬ì†ŒìŠ¤-ë¶„ì„)
5. [ë°°í¬ ê³ ë ¤ì‚¬í•­](#ë°°í¬-ê³ ë ¤ì‚¬í•­)

---

## ğŸ—ï¸ í´ëŸ¬ìŠ¤í„° ê°œìš”

### 1. í´ëŸ¬ìŠ¤í„° ì•„í‚¤í…ì²˜

```mermaid
graph TB
    subgraph VPC["ğŸŒ VPC (10.0.0.0/16)"]
        subgraph AZa["ap-northeast-2a<br/>10.0.1.0/24"]
            Master["`**Master**
            t3.large
            2 vCPU, 8GB`"]
            RabbitMQ["`**RabbitMQ**
            t3.small
            2 vCPU, 2GB`"]
        end
        
        subgraph AZb["ap-northeast-2b<br/>10.0.2.0/24"]
            Worker1["`**Worker-1**
            t3.medium
            2 vCPU, 4GB
            Application`"]
            PostgreSQL["`**PostgreSQL**
            t3.small
            2 vCPU, 2GB`"]
        end
        
        subgraph AZc["ap-northeast-2c<br/>10.0.3.0/24"]
            Worker2["`**Worker-2**
            t3.medium
            2 vCPU, 4GB
            Async Workers`"]
            Redis["`**Redis**
            t3.small
            2 vCPU, 2GB`"]
            Monitoring["`**Monitoring**
            t3.large
            2 vCPU, 8GB`"]
        end
    end
    
    style Master fill:#4A90E2,stroke:#2E5C8A,stroke-width:3px,color:#fff
    style Worker1 fill:#7B68EE,stroke:#4B3C8C,stroke-width:3px,color:#fff
    style Worker2 fill:#9370DB,stroke:#5A478A,stroke-width:3px,color:#fff
    style RabbitMQ fill:#F39C12,stroke:#C87F0A,stroke-width:2px,color:#000
    style PostgreSQL fill:#3498DB,stroke:#2874A6,stroke-width:2px,color:#fff
    style Redis fill:#E74C3C,stroke:#C0392B,stroke-width:2px,color:#fff
    style Monitoring fill:#2ECC71,stroke:#27AE60,stroke-width:2px,color:#fff
    style VPC fill:#F8F9FA,stroke:#6C757D,stroke-width:2px,color:#000
    style AZa fill:#FFF9E6,stroke:#FFE4B3,stroke-width:2px,color:#000
    style AZb fill:#E6F7FF,stroke:#B3E0FF,stroke-width:2px,color:#000
    style AZc fill:#FFF0F5,stroke:#FFD6E8,stroke-width:2px,color:#000
```

### 2. ê¸°ë³¸ ì •ë³´

```yaml
í´ëŸ¬ìŠ¤í„° ì´ë¦„: sesacthon
Kubernetes ë²„ì „: 1.28.4
ë…¸ë“œ ìˆ˜: 7ê°œ (1 Master + 6 Workers)
ë¦¬ì „: ap-northeast-2 (Seoul)
AZ: 3ê°œ (a, b, c)
ê´€ë¦¬ ë°©ì‹: Self-managed (kubeadm)
```

### 3. ì´ ë¦¬ì†ŒìŠ¤

```mermaid
graph LR
    subgraph Resources["ğŸ“Š í´ëŸ¬ìŠ¤í„° ì´ ë¦¬ì†ŒìŠ¤"]
        CPU["`**vCPU**
        14 cores`"]
        RAM["`**RAM**
        30GB`"]
        Storage["`**Storage**
        350GB`"]
        Network["`**Network**
        VPC + 3 Subnets`"]
    end
    
    style CPU fill:#FFE4B3,stroke:#F39C12,stroke-width:3px,color:#000
    style RAM fill:#D1ECF1,stroke:#3498DB,stroke-width:3px,color:#000
    style Storage fill:#D4EDDA,stroke:#2ECC71,stroke-width:3px,color:#000
    style Network fill:#F8D7DA,stroke:#E74C3C,stroke-width:3px,color:#000
    style Resources fill:#F8F9FA,stroke:#6C757D,stroke-width:2px,color:#000
```

| í•­ëª© | ìˆ˜ëŸ‰ |
|------|------|
| **vCPU** | 14 cores |
| **RAM** | 30GB |
| **ìŠ¤í† ë¦¬ì§€** | 350GB |
| **ë„¤íŠ¸ì›Œí¬** | VPC + 3 Public Subnets |

---

## ğŸ–¥ï¸ ë…¸ë“œ ìƒì„¸ ëª…ì„¸

### 1. Control Plane ë…¸ë“œ

#### k8s-master

```yaml
ì—­í• : Control Plane
ì¸ìŠ¤í„´ìŠ¤ íƒ€ì…: t3.large
vCPU: 2 cores
RAM: 8GB
ìŠ¤í† ë¦¬ì§€: 80GB (gp3)
AZ: ap-northeast-2a
Subnet: 10.0.1.0/24
Private IP: 10.0.1.138
Public IP: 52.78.61.199 (Elastic IP)

ì›Œí¬ë¡œë“œ:
  - kube-apiserver
  - kube-controller-manager
  - kube-scheduler
  - etcd
  - CoreDNS

íŠ¹ì§•:
  - EIP í• ë‹¹ (ê³ ì • IP)
  - Control Plane ì „ìš©
  - Application Pod ë°°í¬ ë¶ˆê°€ (Taint)
```

### 2. Application Worker ë…¸ë“œ

#### k8s-worker-1

```yaml
ì—­í• : Application Worker
ì¸ìŠ¤í„´ìŠ¤ íƒ€ì…: t3.medium
vCPU: 2 cores
RAM: 4GB
ìŠ¤í† ë¦¬ì§€: 40GB (gp3)
AZ: ap-northeast-2b
Subnet: 10.0.2.0/24
Private IP: 10.0.2.145
Public IP: 16.184.21.21

ë¼ë²¨:
  workload: application
  instance_type: t3.medium

ì›Œí¬ë¡œë“œ:
  - FastAPI Pods (Sync API)
  - Application Services
  - ë¸”ë£¨-ê·¸ë¦° ë°°í¬ ëŒ€ìƒ

íŠ¹ì§•:
  - NodeSelector: workload=application
  - ë¸”ë£¨-ê·¸ë¦° ë°°í¬ ê°€ëŠ¥
  - 4GB RAM (Blue + Green ë™ì‹œ ê°€ëŠ¥)
```

#### k8s-worker-2

```yaml
ì—­í• : Async Worker
ì¸ìŠ¤í„´ìŠ¤ íƒ€ì…: t3.medium
vCPU: 2 cores
RAM: 4GB
ìŠ¤í† ë¦¬ì§€: 40GB (gp3)
AZ: ap-northeast-2c
Subnet: 10.0.3.0/24
Private IP: 10.0.3.180
Public IP: 15.164.165.44

ë¼ë²¨:
  workload: async-workers
  instance_type: t3.medium

ì›Œí¬ë¡œë“œ:
  - Celery Workers (Async Tasks)
  - Background Jobs
  - ì´ë¯¸ì§€ ì²˜ë¦¬ ì‘ì—…

íŠ¹ì§•:
  - NodeSelector: workload=async-workers
  - CPU ì§‘ì•½ì  ì‘ì—…
  - ë…ë¦½ì  ìŠ¤ì¼€ì¼ë§ ê°€ëŠ¥
```

### 3. Infrastructure Worker ë…¸ë“œ

#### k8s-rabbitmq

```yaml
ì—­í• : Message Queue (ì „ìš©)
ì¸ìŠ¤í„´ìŠ¤ íƒ€ì…: t3.small
vCPU: 2 cores
RAM: 2GB
ìŠ¤í† ë¦¬ì§€: 40GB (gp3)
AZ: ap-northeast-2a
Subnet: 10.0.1.0/24
Private IP: 10.0.1.x

ë¼ë²¨:
  workload: message-queue

ì›Œí¬ë¡œë“œ:
  - RabbitMQ (ë‹¨ì¼ Pod)
  - Message Queue ì „ìš©

íŠ¹ì§•:
  - ì „ìš© ë…¸ë“œ (ë‹¤ë¥¸ Pod ë°°í¬ ë¶ˆê°€)
  - Persistent Volume ì‚¬ìš©
  - Control Planeê³¼ ê°™ì€ AZ
```

#### k8s-postgresql

```yaml
ì—­í• : Database (ì „ìš©)
ì¸ìŠ¤í„´ìŠ¤ íƒ€ì…: t3.small
vCPU: 2 cores
RAM: 2GB
ìŠ¤í† ë¦¬ì§€: 60GB (gp3)
AZ: ap-northeast-2b
Subnet: 10.0.2.0/24
Private IP: 10.0.2.x

ë¼ë²¨:
  workload: database

ì›Œí¬ë¡œë“œ:
  - PostgreSQL 16
  - Database ì „ìš©

íŠ¹ì§•:
  - ì „ìš© ë…¸ë“œ (ê²©ë¦¬)
  - 60GB ìŠ¤í† ë¦¬ì§€ (ë°ì´í„° ë³´ì¡´)
  - Application Workerì™€ ê°™ì€ AZ
```

#### k8s-redis

```yaml
ì—­í• : Cache (ì „ìš©)
ì¸ìŠ¤í„´ìŠ¤ íƒ€ì…: t3.small
vCPU: 2 cores
RAM: 2GB
ìŠ¤í† ë¦¬ì§€: 30GB (gp3)
AZ: ap-northeast-2c
Subnet: 10.0.3.0/24
Private IP: 10.0.3.x

ë¼ë²¨:
  workload: cache

ì›Œí¬ë¡œë“œ:
  - Redis Cache
  - ì¸ë©”ëª¨ë¦¬ ìºì‹œ

íŠ¹ì§•:
  - ì „ìš© ë…¸ë“œ
  - ë©”ëª¨ë¦¬ ìµœì í™”
  - Async Workerì™€ ê°™ì€ AZ
```

#### k8s-monitoring

```yaml
ì—­í• : Monitoring Stack (ì „ìš©)
ì¸ìŠ¤í„´ìŠ¤ íƒ€ì…: t3.large
vCPU: 2 cores
RAM: 8GB
ìŠ¤í† ë¦¬ì§€: 60GB (gp3)
AZ: ap-northeast-2c
Subnet: 10.0.3.0/24
Private IP: 10.0.3.x

ë¼ë²¨:
  workload: monitoring

ì›Œí¬ë¡œë“œ:
  - Prometheus
  - Grafana
  - Alertmanager
  - Node Exporter

íŠ¹ì§•:
  - ì „ìš© ë…¸ë“œ (ëª¨ë‹ˆí„°ë§ ì „ìš©)
  - 8GB RAM (TSDB)
  - 60GB ìŠ¤í† ë¦¬ì§€ (ë©”íŠ¸ë¦­ ë°ì´í„°)
  - t3.large (CPU ì—…ê·¸ë ˆì´ë“œ)
```

---

## ğŸŒ ë„¤íŠ¸ì›Œí¬ êµ¬ì„±

### VPC

```yaml
VPC CIDR: 10.0.0.0/16
DNS Hostnames: Enabled
DNS Support: Enabled
Internet Gateway: igw-xxx

Tags:
  kubernetes.io/cluster/prod-sesacthon: shared
```

### Subnets

```yaml
ap-northeast-2a (10.0.1.0/24):
  - k8s-master (10.0.1.138)
  - k8s-rabbitmq (10.0.1.x)
  - 256 IPs available
  - Public Subnet
  - Tag: kubernetes.io/role/elb=1

ap-northeast-2b (10.0.2.0/24):
  - k8s-worker-1 (10.0.2.145)
  - k8s-postgresql (10.0.2.x)
  - 256 IPs available
  - Public Subnet

ap-northeast-2c (10.0.3.0/24):
  - k8s-worker-2 (10.0.3.180)
  - k8s-redis (10.0.3.x)
  - k8s-monitoring (10.0.3.x)
  - 256 IPs available
  - Public Subnet
```

### CNI êµ¬ì„±

```yaml
CNI Plugin: Calico
ë„¤íŠ¸ì›Œí¬ ëª¨ë“œ: Overlay Network
Pod CIDR: 192.168.0.0/16 (Calico)
Service CIDR: 10.96.0.0/12

íŠ¹ì§•:
  - VPCì™€ ë…ë¦½ì ì¸ Pod ë„¤íŠ¸ì›Œí¬
  - Overlay Network (VXLAN)
  - Network Policy ì§€ì›
  - ALBëŠ” target-type=instance + NodePort ì‚¬ìš©
```

---

## ğŸ“Š ë¦¬ì†ŒìŠ¤ ë¶„ì„

### 1. vCPU ë¶„í¬

```mermaid
pie title vCPU ë¶„í¬ (14 cores)
    "Master" : 2
    "Worker-1" : 2
    "Worker-2" : 2
    "RabbitMQ" : 2
    "PostgreSQL" : 2
    "Redis" : 2
    "Monitoring" : 2
```

### 2. RAM ë¶„í¬

```mermaid
pie title RAM ë¶„í¬ (30GB)
    "Master (Control Plane)" : 8
    "Worker-1 (Application)" : 4
    "Worker-2 (Async)" : 4
    "RabbitMQ" : 2
    "PostgreSQL" : 2
    "Redis" : 2
    "Monitoring" : 8
```

### 3. ìŠ¤í† ë¦¬ì§€ ë¶„í¬

```mermaid
pie title ìŠ¤í† ë¦¬ì§€ ë¶„í¬ (350GB)
    "Master" : 80
    "Worker-1" : 40
    "Worker-2" : 40
    "RabbitMQ" : 40
    "PostgreSQL" : 60
    "Redis" : 30
    "Monitoring" : 60
```

### 4. ë¦¬ì†ŒìŠ¤ ìš”ì•½ í…Œì´ë¸”

| ë…¸ë“œ | vCPU | RAM | ìŠ¤í† ë¦¬ì§€ | ë¹„ìœ¨ (RAM) |
|------|------|-----|----------|-----------|
| **Master** | 2 cores | 8GB | 80GB | 27% |
| **Worker-1** | 2 cores | 4GB | 40GB | 13% |
| **Worker-2** | 2 cores | 4GB | 40GB | 13% |
| **RabbitMQ** | 2 cores | 2GB | 40GB | 7% |
| **PostgreSQL** | 2 cores | 2GB | 60GB | 7% |
| **Redis** | 2 cores | 2GB | 30GB | 7% |
| **Monitoring** | 2 cores | 8GB | 60GB | 27% |
| **Total** | **14 cores** | **30GB** | **350GB** | **100%** |

---

### 5. AZ ë¶„ì‚°

```mermaid
graph TB
    subgraph AZa["ğŸ“ ap-northeast-2a<br/>4 vCPU, 10GB RAM"]
        AZa1["`**Master**
        t3.large
        2 vCPU, 8GB`"]
        AZa2["`**RabbitMQ**
        t3.small
        2 vCPU, 2GB`"]
    end
    
    subgraph AZb["ğŸ“ ap-northeast-2b<br/>4 vCPU, 6GB RAM"]
        AZb1["`**Worker-1**
        t3.medium
        2 vCPU, 4GB`"]
        AZb2["`**PostgreSQL**
        t3.small
        2 vCPU, 2GB`"]
    end
    
    subgraph AZc["ğŸ“ ap-northeast-2c<br/>6 vCPU, 14GB RAM"]
        AZc1["`**Worker-2**
        t3.medium
        2 vCPU, 4GB`"]
        AZc2["`**Redis**
        t3.small
        2 vCPU, 2GB`"]
        AZc3["`**Monitoring**
        t3.large
        2 vCPU, 8GB`"]
    end
    
    style AZa1 fill:#4A90E2,stroke:#2E5C8A,stroke-width:2px,color:#fff
    style AZa2 fill:#F39C12,stroke:#C87F0A,stroke-width:2px,color:#000
    style AZb1 fill:#7B68EE,stroke:#4B3C8C,stroke-width:2px,color:#fff
    style AZb2 fill:#3498DB,stroke:#2874A6,stroke-width:2px,color:#fff
    style AZc1 fill:#9370DB,stroke:#5A478A,stroke-width:2px,color:#fff
    style AZc2 fill:#E74C3C,stroke:#C0392B,stroke-width:2px,color:#fff
    style AZc3 fill:#2ECC71,stroke:#27AE60,stroke-width:2px,color:#fff
    style AZa fill:#FFF9E6,stroke:#FFE4B3,stroke-width:2px,color:#000
    style AZb fill:#E6F7FF,stroke:#B3E0FF,stroke-width:2px,color:#000
    style AZc fill:#FFF0F5,stroke:#FFD6E8,stroke-width:2px,color:#000
```

---

## ğŸš€ ë°°í¬ ê³ ë ¤ì‚¬í•­

### ë¸”ë£¨-ê·¸ë¦° ë°°í¬

**Application Worker ë¦¬ì†ŒìŠ¤**
```yaml
ê°€ìš© Worker ë…¸ë“œ: 2ê°œ (worker-1, worker-2)
ê° Worker RAM: 4GB
ê° Worker vCPU: 2 cores

Application Pod ë¦¬ì†ŒìŠ¤:
  requests:
    memory: 256Mi
    cpu: 100m
  limits:
    memory: 512Mi
    cpu: 500m

ë°°í¬ ì‹œë‚˜ë¦¬ì˜¤:
  í‰ì†Œ: Blue 3 Pods = 1.5GB RAM
  ë°°í¬ ì‹œ: Blue 3 + Green 3 = 3GB RAM
  ì—¬ìœ : 8GB ì¤‘ 3GB ì‚¬ìš© = ì¶©ë¶„ âœ…
```

**NodeSelector ì‚¬ìš©**
```yaml
# Application Podì— NodeSelector ì„¤ì • í•„ìˆ˜
nodeSelector:
  workload: application

# ì´ìœ :
# - Infrastructure ë…¸ë“œ (RabbitMQ, PostgreSQL, Redis, Monitoring)ëŠ” ì „ìš©
# - Application Podì€ worker-1, worker-2ì—ë§Œ ë°°í¬
```

### ì¹´ë‚˜ë¦¬ ë°°í¬

**ë¦¬ì†ŒìŠ¤ íš¨ìœ¨ì„±**
```yaml
Canary 10%:
  - Stable: 9 Pods
  - Canary: 1 Pod
  - Total: 10 Pods = 5GB RAM
  - ì—¬ìœ : ì¶©ë¶„ âœ…

Canary 50%:
  - Stable: 5 Pods
  - Canary: 5 Pods
  - Total: 10 Pods = 5GB RAM
  - ì—¬ìœ : ì¶©ë¶„ âœ…
```

### Auto Scaling

**HPA ì„¤ì • ê¶Œì¥**
```yaml
minReplicas: 3
maxReplicas: 8
targetCPUUtilizationPercentage: 70

ìµœëŒ€ ë¦¬ì†ŒìŠ¤ ì‚¬ìš©:
  8 Pods x 512MB = 4GB
  ê°€ìš©: 8GB (worker-1: 4GB, worker-2: 4GB)
  ì—¬ìœ : ì¶©ë¶„ âœ…
```

### ë¦¬ì†ŒìŠ¤ ëª¨ë‹ˆí„°ë§

**Prometheus ë©”íŠ¸ë¦­**
```promql
# ë…¸ë“œ CPU ì‚¬ìš©ë¥ 
100 - (avg by (instance) (rate(node_cpu_seconds_total{mode="idle"}[5m])) * 100)

# ë…¸ë“œ ë©”ëª¨ë¦¬ ì‚¬ìš©ë¥ 
(1 - (node_memory_MemAvailable_bytes / node_memory_MemTotal_bytes)) * 100

# Pod ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰
sum(container_memory_working_set_bytes{pod=~"backend-.*"}) by (pod) / 1024 / 1024

# Application Worker ê°€ìš© ë©”ëª¨ë¦¬
node_memory_MemAvailable_bytes{node=~".*worker-[12]"}
```

---

## ğŸ“š ê´€ë ¨ ë¬¸ì„œ

- [VPC ë„¤íŠ¸ì›Œí¬ ì„¤ê³„](vpc-network-design.md)
- [ë°°í¬ ì „ëµ ë¹„êµ](../plans/DEPLOYMENT_STRATEGIES_COMPARISON.md)
- [ìµœì¢… K8s ì•„í‚¤í…ì²˜](../architecture/final-k8s-architecture.md)
- [ì„œë¹„ìŠ¤ ì•„í‚¤í…ì²˜](../architecture/SERVICE_ARCHITECTURE.md)

---

**ë¬¸ì„œ ë²„ì „**: 1.0  
**ìµœì¢… ì—…ë°ì´íŠ¸**: 2025-11-05  
**ì†ŒìŠ¤**: Terraform (main.tf) + Ansible (inventory/hosts)  
**ê²€ì¦**: kubectl get nodes ëª…ë ¹ìœ¼ë¡œ í™•ì¸ ì™„ë£Œ

