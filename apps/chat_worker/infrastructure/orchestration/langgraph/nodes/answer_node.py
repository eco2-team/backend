"""Answer Generation Node - ì˜¤ì¼€ìŠ¤íŠ¸ë ˆì´ì…˜ ì „ìš©.

ë…¸ë“œ ì±…ì„: ì´ë²¤íŠ¸ ë°œí–‰ + ì„œë¹„ìŠ¤ í˜¸ì¶œ + state ì—…ë°ì´íŠ¸
ë¹„ì¦ˆë‹ˆìŠ¤ ë¡œì§: AnswerGeneratorServiceì— ìœ„ì„

Clean Architecture:
- Node: ì˜¤ì¼€ìŠ¤íŠ¸ë ˆì´ì…˜ (ì´ íŒŒì¼)
- Service: AnswerGeneratorService (ë¹„ì¦ˆë‹ˆìŠ¤ ë¡œì§)
- Port: LLMPort (ìˆœìˆ˜ LLM í˜¸ì¶œ)
"""

from __future__ import annotations

import logging
from typing import TYPE_CHECKING, Any

from chat_worker.application.answer.dto import AnswerContext
from chat_worker.application.answer.services import AnswerGeneratorService

if TYPE_CHECKING:
    from chat_worker.application.ports.events import ProgressNotifierPort
    from chat_worker.application.ports.llm import LLMClientPort

logger = logging.getLogger(__name__)

ANSWER_SYSTEM_PROMPT = """ë„ˆëŠ” "ì´ì½”"ì•¼, EcoÂ² ì•±ì˜ ì¹œì ˆí•œ ë¶„ë¦¬ë°°ì¶œ ë„ìš°ë¯¸.

## ì„±ê²©
- ì¹œì ˆí•˜ê³  ê·€ì—¬ìš´ ë§íˆ¬
- í™˜ê²½ ë³´í˜¸ì— ì—´ì •ì 

## ë‹µë³€ ê·œì¹™
1. ê°„ê²°í•˜ê³  ì‹¤ìš©ì ì¸ ì •ë³´ ì œê³µ
2. ë¶„ë¦¬ë°°ì¶œ ë°©ë²•ì€ ë‹¨ê³„ë³„ë¡œ ì„¤ëª…
3. ì˜ëª»ëœ ì •ë³´ë³´ë‹¤ ëª¨ë¥¸ë‹¤ê³  ì†”ì§íˆ ë§í•˜ê¸°
4. ì‚¬ìš©ìë¥¼ ê²©ë ¤í•˜ê³  ì‘ì›í•˜ê¸°
"""


def create_answer_node(
    llm: "LLMClientPort",
    event_publisher: "ProgressNotifierPort",
):
    """ë‹µë³€ ìƒì„± ë…¸ë“œ íŒ©í† ë¦¬.

    ë…¸ë“œëŠ” thin wrapperë¡œ:
    1. ì´ë²¤íŠ¸ ë°œí–‰
    2. AnswerGeneratorService í˜¸ì¶œ
    3. state ì—…ë°ì´íŠ¸
    """
    # ì„œë¹„ìŠ¤ ì¸ìŠ¤í„´ìŠ¤ (ë¹„ì¦ˆë‹ˆìŠ¤ ë¡œì§ ë‹´ë‹¹)
    answer_service = AnswerGeneratorService(llm)

    async def answer_node(state: dict[str, Any]) -> dict[str, Any]:
        job_id = state["job_id"]
        message = state.get("message", "")
        classification = state.get("classification_result")
        disposal_rules = state.get("disposal_rules")
        character_context = state.get("character_context")
        location_context = state.get("location_context")

        # 1. ì´ë²¤íŠ¸: ì‹œì‘
        await event_publisher.notify_stage(
            task_id=job_id,
            stage="answer",
            status="started",
            progress=70,
            message="ğŸ’­ ë‹µë³€ ê³ ë¯¼ ì¤‘...",
        )

        try:
            # 2. ì»¨í…ìŠ¤íŠ¸ êµ¬ì„± (Serviceì˜ íŒ©í† ë¦¬ ë©”ì„œë“œ ì‚¬ìš©)
            context = AnswerContext(
                classification=classification,
                disposal_rules=disposal_rules.get("data") if disposal_rules else None,
                character_context=character_context,
                location_context=location_context,
                user_input=message,
            )

            # 3. ì„œë¹„ìŠ¤ í˜¸ì¶œ (ìŠ¤íŠ¸ë¦¬ë°)
            answer_parts = []
            async for token in answer_service.generate_stream(
                context=context,
                system_prompt=ANSWER_SYSTEM_PROMPT,
            ):
                # í† í° ì´ë²¤íŠ¸ ë°œí–‰ (SSE ìŠ¤íŠ¸ë¦¬ë°)
                await event_publisher.notify_token(
                    task_id=job_id,
                    content=token,
                )
                answer_parts.append(token)

            answer = "".join(answer_parts)

            logger.info(
                "Answer generated",
                extra={
                    "job_id": job_id,
                    "length": len(answer),
                },
            )

            # 4. ì´ë²¤íŠ¸: ì™„ë£Œ
            await event_publisher.notify_stage(
                task_id=job_id,
                stage="answer",
                status="completed",
                progress=100,
            )

            return {**state, "answer": answer}

        except Exception as e:
            logger.error(
                "Answer generation failed",
                extra={"job_id": job_id, "error": str(e)},
            )
            await event_publisher.notify_stage(
                task_id=job_id,
                stage="answer",
                status="failed",
                result={"error": str(e)},
            )
            return {
                **state,
                "answer": "ì£„ì†¡í•´ìš”, ë‹µë³€ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆì–´ìš”. ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”! ğŸ™",
            }

    return answer_node
