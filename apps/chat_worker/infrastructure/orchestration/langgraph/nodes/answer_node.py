"""Answer Generation Node - LangGraph ì–´ëŒ‘í„°.

ì–‡ì€ ì–´ëŒ‘í„°: state ë³€í™˜ + Command í˜¸ì¶œ + progress notify (UX).
ì •ì±…/íë¦„ì€ GenerateAnswerCommand(Application)ì—ì„œ ì²˜ë¦¬.

Clean Architecture:
- Node(Adapter): ì´ íŒŒì¼ - LangGraph glue code
- Command(UseCase): GenerateAnswerCommand - ì •ì±…/íë¦„
- Service: AnswerGeneratorService - ìˆœìˆ˜ ë¹„ì¦ˆë‹ˆìŠ¤ ë¡œì§
"""

from __future__ import annotations

import logging
from typing import TYPE_CHECKING, Any

from chat_worker.application.commands.generate_answer_command import (
    GenerateAnswerCommand,
    GenerateAnswerInput,
)
from chat_worker.infrastructure.assets.prompt_loader import PromptBuilder

if TYPE_CHECKING:
    from chat_worker.application.ports.cache import CachePort
    from chat_worker.application.ports.events import ProgressNotifierPort
    from chat_worker.application.ports.llm import LLMClientPort

logger = logging.getLogger(__name__)


def create_answer_node(
    llm: "LLMClientPort",
    event_publisher: "ProgressNotifierPort",
    cache: "CachePort | None" = None,
):
    """ë‹µë³€ ìƒì„± ë…¸ë“œ íŒ©í† ë¦¬.

    NodeëŠ” LangGraph ì–´ëŒ‘í„°:
    - state â†’ input DTO ë³€í™˜
    - Command(UseCase) í˜¸ì¶œ
    - í† í° ìŠ¤íŠ¸ë¦¬ë° â†’ SSE ë°œí–‰
    - output â†’ state ë³€í™˜

    Args:
        llm: LLM í´ë¼ì´ì–¸íŠ¸
        event_publisher: ì´ë²¤íŠ¸ ë°œí–‰ì (SSE)
        cache: ìºì‹œ í´ë¼ì´ì–¸íŠ¸ (ì„ íƒ, Answer ìºì‹±ìš©)

    Returns:
        answer_node í•¨ìˆ˜
    """
    # Command(UseCase) ì¸ìŠ¤í„´ìŠ¤ ìƒì„± - Port ì¡°ë¦½
    prompt_builder = PromptBuilder()
    command = GenerateAnswerCommand(
        llm=llm,
        prompt_builder=prompt_builder,
        cache=cache,
    )

    async def answer_node(state: dict[str, Any]) -> dict[str, Any]:
        """LangGraph ë…¸ë“œ (ì–‡ì€ ì–´ëŒ‘í„°).

        ì—­í• :
        1. stateì—ì„œ ê°’ ì¶”ì¶œ (LangGraph glue)
        2. Command í˜¸ì¶œ (ì •ì±…/íë¦„ ìœ„ì„)
        3. í† í° ìŠ¤íŠ¸ë¦¬ë° â†’ SSE (UX)
        4. output â†’ state ë³€í™˜

        Args:
            state: í˜„ì¬ LangGraph ìƒíƒœ

        Returns:
            ì—…ë°ì´íŠ¸ëœ ìƒíƒœ
        """
        job_id = state["job_id"]

        # Progress: ì‹œì‘ (UX)
        await event_publisher.notify_stage(
            task_id=job_id,
            stage="answer",
            status="started",
            progress=70,
            message="ğŸ’­ ë‹µë³€ ê³ ë¯¼ ì¤‘...",
        )

        try:
            # 1. state â†’ input DTO ë³€í™˜
            # recyclable_price_contextì—ì„œ context ë¬¸ìì—´ ì¶”ì¶œ
            price_ctx = state.get("recyclable_price_context")
            price_context_str = price_ctx.get("context") if isinstance(price_ctx, dict) else None

            # bulk_waste_contextì—ì„œ context ë¬¸ìì—´ ì¶”ì¶œ
            waste_ctx = state.get("bulk_waste_context")
            waste_context_str = waste_ctx.get("context") if isinstance(waste_ctx, dict) else None

            # weather_contextì—ì„œ context ë¬¸ìì—´ ì¶”ì¶œ
            weather_ctx = state.get("weather_context")
            weather_context_str = weather_ctx.get("context") if isinstance(weather_ctx, dict) else None

            # collection_point_contextì—ì„œ context ë¬¸ìì—´ ì¶”ì¶œ
            collection_ctx = state.get("collection_point_context")
            collection_context_str = collection_ctx.get("context") if isinstance(collection_ctx, dict) else None

            input_dto = GenerateAnswerInput(
                job_id=job_id,
                message=state.get("message", ""),
                intent=state.get("intent", "general"),
                additional_intents=state.get("additional_intents", []),
                has_multi_intent=state.get("has_multi_intent", False),
                classification=state.get("classification_result"),
                disposal_rules=state.get("disposal_rules"),
                character_context=state.get("character_context"),
                location_context=state.get("location_context"),
                web_search_results=state.get("web_search_results"),
                recyclable_price_context=price_context_str,
                bulk_waste_context=waste_context_str,
                weather_context=weather_context_str,
                collection_point_context=collection_context_str,
            )

            # 2. Command ì‹¤í–‰ (ìŠ¤íŠ¸ë¦¬ë°)
            answer_parts = []
            async for token in command.execute(input_dto):
                # í† í° ì´ë²¤íŠ¸ ë°œí–‰ (SSE ìŠ¤íŠ¸ë¦¬ë°)
                await event_publisher.notify_token(
                    task_id=job_id,
                    content=token,
                )
                answer_parts.append(token)

            answer = "".join(answer_parts)

            logger.info(
                "Answer generated",
                extra={"job_id": job_id, "length": len(answer)},
            )

            # Progress: ì™„ë£Œ (UX)
            await event_publisher.notify_stage(
                task_id=job_id,
                stage="answer",
                status="completed",
                progress=100,
            )

            # 3. output â†’ state ë³€í™˜
            return {**state, "answer": answer}

        except Exception as e:
            logger.error(
                "Answer generation failed",
                extra={"job_id": job_id, "error": str(e)},
            )
            await event_publisher.notify_stage(
                task_id=job_id,
                stage="answer",
                status="failed",
                result={"error": str(e)},
            )
            return {
                **state,
                "answer": "ì£„ì†¡í•´ìš”, ë‹µë³€ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆì–´ìš”. ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”! ğŸ™",
            }

    return answer_node
